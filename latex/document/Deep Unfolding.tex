\chapter{Deep Unfolding}

We give a story like introduction to \ac{DL} and by pointing out some \ac{DL} limitations motivate the use of \ac{DU}/\ac{AU} 
which is \ac{DL} but with context inspired architecture.

\section{Multi-Layer Perceptrons}
Let $\boldsymbol{x} \in \mathbb{R}^n,\boldsymbol{y} \in \mathbb{R}^m, \boldsymbol{h}_l \in \mathbb{R}^{k_l}, \boldsymbol{W}^1 \in \mathbb{R}^{k_1 \times n}
,\boldsymbol{W}^{N+1} \in \mathbb{R}^{m \times k_N},\boldsymbol{W}^{l+1} \in \mathbb{R}^{k_l \times k_{l+1}}$ where $m,n,l,k_l \in \mathbb{N}$ and $\varphi$ is a
function from $\mathbb{R}$ to $\mathbb{R}$ with certain properties. 
Consecutive mappings:
\begin{equation}
  \begin{split}
    h_i^{1}   &= \sigma \left( \sum_{j}^{} W_{ij}^{1}x_j + b_i^{1} \right)\\ 
              & \vdots\\
    h_i^{l+1} &= \sigma \left( \sum_{j}^{} W_{ij}^{l+1}h_j^l + b_i^{l+1} \right)\\
              & \vdots\\
    y_i^{}    &= \sigma \left( \sum_{j}^{} W_{ij}^{N+1}h_j^N + b_i^{N+1} \right)
  \end{split}
  \end{equation}
% \begin{equation*}
  % x_i^{l+1} = \sigma \left( \sum_{j}^{} W_{ij}^{l+1}x_j^l + b_i^{l+1} \right)
% \end{equation*}
that take $\boldsymbol{x}$ to $\boldsymbol{y}$ is called a \ac{MLP} architecture.
In the \ml jargon $\boldsymbol{x},\boldsymbol{y},\boldsymbol{h}^j,\boldsymbol{W}^j,\boldsymbol{b}^j,\varphi$ are called \emph{input}, \emph{output}, \emph{hidden variables},
\emph{weight matrices}, \emph{biases}, and \emph{activation function}. \ac{MLP}s were designed to approximate outputs from inputs without knowing the the actual mapping and with observations and measurements. 
It is worth noting that the existence of $\varphi$ is necessary otherwise the pairs $\boldsymbol{W}^j,\boldsymbol{b}^j$ would 
just be an affine transformation and stacking multiple affine transformation would still be an affine transformation and stacking 
such layers would be redundant. Not having an activation function also means the whole mapping would perform poorly when approximating 
complex mappings that do not resemble affine transformation. The schematic involving only the input, output, and hidden variables can be seen 
in \cref{fig:multi_layer_perceptron}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% Multi-Layer Perceptron %%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
  \centering
  % \captionsetup{justification=centering}
  \resizebox{0.7\textwidth}{!}{\input{./tikz/neural_networks/sample.tex}}
  \caption{$\boldsymbol{x} =\text{input},\boldsymbol{h}^j=\text{hidden variables},\boldsymbol{y}=\text{output}$}
  \label{fig:multi_layer_perceptron}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Deep Learning}
The advancements in \ac{HPC}\cite{Meuer} in terms of both software\cite{OpenMPCommette}\cite{MPICommittee}\cite{Nvidia} and 
hardware\cite{Patterson2014}\cite{Hennessy2019}\cite{Nvidia}, Mathematical Optimization\cite{Boyd2004}\cite{Nocedal2006}\cite{Sun2019}, 
\ac{AD}\cite{Naumann2011}\cite{Griewank2008} combined with the abundance of data gathered during the Web $2.0$ era, 
and finally complex approximation architectures gave birth to what is currently known as 
\ac{DL}\cite{LeCun2015}\cite{Higham2018}\cite{Berner2021}. In the \ac{ML}/\ac{DL}/\ac{AI} folklore there were these two dark 
periods known as the \emph{\ac{AI} winters} that meant the substantial reduction in \ac{ML}/\ac{DL}/\ac{AI} funding and 
interest. In 2012 when Alex Krizhevsky's and his supervisors' \dl architecture \cite{Krizhevsky2017} decimated all other 
competitors in the ImageNet \cite{Vision2021} image classification challenge\cite{Vision} he ended the second harsh winter 
that the \ac{ML}/\ac{AI} scientists were experiencing. It was no fluke and at the time of writing the challenge is 
always won by \ac{DL} architectures \cite{Szegedy2014}\cite{He2015}\cite{Simonyan2014} and alike and not carefully 
handcrafted feature extraction modules. All and all \ac{DL} architectures had great success in a broad flavors of 
problems ranging from single object image classification \cite{Rawat2017} to real-time multi-object classification and 
tracking\cite{Luo2021}. \ac{DL} approach has pros:

\begin{itemize}
  \item can extract extremely complicated mappings or extremely subtle features(depending on the desired wording),
  \item requires little to no knowledge about the exact internals of the problem(no need for handcrafted feature engineering),
  \item currently can beat human level performance in lots of areas,
\end{itemize}

and cons:

\begin{itemize}
  \item requires large and high quality datasets\cite{Vision2021} which are expensive to acquire and store,
  \item requires tremendous raw computational power and storage which in turn would result in large electricity bills and expensive maintenance costs,
  \item since mostly there is no interpretability associated with models, there will be no reasoning when a model gives objectively wrong answers. 
\end{itemize}

Due to the pros and cons tied to \ac{DL} what is getting a bit of traction at least in \ac{DSIP} and \ac{CV} is \ac{AU} or \ac{DU}. 
\ac{DU}/\ac{AU} is the process of unfolding/unrolling an iterative algorithm finite times and training certain parameters in 
the hopes of improving some metric by putting the unrolled algorithm in a model. It is best to look at the process in \cref{fig:deep_unfolding_unrolling} 
to get a better feeling of what is being done. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% Deep Unfolding(Schematic Diagram) %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
  \centering
	\captionsetup{justification=centering}
  \resizebox{35em}{10em}{\input{./tikz/diagrams/unfolding.tex}}
  \caption{The Schematic Unfolding/Unrolling of an Iterative Algorithm}
  \label{fig:deep_unfolding_unrolling}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ac{DU}/\ac{AU} tries to bring the best of the both worlds(analytical approach and data driven approach) in one and reconcile the two. 
Since in the \ac{DU}/\ac{AU} you significantly reduce the number of parameters by using a context inspired model and not a full blown 
general \ac{DL} architecture, the need for large datasets and computational power ceases to exist. initialization of the weights 
and interpretation of the models become way easier. A nice review on the recent application of \ac{DU}/\ac{AU} in \ac{DSIP} is done by\cite{Monga2019}.


