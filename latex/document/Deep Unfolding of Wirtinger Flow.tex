\chapter{Deep Unfolding of Wirtinger Flow}\label{ch:deep_unfolding_of_wirtinger_flows}



\section{Deep Unfolding}

We give a story like introduction to \dl\cite{Goodfellow2016} and by pointing out some \dl limitations motivate the use of \du/\au\cite{Monga2019} 
which is \dl like, but with context inspired architecture.

\subsection{Multi-Layer Perceptrons}
Let $\boldsymbol{x} \in \mathbb{R}^n,\boldsymbol{y} \in \mathbb{R}^m, \boldsymbol{h}^l \in \mathbb{R}^{k_l}, \boldsymbol{W}^1 \in \mathbb{R}^{k_1 \times n}
, \boldsymbol{b}^1 \in \mathbb{R}^{k_1}, \boldsymbol{W}^{N+1} \in \mathbb{R}^{m \times k_N}, \mathbb{b}^m,\boldsymbol{W}^{l+1} \in \mathbb{R}^{k_l \times k_{l+1}}$ 
where $m,n,l,k_{l\in \left\{0,\ldots,N-1\right\}} \in \mathbb{N}$ and $\varphi \colon \mathbb{R} \to \mathbb{R}$ is a 
nonlinear function with certain properties. 
Consecutive mappings:
\begin{equation}
  \begin{split}
    h_i^{1}   &= \sigma \left( \sum_{j}^{} W_{ij}^{1}x_j + b_i^{1} \right)\\ 
              & \vdots\\
    h_i^{l+1} &= \sigma \left( \sum_{j}^{} W_{ij}^{l+1}h_j^l + b_i^{l+1} \right)\\
              & \vdots\\
    y_i^{}    &= \sigma \left( \sum_{j}^{} W_{ij}^{N+1}h_j^N + b_i^{N+1} \right)
  \end{split}
  \end{equation}
that take $\boldsymbol{x}$ to $\boldsymbol{y}$ is called a \ac{MLP}\cite{Bishop2006} architecture.
In the \ml/\dl jargon \cite{Goodfellow2016}\cite{ShalevShwartz2014} $\boldsymbol{x},\boldsymbol{y},\boldsymbol{h}^j,\boldsymbol{W}^j,\boldsymbol{b}^j,\varphi$ are called \emph{input}, \emph{output}, \emph{hidden variables/layers},
\emph{weight matrices}, \emph{biases}, and \emph{activation function}. \ac{MLP} is one of the variation of a \nn and the \nn name itself is coming from a 
time where scientists were trying sell their architecture as if they were working like the human brain and unfortunately the name stuck\footnote{
  In older non-math books you can find phrases like ``neurons firing up and this is modeled by our activation function'' 
  instead of the non-linearity argument we gave above. Just to be clear \nns do not work like the human brain.
}.
\ac{MLP}s were designed to approximate outputs from inputs without knowing the the actual mapping 
and with just having $P$ pairs of $\left(\boldsymbol{x}^*,\boldsymbol{y}^*\right)$ (having $P$ sample points) through adjusting the \emph{weight matrices} and the \emph{biases} which is know as the \emph{training process}. 
The main steps in the training process are:
\begin{enumerate}
  \item Initialize the \emph{weight matrices} and the \emph{biases} (mostly done randomly).
  \item Predict $\boldsymbol{y}^*$s by giving your model $\boldsymbol{x}^*$s and call it $\boldsymbol{y}_e$.
  \item Select a metric\cite{Alt2016} and and calculate the distance between $\boldsymbol{y}^*$ and $\boldsymbol{y}_e$. Easiest metric that 
  you can come up with if you take the norm (like \cref{def:scalar_product_induced_norm_complex_matrices}, \cref{def:scalar_product_induced_norm_complex_matrices}, or \cref{def:p-norm}) 
  of the difference, $\boldsymbol{y}^* - \boldsymbol{y}_e$, and call it a day.
  \item Try to minimize the distance between your estimate $\boldsymbol{y}_e$ and your ground truth $\boldsymbol{y}^*$ using an optimizer like \adam\cite{Kingma2014}.
\end{enumerate}  
It is worth noting that the existence of $\varphi$ is necessary otherwise the pairs $\boldsymbol{W}^j,\boldsymbol{b}^j$ would 
just be an affine transformation and stacking multiple affine transformation would still be an affine transformation and stacking 
such layers would be redundant. Not having an activation function also means the whole mapping would perform poorly when approximating 
complex mappings that do not resemble affine transformation. The schematic involving only the input, output, and hidden variables can be seen 
in \cref{fig:multi_layer_perceptron}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% Multi-Layer Perceptron %%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
  \centering
  % \captionsetup{justification=centering}
  \resizebox{0.7\textwidth}{!}{\input{./tikz/neural_networks/sample.tex}}
  \caption{$\boldsymbol{x} =\text{input},\boldsymbol{h}^j=\text{hidden variables},\boldsymbol{y}=\text{output}$}
  \label{fig:multi_layer_perceptron}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Deep Learning}
The advancements in \ac{HPC}\cite{Meuer} in terms of both software\cite{Dagum1998}\cite{1993}\cite{Nvidia} and 
hardware\cite{Patterson2014}\cite{Hennessy2019}\cite{Nvidia}, Mathematical Optimization\cite{Boyd2004}\cite{Nocedal2006}\cite{Sun2019}, 
\ad\cite{Naumann2011}\cite{Griewank2008} combined with the abundance of data gathered during the Web $2.0$ era, 
and finally complex approximation architectures gave birth to what is currently known as 
\dl\cite{LeCun2015}\cite{Higham2018}\cite{Berner2021}. In the \ml/\dl/\ai folklore there were these two dark 
periods known as the \emph{\ai winters} that meant the substantial reduction in \ml/\dl/\ai funding and 
interest. In $2012$ when Alex Krizhevsky's and his supervisors' \dl architecture \cite{Krizhevsky2017} decimated all other 
competitors in the ImageNet \cite{Deng2009} image classification challenge\cite{SVLL} he ended the second harsh winter 
that the \ml/\ai scientists were experiencing. It was no fluke and at the time of writing the challenge is 
always won by \dl architectures \cite{Szegedy2014}\cite{He2015}\cite{Simonyan2014} and alike and not carefully 
handcrafted feature extraction modules. All and all \dl architectures had great success in a broad flavors of 
problems ranging from single object image classification \cite{Rawat2017} to real-time multi-object classification and 
tracking\cite{Luo2021}. \DL approach has the following pros:

\begin{itemize}
  \item can extract extremely complicated mappings or extremely subtle features (depending on the desired wording),
  \item requires little to no knowledge about the exact internals of the problem (no need for handcrafted feature engineering),
  \item currently can beat human level performance in lots of areas; object detection (single and multi), 
  object tracking(single and multi), anomaly detection, fraud detection, and strategical games (\cite{Silver2016} being the most astounding one) 
  just to name a few.
\end{itemize}

and the following cons:

\begin{itemize}
  \item requires large and high quality datasets like the ImageNet\cite{Deng2009}, in the case of image classification, which are expensive to acquire and store,
  \item requires tremendous raw computational power at the level of top 500 supercomputers listed in \cite{Meuer} and 
  storage which in turn would result in large electricity bills
  \footnote{In the $2022$ iteration of the \emph{Introduction to High-Performance Computing} course; Matthias M\"{u}ller, the head of 
  the \ac{HPC} center at RWTH Aachen University, explained his rule of thumb as one Megawatt costing them one million 
  euros per year.} and expensive maintenance costs,
  \item since mostly there is no interpretability associated with models, there will be no reasoning when a model gives objectively wrong answers. 
\end{itemize}

Due to the pros and cons tied to \dl what is getting a bit of traction at least in \dsip and \cv is \du or \au. Using whether to use \du 
or \au depends on the preference of the practitioner and most importantly their educational backgrounds. We go with \du as it is befitting ours from now on. \du is 
the process of \emph{unfolding} an iterative algorithm finite times, interpreting it as a \nn and 
adjusting certain parameters in the hopes of improving some metric during the training process. \DU architectures are 
different from other \nns in the following aspects:
\begin{itemize}
  \item In the context of the iterative algorithms we say the internal structure of the $k+1$-th iteration acted on the input values, $\boldsymbol{z}^k \in \mathbb{C}^N$, and gave $\boldsymbol{z}^{k+1} \in \mathbb{C}^N$ 
  as the output values. In the \ml/\dl context we say values at layer $k$, $\boldsymbol{z}^k \in \mathbb{C}^N$, are mapped to values at layer $k+1$, $\boldsymbol{z}^{k+1} \in \mathbb{C}^N$.
  \item The previous point is not just a reinterpretation as the internal structure of the iteration dictates the activation functions and the dependency graph. 
  Most of the times the said activation functions and the dependency graphs are so sophisticated that it is next to impossible to guess them unlike the general \nns 
  with their corresponding activation functions and mappings.
  \item In general \nns we add more and more parameters in the hope of retrieving the complex mappings while in \dl we already know the mapping 
  and even if we decide to make the \nn more complex or add more parameters we do it based on analytical arguments. This is in fact one of the most crucial points of 
  \du compared to general \nns as number of parameters have direct and substantial effects over datasets'
   sizes, training runtimes, required computational powers and many more. 
  \item Initialization of the weight matrices and biases are mostly done randomly for the case of general \nns while in the \du initialization is coming for the 
  iterative algorithm which is mostly well thought and closely inspected.   
\end{itemize}
In short \du tries to bring the best of the both worlds (analytical approach and data driven approach) in one and reconcile the two. 
While nothing beats an actual numerical experiment based on \du like what we did in \cref{ch:results} for now it is best to look at 
the process in \cref{fig:deep_unfolding_unrolling} to get a better feeling of what is being done. A word of caution to the people who are more familiar with \ml/\dl is that \du is not a \rnn\cite{Gregor2010}. \DU looks like a \rnn\cite{Gregor2010} but 
a closer look will result in a couple of subtleties between the two. The first one is that \rnns need time-series like data meaning we need all 
the intermediate values of an iterative algorithm to train a \rnn while in \du we only need the value that goes in and the value that comes out of an iterative algorithm. The second is that if 
you decide to copy the structure of an iteration and interpret it as a layer but then change some of the parameters while keeping the rest the same then 
the resulting \nn can not be reinterpreted as a \rnn as the layers are completely distinct and there is no \emph{reentry}. Since in the \du
 you significantly reduce the number of parameters by using a context inspired model and not a full blown general \ml/\dl architecture, 
 the need for large datasets and computational power ceases to exist\footnote{It is worthwhile to check the system we used for our 
study in \cref{tab:system_specs} to the general systems\cite{Meuer} that are used in training large \ml/\dl models.}. Initialization 
of the weights, which for some time were the center of attention in \ml/\dl community \cite{Glorot2010}\cite{He2015a}, 
and interpretation of the models become way easier. For initialization in the \du interpretation you basically set them according 
to the original model that is being \emph{unfolded}. We will explain this in more details when we are describing the 
training process of our \ml/\dl model in our study. A nice review on the recent application of \du in \dsip is done by \cite{Monga2019}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% Deep Unfolding(Schematic Diagram) %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
  \centering
	\captionsetup{justification=centering}
  \resizebox{35em}{10em}{\input{./tikz/diagrams/unfolding.tex}}
  \caption{The Schematic Unfolding/Unrolling of an Iterative Algorithm}
  \label{fig:deep_unfolding_unrolling}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%








\section{Wirtinger Flow Variants}
\ac{WF} variants emerged as a response to solve certain settings in \acl*{PR} problems. As the same suggests \acl*{PR} is the process of 
determining the phase (up to a global phase) of an image simply because it contains information which is of interest depending on the context. We first explain 
what phase is and why it is important by giving a synthetic example. Then we proceed to formulate mathematical formulation and in quick succession 
the variant where the \ac{WF} variants are based on. We also briefly explain the difficulties associated with the \acl*{PR} problems. 
 One application of the \ac{WF} variants is given which is used in imaging to both motivate the reasoning behind 
 considering complex numbers in our formulation and motivate the reader by giving a synthetic example based on 
natural images. 
\subsection{Phase and Its Importance}\label{sec:phase_importance}

Consider  $z= \operatorname{Re}(z)+\operatorname{Im}(z) \in \mathbb{C}$ that is written as:
\begin{equation}\label{eq:complex_polar_form}
  z = r \mathrm{e}^ {\mathrm{i}\theta}
\end{equation}
where $r = \sqrt{\operatorname{Re}(z)^2+\operatorname{Im}(z)^2}$ and $\theta \in [0,2\pi)$ is chosen in a way that 
$\cos \theta = \frac{\operatorname{Re}(z)}{\sqrt{\operatorname{Re}(z)^2+\operatorname{Im}(z)^2}}$ and 
$\sin = \theta \frac{\operatorname{Im}(z)}{\sqrt{\operatorname{Re}(z)^2+\operatorname{Im}(z)^2}}$ 
which is doable due to \cref{theorem:euler_formula} and a couple of elementary arithmetics. $\theta$, $r$ in \cref{eq:complex_polar_form} are called 
argument, absolute value of $z$ in math and phase, amplitude of $z$ in physics. Depending on the quantity of interest the phase of an object 
might contain valuable information. Here we would like to give a synthetic example to show the importance of the phase. In \cref{theorem:dft_idft_inverse} 
we pointed that The \ac{DFT} and the \ac{IDFT} (both 1D and 2D) are inverse to each other therefore taking the \ac{DFT} and then the \ac{IDFT} in 
succession on a 1D/2D signal theoretically must have no effect on the signal. Now take the 2D \ac{DFT} of two different natural images but 
before taking the 2D \ac{IDFT} of the natural images; swap the phase and then perform the 2D \ac{IDFT} to arrive at the two reconstructed images. This 
was done by \cite{Oppenheim1981} to show the importance of phase in signals. The result of the phase swap can be seen in \cref{image:phase_swap}. 
The first and the third images are the original images which from now on we call \textsc{Image1} and \textsc{Image2}. 
The second image is reconstructed using amplitude of the \textsc{Image2} and the phase of the \textsc{Image1} and the fourth image is reconstructed using 
the amplitude of the \textsc{Image1} and the phase of the \textsc{Image2} just so that there is no confusion of what is what. The images 
\emph{look like} the image with the corresponding phase which in a way shows that phase is more important than the amplitude. Different manipulations 
on the amplitude and the phase of the \ac{FT} during the reconstructed process 
 were also investigated by \cite{Oppenheim1981}. The reconstruction process is not always as easy as just taking the \ac{IDFT} and 
 is explained in more detail in \cite{Oppenheim1979} which in short is similar to the Gerchberg-Saxton algorithm\cite{Gerchberg1972}. 
 Other manipulations and and their effects on the reconstructed images were also conducted by \cite{Oppenheim1981} which are:
 \begin{itemize}
	\item taking the \ac{DFT}, setting the phase to zero and then reconstructing the image
  \item taking the \ac{DFT}, setting the amplitude to one and then reconstructing the image
	\item taking the \ac{DFT}, setting a random amplitude and then reconstructing the image
	\item taking the \ac{DFT}, setting the amplitude to the average of amplitudes of images of the same type and then reconstructing the image.
 \end{itemize} 
 All of them suggested that if you have the right phase then the reconstructed image have some level of resemblance to the original image \cite{Oppenheim1981}   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% Phase in Fourier Reconstruction %%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
  % \clearpage % Start a new page
  % \thispagestyle{empty} % No header/footer on this page
  \begin{figure}
    \centering
	\captionsetup{justification=centering}
    \includegraphics[width=1.0\textwidth,height=65em]{./images/phase_importance/phase_importance_comparison.png}
    \caption{The Importance of Phase in Fourier Reconstruction which was inspired by \cite{Oppenheim1981}}
    \label{image:phase_swap}
  \end{figure}
  % \clearpage % End the page
}   

\subsection{Phase Problem and Its Solution}
Sometimes the precious and coveted phase we talked about in \cref{sec:phase_importance} or other forms of it gets lost during a process. The physics behind the phase loss is beyond the scope of the current work, but 
in \cref{pro:phase_problem} we give a mathematical formula that explicitly shows the destruction of the phase. 
For the case where material limitations and quantum mechanical effects are responsible for the phase loss we invite 
the intrigued party to take a look at \cite{Shechtman2015} (material limitations) and \cite{DGDS2018}\cite{FranzSchwabl2007} (quantum mechanical effects). 
Mathematically the phase problem can be stated as:
\begin{Pro}[Phase Problem]\label{pro:phase_problem} For $G_j \in \mathbb{R}^M$ and matrices 
  $\boldsymbol{A}_j \in \mathbb{C}^{M \times N}$ for $j \in \left\{1,\ldots,K\right\}$, find 
$\psi \in \mathbb{C}^N$ such that either $G_j = \varphi(\boldsymbol{A}_j\psi)$ or $G_j \approx \varphi(\boldsymbol{A}_j\psi)$ where 
$\varphi \colon \mathbb{C} \to \mathbb{R}$ is a function with the phase destruction 
capability\footnote{Or in the more general case something that is not bijective and would result in phase corruption $\varphi \colon \mathbb{C} \to \mathbb{C}$ 
that partially destroys the information encoded in the phase.} like $z \rightarrow \left|z\right| \lor {\left|z\right|^2}$. 
\end{Pro} 

\noindent One way to approximately find the solution is to formulate it like:
\begin{Pro}[Phase Retrieval Problem]\label{pro:phase_retrieval_problem} For $G_j \in \mathbb{R}^M$ and matrices 
  $\boldsymbol{A}_j \in \mathbb{C}^{M \times N}$ for $j \in \left\{1,\ldots,K\right\}$, and 
  $\varphi \colon \mathbb{C} \to \mathbb{R}$ is a function with the phase destruction capability, then minimize:
  \begin{equation}\label{eq:phase_retrieval_problem}
    E(\psi) = \underbrace{\frac{1}{2KN} \sum_{j=1}^{K} {\left|\left|\phi(\boldsymbol{A}_j\psi)-G_j\right|\right|}^2}_{\coloneqq D(\psi)}+ R(\psi)
    \end{equation}
    where:
  \begin{itemize}
    \item $D(\psi)$ is the data term,
    \item $R(\psi)$ is the regularization term,
    \item $G_j$ is the $j$-th measurement.
    \end{itemize}
\end{Pro}

\noindent Solving the \acl*{PR} is not quite easy due to the non-convex and non-holomorphic characteristics of the \acl*{PR}. 
In short the \acl*{PR} can be solved by the iterative algorithm:
  \begin{equation}\label{eq:pr_solution}
	\psi^{k+1} = \text{prox}_{\tau_{k}R}(\psi^k-\tau_k\nabla{D(\psi^k)})
  \end{equation}

  with the gradient-like structure looks like: 
  \begin{equation}\label{eq:gradient_pr_solution}
	\nabla{D(\psi^k)} = \frac{1}{KN} \sum_{j=1}^{K} \boldsymbol{A}_j^*\left(\varphi\left(\boldsymbol{A}_j\psi\right)-G_j\right)\odot \varphi'(\boldsymbol{A}_j\psi).
  \end{equation}

  where:
  \begin{itemize}
	\item $\mathrm{prox}$ is the Proximal operator mostly used in non-differentiable but still convex setting \cite{Bredies2018}.
	\item $\odot$ is the Hadamard product\cite{Hackbusch2019} which is simply the element-wise product between two multidimensional arrays.
	\item $\boldsymbol{A}^*_j$ is the $j$-th conjugate matrix of the $j$-th matrix $\boldsymbol{A}_j$.
	\item $\varphi' \colon \mathbb{C} \to \mathbb{C}$ is the first derivative of the $\varphi \colon \mathbb{C} \to \mathbb{R}$ function.
  \end{itemize}

  A simpler variation of the above formulation were studied extensively and is at the core of the \ac{WF} variants. 
  From now on we consider the following as the \acl*{PR} problem:

  \begin{Pro}\label{pro:phase_retrieval_simple}
    Given $\boldsymbol{y} \in \mathbb{R}^M$ the measurement vector and $\boldsymbol{A} \in \mathbb{C}^{M \times N}$ the sampler matrix, 
    and the phase destroyer $\varphi \colon \mathbb{C} \to \mathbb{R}$; find $\boldsymbol{x} \in \mathbb{C}^N$ up to a global phase 
    by minimizing $ \left|\left|\varphi(\boldsymbol{A}\boldsymbol{x})-\boldsymbol{y}\right|\right|$.
  \end{Pro}
  \begin{Rem}
The function we are trying to minimize in \cref{pro:phase_retrieval_simple} is non-convex \cite{Candes2014}. Set $n=1$, $m=2$, $\boldsymbol{x}_1 = \begin{pmatrix}1+i\end{pmatrix}^{1 \times 1}$, 
$\boldsymbol{x}_2 = \begin{pmatrix}-1-i\end{pmatrix}^{1 \times 1}$, $\boldsymbol{A}=\begin{pmatrix}1\\i \end{pmatrix}^{2 \times 1}$, 
$\boldsymbol{y}=\begin{pmatrix}1\\2 \end{pmatrix}^{2 \times 1}$, and $\lambda=1/2$ to build a counterexample. Non-convexity is bad news for 
optimization as most of the optimization methods are build on top of convexity \cite{Boyd2004}\cite{Nocedal2006}.
  \end{Rem}
  \begin{Rem}
    The function we are trying to minimize is not holomorphic\footnote{Cauchy-Riemann equations do not hold. 
    You can take a look at the eleventh chapter of \cite{Rudin1987} for further details.} which in turn makes arriving at the 
    gradient descent like structure in \cref{eq:gradient_pr_solution} more involved.
  \end{Rem}
  \begin{Rem} 
    $\min \left|\left|\boldsymbol{x}-\boldsymbol{z}\mathrm{e}^{-\mathrm{i}\theta}\right|\right|^2$ is chosen as the difference between the 
    true signal and the approximated one and the reason is:
    \begin{equation}
      \left|\boldsymbol{A}\boldsymbol{x}\mathrm{e}^{\mathrm{i}\theta}\right| = \left|\boldsymbol{A}\boldsymbol{x}\right|
    \end{equation}
    where $\left|\boldsymbol{.}\right|$ is the absolute value applied element-wise. This would be the expression you encounter throughout the \ac{WF} variant papers \cite{Candes2014}\cite{Chen2015}\cite{Zhang2016}. 
    This remark is also important because in the training process of the \du you have to use this costume loss function which is why we give the explicit formulation 
    for finding $\min \left|\left|\boldsymbol{x}-\boldsymbol{z}\mathrm{e}^{-\mathrm{i}\theta}\right|\right|^2$.
  \end{Rem}






	\begin{Prop}\label{theorem:min distance}
		Let $\varphi, \theta \in \mathbb{R}$, $x,z \in \mathbb{C}^n$, $w \coloneqq \langle z,x \rangle$ such that 
    $\cos\theta = \frac{\operatorname{Re}(w)}{\sqrt{\operatorname{Re}(w)^2+\operatorname{Im}(w)}}$ and $\sin\theta = \frac{\operatorname{Im}(w)}{\sqrt{\operatorname{Re}(w)^2+\operatorname{Im}(w)^2}}$;  where the scalar product $\langle z,x \rangle$ is the same as the one in \cref{def:scalar_product_complex_vectors} 
    and would induce the norm $\left|\left|\cdot\right|\right|$ as in \cref{def:scalar_product_induced_norm_complex_vectors}; then the function:
		\begin{equation}\label{eq:loss_function}
			f \colon \mathbb{R}\to\mathbb{R} \qquad , \qquad \varphi\mapsto \left|\left|x-\mathrm{e}^{-\mathrm{i}\varphi}z\right|\right|^2
		\end{equation}
		has a minimum of $\left|\left|x-\mathrm{e}^{-\mathrm{i}\varphi^*}z\right|\right|^2$ where:
    \begin{equation}
      \varphi* =
      \begin{cases}
			  \theta + \pi & 0   \leq \theta <  \pi\\
			  \theta - \pi & \pi \leq \theta < 2\pi\\
		  \end{cases}
    \end{equation}
  \end{Prop}
		\begin{Proof}
			$f$ is continuous and periodic with the period of $2\pi$ therefore it will attain its minimum and maximum 
      for $\varphi^\ast \in [0,2\pi)$\cite{Rudin1976}. 
			
      
      

      
      
      
      
      
			\begin{equation}
				\begin{split}
				f(\varphi) &= \left|\left|x-\mathrm{e}^{-\mathrm{i}\varphi}z\right|\right|^2 = 
				\langle x-\mathrm{e}^{-\mathrm{i}\varphi}z,x-\mathrm{e}^{-\mathrm{i}\varphi}z \rangle\\
						   &= \langle x,x \rangle - \mathrm{e}^{\mathrm{i}\varphi}\langle x,z \rangle-\mathrm{e}^{-\mathrm{i}\varphi}\langle z,x \rangle+\langle z,z \rangle \\
               &= \langle x,x \rangle - (\cos\varphi+\mathrm{i}\sin\varphi)\left(\operatorname{Re}(w) -\mathrm{i}\operatorname{Im}(w)\right)\\
						   &\;\;\;\;\; + \langle z,z \rangle - (\cos(\varphi)-\mathrm{i}\sin\varphi)\left(\operatorname{Re}(w)+\mathrm{i}\operatorname{Im}(w)\right)\\
						   &= \langle x,x \rangle - 2\left(\cos\varphi\operatorname{Re}(w)+\sin\varphi\operatorname{Im}(w)\right)+ \langle z,z \rangle
				\end{split}
			  \end{equation}
			  Utilizing optimization methods for continuous and differentiable $\mathbb{R} \rightarrow \mathbb{R}$ functions \cite{Boyd2004}\cite{Nocedal2006} and also \cref{lemma:inverse_a_sin_b_cos}:
			  \begin{equation}
				\begin{split}
				\frac{\mathrm{d}f(\varphi)}{\mathrm{d}\varphi} &= - 2\left(-\sin\varphi\operatorname{Re}(w)+\cos\varphi\operatorname{Im}(w)\right) = 2\sqrt{\operatorname{Re}(w)^2+\operatorname{Im}(w)^2}\sin(\varphi-\theta)\\ 
				\end{split}
			  \end{equation}
			  \begin{equation}
				\begin{split}
				\frac{\mathrm{d}^2f(\varphi)}{\mathrm{d}\varphi^2} &= -2\sqrt{\operatorname{Re}(w)^2+\operatorname{Im}(w)^2}\cos(\varphi-\theta)\\ 
				\end{split}
			  \end{equation}
        We look for the points where the first derivative is zero which translates into:
        \begin{equation}
          \varphi^* - \theta = k\pi \quad , \quad k \in \mathbb{Z} 
        \end{equation}
        and the second derivative is positive which translates into:
        \begin{equation}
         2k\pi + \frac{\pi}{2} \leq \varphi^* - \theta \leq 2k\pi + \frac{3\pi}{2} \quad , \quad k \in \mathbb{Z}
        \end{equation}
        combining the two would give:
        \begin{equation}
          \varphi^* -\theta = (2k+1)\pi, k \in \mathbb{Z}
        \end{equation}
        for $0   \leq \theta <  \pi$, $k=0$ would place $\varphi*$ in $[0,2\pi)$ and for  $\pi \leq \theta < 2\pi$, 
        $k=-1$ would place $\varphi*$ in $[0,2\pi)$ and to summarize minimum will be attained by:
        \begin{equation}
          \varphi* =
          \begin{cases}
            \theta + \pi & 0   \leq \theta <  \pi\\
            \theta - \pi & \pi \leq \theta < 2\pi.\\
          \end{cases}
        \end{equation}
        Since we could only find one local minimum and we already know there exist a global minumum therefore this local minimum is in fact the global minimum.
		\end{Proof}


  




\subsection{Motivating Application}\label{sec:motivating_application}

Since introducing complex numbers introduced a couple of quite difficult obstacles in the math part and will introduce others in the implementation part 
we think we owe it to the reader to motivate this particular assumption by giving one application which is imaging using \ac{CDP} coupled with \ac{WF} variants.

\subsubsection{Diffraction Imaging}\label{sec:diffraction_imaging}

Without going too much into the Physics of it Consider a ray that is emitted onto an object of interest and the diffracted rays are collected and 
measured at some distance from the object. The goal is to reconstruct the image using the said measurements. If the distance between 
the sample and the detector is far enough, characterized by the Fraunhofer condition\cite{Lipson1995}, then the solution of the diffraction 
problem is well approximated by the \ac{FT} of the emitted ray. Due to quantum mechanical effects\cite{DGDS2018}\cite{FranzSchwabl2007} and material limitations\cite{Shechtman2015} 
the phase of the \ac{FT} can not be measured (we can only measure the amplitude of the \ac{FT}) therefore we encounter the \pp problem in \cref{pro:phase_problem}. 
In summary in the discrete 1D case we have:
\begin{equation}
	y_k = \left| \frac{1}{N}\sum_{n=0}^{N-1} x_n e^{-2\pi\mathrm{i}nk} \right|^2 , \quad \boldsymbol{x} \in \mathbb{C}^N , \quad \boldsymbol{y} \in \mathbb{R}^M, \quad M > N 
\end{equation}
where $\boldsymbol{x} \in \mathbb{C}^N$ is the emitted ray and $\boldsymbol{y} \in \mathbb{R}^M$ represent the measurements. By some physics technics\cite{Loewen2018}\cite{Candes2011} you can modulate 
the ray before it diffracts which in 1D discrete setting becomes:
\begin{equation}\label{eq:modulation_effect}
	y_k = \left| \frac{1}{N}\sum_{n=0}^{N-1} x_n\overline{d_n} e^{-2\pi\mathrm{i}nk} \right|^2 , \quad \boldsymbol{x}, \boldsymbol{d} \in \mathbb{C}^N , \quad \boldsymbol{y} \in \mathbb{R}^M, \quad M > N 
\end{equation}

If we do the process using $L$ different modulations/masks we would be having:
\begin{equation}\label{eq:l_modulation_effect}
	y_{l,k} = \left| \frac{1}{N}\sum_{n=0}^{N-1} x_n\overline{d_n} e^{-2\pi\mathrm{i}nk} \right|^2, \quad \begin{split}
	0 &\leq k \leq N-1\\
	1 &\leq l \leq L
	\end{split}
  , \quad \boldsymbol{x}, \boldsymbol{d} \in \mathbb{C}^N , \quad \boldsymbol{y} \in \mathbb{R}^M, \quad M > N 
  .
  \end{equation}
\cite{ECXLMS2013} proved that if the number of modulations/masks $L$ is high enough, then the recovery of the ray is possible using \acl*{PR} methods. 
\cite{Gross2017} furthered improved the bounds in which the recovery is possible.
There are however some restrictions regarding the $\boldsymbol{d} \in \mathbb{C}^N$ in \cref{eq:modulation_effect} and \cref{eq:l_modulation_effect}. \cite{ECXLMS2013} proposed:
\begin{equation}
	\mathbb{E}\left[d\right] = 0, \qquad \mathbb{E}\left[d^2\right] = 0, \qquad\mathbb{E}\left[\left|d\right|^4\right] = 2\mathbb{E}\left[\left|d\right|^2\right]^2
\end{equation}
where $\mathbb{E}$ is the expectation value operator and $d$ the random variable, as the admissibility condition and proposed two settings where the conditions are met:
\begin{itemize}
	\item first setting nicknamed \emph{Ternary Modulation} as there are 3 different values(outcomes) for the random variable $d$ which are:
	\begin{equation}
	  d =
		  \begin{cases}
			  +1 & \text{with prob.  $1/4$}\\
			  0 & \text{with prob.  $1/2$}\\
			  -i & \text{with prob.  $1/4$}
		  \end{cases}  
	\end{equation}
	\item second setting nicknamed \emph{Octanary Modulation} as there are 8 different values(outcomes) for the random variable $d$ which are:
	\begin{equation}\label{eq:octanary_modulation}
		d = b_1b_2 \qquad \text{such that} \qquad
	  b_1 =
		  \begin{cases}
			  +1 & \text{with prob.  $1/4$}\\
			  -1 & \text{with prob.  $1/4$}\\
			  -i & \text{with prob.  $1/4$}\\
			  +i & \text{with prob.  $1/4$}\\
	
		  \end{cases}  
		  \qquad \text{and} \qquad 
	  b_2 
		  \begin{cases}  
			+\sqrt{2}/2 & \text{with prob.  $4/5$}\\
			+\sqrt{3} & \text{with prob.  $1/5$}\\
		\end{cases}   
	\end{equation}
\end{itemize}

\begin{Rem}
It is stated by \cite{ECXLMS2013} different \acl*{CDP}s might be better suited for different applications so naturally 
the possibility exists that the practitioners in their respective field want to experiment with different $d$s. Therefore 
we provide the double checking of the admissibility conditions for the \emph{Octanary Modulation} proposed by \cite{Candes2014} 
in case the reader is in fact one of those practitioners.
\end{Rem}

\begin{Thm}\label{theorem:expectation_general}
  Let $X$ and $Y$ be independent discrete random variables. For expectation value of functions of our random variables we would be having:
\begin{equation}
  \mathbb{E}(f(X,Y)) = \sum_{X=x}^{}\sum_{Y=y}^{}f(x,y)p(x)p(y)
\end{equation}
  where $p(x)$ and $p(y)$ are the probability of $x$ and $y$ being realized from the random variables $X$ and $Y$.
\end{Thm}
% \begin{Proof}
  % The general case where $X$ and $Y$ are not independent is usually discussed. Then by assuming an independent setting 
  % you will arrive at the conclusion \cite{DasGupta2010}\cite{DasGupta2011}.
% \end{Proof}
\begin{Lem}\label{lemma:expectation_multiplication}
Let $X$ and $Y$ be independent discrete random variables. We would be having:
\begin{equation}
  \mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)
\end{equation}
\end{Lem}

\begin{Prop}
	The Octanary Modulation setting in \cref{eq:octanary_modulation} is admissible by the criteria proposed by \cite{Candes2014}. 
\end{Prop}

\begin{Proof}
	By using \cref{lemma:expectation_multiplication} and \cref{theorem:expectation_general} we have:
	\begin{equation*}
		\begin{split}
		E(d) &= \left(+1 \times \frac{\sqrt{2}}{2}\right) \times \frac{4}{20} + \left(+1 \times \sqrt{3}\right) \times \frac{1}{20}+\left(-1 \times \frac{\sqrt{2}}{2}\right) \times \frac{4}{20}+\left(-1 \times \sqrt{3}\right) \times \frac{1}{20}\\
		     &+ \left(+i \times \frac{\sqrt{2}}{2}\right) \times \frac{4}{20} + \left(+i \times \sqrt{3}\right) \times \frac{1}{20}+\left(-i \times \frac{\sqrt{2}}{2}\right) \times \frac{4}{20}+\left(-i \times \sqrt{3}\right) \times \frac{1}{20}\\ 
			 &= 0
		\end{split}
	  \end{equation*}
	  \begin{equation*}
		\begin{split}
		E(d^2) &= \left(+1 \times \frac{1}{2}\right) \times \frac{4}{20} + \left(+1 \times 3\right) \times \frac{1}{20}+\left(+1 \times \frac{1}{2}\right) \times \frac{4}{20}+\left(+1 \times 3\right) \times \frac{1}{20}\\
		     &+ \left(-1 \times \frac{1}{2}\right) \times \frac{4}{20} + \left(-1 \times 3\right) \times \frac{1}{20}+\left(-1 \times \frac{1}{2}\right) \times \frac{4}{20}+\left(-1 \times 3\right) \times \frac{1}{20}\\ 
			 &= 0
		\end{split}
	 \end{equation*}
	 \begin{equation*}
		\begin{split}
		E(\left|d\right|^2) &= \left(\left|+1 \times \frac{\sqrt{2}}{2} \right|\right)^2\times \frac{4}{20} + \left(\left|+1 \times \sqrt{3}\right|\right)^2\times \frac{1}{20}\\
		                    &+\left(\left|-1 \times \frac{\sqrt{2}}{2}\right|\right)^2\times \frac{4}{20}+\left(\left|-1 \times \sqrt{3}\right|\right)^2\times \frac{1}{20}\\
		                    &+ \left(\left|+i \times \frac{\sqrt{2}}{2}\right|\right)^2\times \frac{4}{20} + \left(\left|+i \times \sqrt{3}\right|\right)^2\times \frac{1}{20}\\
							&+\left(\left|-i \times \frac{\sqrt{2}}{2}\right|\right)^2\times \frac{4}{20}+\left(\left|-i \times \sqrt{3}\right|\right)^2\times \frac{1}{20}\\ 
			 &= 1
		\end{split}
	  \end{equation*}
	  \begin{equation*}
		\begin{split}
		E(\left|d\right|^4) &= \left(\left|+1 \times \frac{\sqrt{2}}{2}\right|\right)^4 \times \frac{4}{20} + \left(\left|+1 \times \sqrt{3}\right|\right)^4 \times \frac{1}{20}\\
		                    &+\left(\left|-1 \times \frac{\sqrt{2}}{2}\right|\right)^4 \times \frac{4}{20}+\left(\left|-1 \times \sqrt{3}\right|\right)^4 \times \frac{1}{20}\\
		                    &+ \left(\left|+i \times \frac{\sqrt{2}}{2}\right|\right)^4 \times \frac{4}{20} + \left(\left|+i \times \sqrt{3}\right|\right)^4 \times \frac{1}{20}\\
							&+\left(\left|-i \times \frac{\sqrt{2}}{2}\right|\right)^4 \times \frac{4}{20}+\left(\left|-i \times \sqrt{3}\right|\right)^4 \times \frac{1}{20}\\ 
			 &= 2
		\end{split}
	 \end{equation*}
	 which closes the proof.
\end{Proof}

\subsubsection{Synthetic Example}
If you want to take a natural image you just pick up your digital camera and take a picture but for the sake of the argument assume that is not possible to do so. 
Assume the scenery you want to take an image of has to be handled by diffraction imaging and inverse methods. Prepare $L$ different masks using the criteria in \cref{eq:octanary_modulation}, 
apply them to your natural digital image that you have according to the \cref{eq:l_modulation_effect} to arrive at the synthetic measurements. In our case where our image 
is $1,000 \times 553$ we used $20$ masks for retrieving the original image from the measurements. We retrieved the original image using three different \ac{WF} variants namely 
the original \ac{WF}, the \ac{TWF}, and the \ac{RWF} for demonstration purposes. The relative error in different color channels during the retrieval process is 
depicted in \cref{fig:cdp_wf_twf_rwf}. What is quite interesting is that the measurements induced by only one mask shows almost nothing that can resemble the characteristics 
of the original image but when $L$ masks are applied somehow magically you can now retrieve the original image quite perfectly. 
\cref{image:cdp_effect_fourier_visual} shows that when a mask is applied the usual star like pattern that can be seen in the spectral visualization of the 
\ac{FT} of an image is completely destroyed. \cref{image:cdp_effect_measurements_zoomed_visual} shows a zoomed version to have a closer 
look at the structure after the mask is applied which again shows no apparent pattern. During the reconstruction process we saved all reconstructed images at each iteration, but 
obviously it is not possible to look at all of them within the current document. It is however informative to have a couple of them to both have a sense 
of the iteration process and also compare the three algorithms side by side visually. The \ac{WF} vs the \ac{TWF} can be seen in \cref{image:wf_vs_twf}. It can be 
easily be seen that the \ac{TWF} is a superior algorithm both at the initialization and also is better at reconstructing the image at equal iteration number. For 
the \ac{WF} vs \ac{RWF} in \cref{image:wf_vs_rwf} both seen to be equal when it comes to initialization step but the \ac{RWF} quickly starts to reconstruct better 
images compared to the \ac{WF}. The case of \ac{TWF} vs \ac{RWF} in \cref{image:twf_vs_rwf} is more interesting as at first \ac{TWF} is in the lead regarding the 
quality of the reconstructed image at the initialization step but the \ac{RWF} manages to reconstruct almost the perfect, relative error of $10^{-16}$, image at only $150$ iteration 
while it take the \ac{TWF} $200$ iterations. A a side not each iteration approximately is the equivalent of two \textsc{FFT}s on a grid of $1,000 \times 553$ for each color channel which is a lot. 
That shows the importance of looking for better and better algorithms in our case and in the general case of \acl*{PR}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% WF, TWF, RWF for CDPs Retrieval of Natural Images %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
%   \clearpage % Start a new page
  \begin{figure}
    \subfloat[Natural Image Retrieval Using  \ac{CDP}s Combined with \ac{WF}]{\input{./tikz/wf/wf_error_sat_phone_500_sat_phone.tex}}\\  
    \subfloat[Natural Image Retrieval Using  \ac{CDP}s Combined with \ac{TWF}]{\input{./tikz/twf/twf_error_sat_phone_500_sat_phone.tex}}\\  
    \subfloat[Natural Image Retrieval Using  \ac{CDP}s Combined with \ac{RWF}]{\input{./tikz/rwf/rwf_error_sat_phone_500_sat_phone.tex}}\\  
  \caption{Natural Image Retrieval Using  \ac{CDP}s Combined with \ac{WF}, \ac{TWF}, and \ac{RWF}}\label{fig:cdp_wf_twf_rwf}
  \end{figure}
%   \clearpage % End the page
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%% CDPs on Fourier visualization %%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
  % \clearpage % Start a new page
  \thispagestyle{empty} % No header/footer on this page
  \begin{figure}
    \centering
	\captionsetup{justification=centering}
    \includegraphics[width=1.0\textwidth,height=65em]{./images/mask_effect/regular_fourier_vs_modulated.png}
    \caption{Effect of Modulation Due to a Random Mask on Specteral Visualization of the Sat Phone Image from Top to Buttom: 
    Red Channel, Green Channel, Blue Channel, and Full RGB}\label{image:cdp_effect_fourier_visual}
  \end{figure}
  % \clearpage % End the page
}
\afterpage{%
  % \clearpage % Start a new page
  \thispagestyle{empty} % No header/footer on this page
  \begin{figure}
    \centering
	\captionsetup{justification=centering}
    \includegraphics[width=1.0\textwidth,height=65em]{./images/coded_diffractions_measurements_sat_phone/measurements.png}
    \caption{Measurements in Diffractive Imaging Resulted from the Sat Phone Image Due to a Random Modulation Plate from Top to Buttom: 
    Red Channel, Green Channel, Blue Channel, and Full RGB}\label{image:cdp_effect_measurements_visual}
  \end{figure}
  % \clearpage % End the page
}
\afterpage{%
  % \clearpage % Start a new page
  \thispagestyle{empty} % No header/footer on this pages
  \begin{figure}
    \centering
	\captionsetup{justification=centering}
    \includegraphics[width=1.0\textwidth,height=65em]{./images/coded_diffractions_measurements_zoomed_sat_phone/measurements.png}
    \caption{Measurements in Diffractive Imaging Resulted from the Sat Phone Image Due to a Random Modulation Plate from Top to Buttom: 
    Red Channel, Green Channel, Blue Channel, and Full RGB Zoomed Version}\label{image:cdp_effect_measurements_zoomed_visual}
  \end{figure}
  % \clearpage % End the page
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% WF Variants CDP Reconstruction Comparison %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
  % \clearpage % Start a new page
  \thispagestyle{empty} % No header/footer on this page
  \begin{figure}
    \centering
    % \captionsetup{justification=centering}
    \includegraphics[width=1.0\textwidth]{./images/cdp/out_wf_twf.png}
  \caption{\ac{WF}(left) vs \ac{TWF}(right) Using Coded Diffraction Patterns for Retrieval of the Sat Phone Image from Top to Buttom: After Initialization, 
	at Iteration $=120$, at Iteration $=350$ for \ac{WF} and at Iteration $=200$ for \ac{TWF}, and the Original Image}
  \label{image:wf_vs_twf}
  \end{figure}
  % \clearpage % End the page
}
\afterpage{%
  % \clearpage % Start a new page
  \thispagestyle{empty} % No header/footer on this page
  \begin{figure}
    \centering
    % \captionsetup{justification=centering}
    \includegraphics[width=1.0\textwidth]{./images/cdp/out_wf_rwf.png}
  \caption{\ac{WF}(left) vs \ac{RWF}(right) Using Coded Diffraction Patterns for Retrieval of the Sat Phone Image from Top to Buttom: After Initialization, 
	at Iteration $=120$, at Iteration $=350$ for \ac{WF} and at Iteration $=150$ for \ac{RWF}, and the Original Image}
  \label{image:wf_vs_rwf}
  \end{figure}
  % \clearpage % End the page
}
\afterpage{%
  % \clearpage % Start a new page
  \thispagestyle{empty} % No header/footer on this page
  \begin{figure}
    \centering
    % \captionsetup{justification=centering}
    \includegraphics[width=1.0\textwidth]{./images/cdp/out_twf_rwf.png}
  \caption{\ac{TWF}(left) vs \ac{RWF}(right) Using Coded Diffraction Patterns for Retrieval of the Sat Phone Image from Top to Buttom: After Initialization, 
	at Iteration $=25$, at Iteration $=200$ for \ac{TWF} and at Iteration $=150$ for \ac{RWF}, and the Original Image}
  \label{image:twf_vs_rwf}
  \end{figure}
  % \clearpage % End the page
}

\subsection{Wirtinger Flow}

The whole thing about \ac{WF} variants started with the seminal work of \cite{Candes2014}. They were designed to solve the \cref{pro:phase_retrieval_simple}. 
\cite{Candes2014} coined the the term \ac{WF} due to the use of \emph{Wirtinger Derivatives}\cite{Wirtinger1927} to reach \cref{eq:pr_solution} from \cref{eq:phase_retrieval_problem}. They 
also put some assumptions on $\boldsymbol{x} \in \mathbb{C}^N$, and $\boldsymbol{A} \in \mathbb{C}^{M \times N}$ so that they can 
come up with guarantees on the convergence. Due to their small memory footprint and easy implementation they are currently the center of attention as one of the 
most important method for phase retrieval.   
The \ac{WF} variants have two stages:
\begin{itemize}
  \item Initialization in order to try and be where the loss function is convex.
  \item Iteration loop that uses the gradient like structure in \cref{eq:gradient_pr_solution} to get a reduction in the 
  loss function and hopefully converge to the desired solution. 
\end{itemize}

Checking the guarantees and the theoretical results is involved and also not the center of the current study but they can be found in their respective papers. 
The most important improvements on the \ac{WF} were done chronologically by \cite{Chen2015}, \cite{Kolte2016}, and \cite{Zhang2016}
with the nicknames of \ac{TWF}, \ac{ITWF}, \ac{RWF}, \ac{IRWF}, and \ac{IMRWF}. To get the feeling of their performance compared to each other we implemented 
all of them and tested them on $\boldsymbol{x} \in \mathbb{C}^N$, and $\boldsymbol{A} \in \mathbb{C}^{M \times N}$ 
where $N = 64, M = 640$ and the samples were taken from the normal distribution centered at zero with the standard deviation of one. 
The results can be seen in \cref{fig:wf_variants}. For a quite extensive survey on \ac{WF} variants please refer to \cite{Liu2019}. \cite{Chandra2017} 
gathered quite number of \emph{Phase Retrieval} methods including a couple of \emph{\ac{WF}} variants in the MATLAB\textregistered\space 
problem solving environment in a uniform manner. The summary for all the variants in terms of formulation is in table\ref{tab:formulation}  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% WF vs TWF vs RWF vs IRWF vs IMRWF  %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \afterpage{%
%   \clearpage % Start a new page
\begin{figure}
  \centering
\captionsetup{justification=centering}
% \begin{turn}{-90}
  \input{tikz/wf_variants.tex}
% \end{turn}
\caption{\ac{WF} vs \ac{TWF} vs \ac{RWF} vs \ac{IRWF} vs \ac{IMRWF}}
  \label{fig:wf_variants}
\end{figure}
%   \clearpage % End the page
% }






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%% Summary of WF* variants %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}
	\centering
	\begin{tabular}{||c l c||} 
	 \hline
	 \ac{WF} Variant & $\varphi$ 						& loss functions\\ [0.5ex] 
	 \hline\hline
	 \ac{WF}\index{WF}                & $\left|\boldsymbol{z}\right|^2$ 	& quadratic 	\\
	 \ac{TWF}\index{TWF}   & $\left|\boldsymbol{z}\right|^2$ 	& quadratic 	\\
	 \ac{ITWF}\index{ITWF}  & $\left|\boldsymbol{z}\right|^2$   & quadratic 	\\
	 \ac{RWF}\index{RWF}  & $\left|\boldsymbol{z}\right|$ 	& quadratic 	\\
	 \ac{IRWF}\index{IRWF}   & $\left|\boldsymbol{z}\right|$ 	& quadratic 	\\
	 \ac{IMRWF}\index{IMRWF}   & $\left|\boldsymbol{z}\right|$ 	& quadratic 	\\ [1ex]
	 \hline
	\end{tabular}
	\caption{$\varphi$ and the loss function used in \cite{Candes2014}, \cite{Chen2015}, \cite{Kolte2016}, and \cite{Zhang2016}}
	\label{tab:formulation}
	\end{table}


\section{Implementation}

As mentioned previously \du looks like the \rnns but it is not therefore it is not possible to use the higher level functions of the usual 
\ml/\dl frameworks\footnote{At the time of writing \tensorflow\cite{Abadi2016}, \keras\cite{Chollet2023}, and \pytorch\cite{Paszke2019} are the 
main players in the \ml/\dl world.}. To put it explicitly there is no:
\begin{itemize}
	\item real-time visualization of the training process vs epochs\footnote{Which is very nice feature to have and makes it easy to stop the training process 
  and save the 
  model the moment you feel that the loss function on the train and the test data are deviating from each other}.
  \item summary of trainable/non-trainable parameters
	\item graph based visualization\footnote{\tensorflow accomplishes this for the already defined layers using the \graphviz package.} of the dependency of the parameters in the layers.
	\item high level splitting and sampling mechanisms
	\item automatic \ac{GPU} acceleration. 
\end{itemize}

There are also other difficulties coming from the numerics alone:
\begin{itemize}
	\item suggested learning rates are mostly safe to use with known architectures
	\item when input and outputs are not normalized according to the common practices, tuning the learning rate and in turn training becomes more difficult and it is not possible to normalize them since they are coming from context inspired algorithms.
	\item the introduction of \ac{CVNN}\cite{CTOBYZDSSSJFSSMNRYBCP2017}\cite{Bassey2021}\cite{Barrachina2023} hinders what is under the hood in the \ml/\dl frameworks.
\end{itemize}

While both tuning the pseudo learning rate is possible through more trial and error compared to the known architectures and 
writing the helping tools and even the \ac{GPU} acceleration is not that difficult and can be done single-handedly
\footnote{Fran\c{c}ois Chollet wrote \keras\cite{Chollet2023} on top of \tensorflow\cite{Abadi2016} all be himself which later was acquired by \google.}, the \ac{CVNN} is another beast. 
One of the beating hearts of \ml/\dl frameworks is \ad/\cd. It is worth emphasizing that \ac{AD}/\ac{CD} is not symbolic differentiation and there are fundamental differences between the two \cite{Naumann2011}\cite{Griewank2008}. 
The difficulty of \ac{CVNN}\cite{CTOBYZDSSSJFSSMNRYBCP2017}\cite{Bassey2021}\cite{Barrachina2023} is directly coming from the \ac{AD}/\ac{CD} part 
that is used during backpropagation. The funny thing is that the general implementation of \ac{CVNN} requires the understanding of the 
\emph{Wirtinger Derivatives}\cite{Wirtinger1927} which \cite{Candes2014} payed homage to by including the \ac{WF} name in the title of their paper \cite{Candes2014}. 
For theory and implementation of \ac{CVNN} please refer to \cite{Wirtinger1927}\cite{KreutzDelgado2009}. For this sole reason 
we used \pytorch\cite{Paszke2019} as our frameworks as it seamlessly supports \ac{CVNN} using its flexible \textsc{Autograd}\cite{Paszke2019}. For our final remarks 
on implementation we would like to give some tips that might come in handy:
\begin{itemize}
  \item In the initialization of the \ac{WF} calculating the biggest eigenvalue is part of the process. That is usually done using the Power iteration which is 
  described in almost any standard numerical linear algebra book like \cite{Trefethen2022}. 
  \item If vectorized operations are still a thing in the future, try to formulate the algorithm in the most vectorized form possible in whatever 
  numerical framework you are using. For a first exposure to vectorized operations you can have a look at \cite{Hager2010}. For in depth understanding of 
  computer architecture that leads to the vectorized operations please refer to either \cite{Patterson2014} or \cite{Hennessy2019}.
  \item Try to port as much as computation possible from \ac{CPU} to accelerators like \ac{GPU}s and \ac{TPU}s if 
  your analysis on the porting confirms it to be worthwhile.
\end{itemize}

We give the algorithms and the code that is typesetted directly from the source code as to minimize the chance of mistake and give you a head start in case you wanted to do some experiments yourself. 
The algorithms/codes for the \ac{WF}, the \ac{TWF}, the \ac{RWF}, the \ac{IRWF}, and the \ac{IMRWF} are given in \cref{pseudocode:wf}/\cref{code:wf}, \cref{pseudocode:twf}/\cref{code:twf}, 
\cref{pseudocode:rwf}/\cref{code:rwf}, \cref{pseudocode:irwf}/\cref{code:irwf}, and \cref{pseudocode:imrwf}/\cref{code:imrwf} respectively. For the rest that 
goes into making a \ml/\dl and training process with \pytorch we recommend \pytorch documentation as it is rich in content with an active forum. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% WF Variants Algorithms %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% Wirtinger Flow %%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
\clearpage % Start a new page
\begin{algorithm}
  \caption{\ac{WF}\index{WF} suggested by \cite{Candes2014}}\label{pseudocode:wf}
    \textbf{Input}: Let $\boldsymbol{y}=\{y_i\}_{i=1}^m$ and $\{\boldsymbol{a}_i\}_{i=1}^m$ be our measurements and sampling vectors; \\
    \textbf{Parameters}:  step size $\mu$;\\
    \textbf{Initialization}: Let $\hat{\boldsymbol{z}}$, be the eigenvector corresponding to the largest eigenvalue of:
    \begin{equation}
      \boldsymbol{Y} \coloneqq \frac{1}{m}\sum_{i=1}^m y_i\boldsymbol{a}_i \boldsymbol{a}_i^*\boldsymbol{1}
    \end{equation}
    and find it using power iteration. Let also $\Lambda=\sqrt{n\sum_{i=1}^m \frac{y_i}{\|\boldsymbol{a}_i\|_X^2}}$ to be the norm estimation of the signal of interest that we are trying to recover. 
    Set $\boldsymbol{z}^{(0)}=\Lambda\hat{\boldsymbol{z}}$ for the initialization.\\
    \textbf{Update loop}: for $t=0, \ldots ,t=T-1$ do the update as:
    \begin{flalign}
      \boldsymbol{z}^{(t+1)}=\boldsymbol{z}^{(t)}- \frac{\mu_{t+1}}{\left|z_0\right|_X^2}\left(\frac{1}{m}\sum_{i=1}^{m}\left(\left|\boldsymbol{a}_i^*\boldsymbol{z}^{(t)}\right|_X-y_i\right)\left(\boldsymbol{a}_i\boldsymbol{a}_i^*\right)\boldsymbol{z}^{(t)}\right).
    \end{flalign}
    \textbf{Output}: $\boldsymbol{z}^{(T)}$.
  \end{algorithm}
  A possible \pytorch\cite{Paszke2019} implementation with \ac{CUDA}\cite{Nvidia} acceleration is listed below as:
  % \lstinputlisting[language=Python, firstline=1,lastline=20]{./algorithms/wf.py}
  \lstinputlisting[language=Python]{./algorithms/wf.py}\index{WF}\label{code:wf}    
\clearpage % End the page
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% Truncated Wirtinger Flow %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
\clearpage % Start a new page
\begin{algorithm}
  \caption{\ac{TWF}\index{TWF} suggested by \cite{Chen2015}}\label{pseudocode:twf}
    \textbf{Input}: Let $\boldsymbol{y}=\{y_i\}_{i=1}^m$ and $\{\boldsymbol{a}_i\}_{i=1}^m$ be our measurements and sampling vectors; \\
    \textbf{Parameters}: step size $\mu$, and thresholds $\alpha_{z}^{lb},\alpha_{z}^{ub},\alpha_{h},\alpha_{y}$;\\
    \textbf{Initialization}: Let $\hat{\boldsymbol{z}}$, be the eigenvector corresponding to the largest eigenvalue of:
    \begin{equation}
      \boldsymbol{Y} \coloneqq \frac{1}{m}\sum_{i=1}^m y_i\boldsymbol{a}_i \boldsymbol{a}_i^*\boldsymbol{1}_{\left\{\left|y_i\right|_X\leq \frac{\alpha_{y}^2\sum_{i=1}^{m}y_i}{m}\right\}}
    \end{equation}
    and find it using power iteration. Let also $\Lambda=\sqrt{n\sum_{i=1}^m \frac{y_i}{\|\boldsymbol{a}_i\|_X^2}}$ to be the norm estimation of the signal of interest that we are trying to recover. 
    Set $\boldsymbol{z}^{(0)}=\Lambda\hat{\boldsymbol{z}}$ for the initialization.\\
    \textbf{Update loop}: for $t=0, \ldots ,t=T-1$ do the update as:
    \begin{flalign}
      \boldsymbol{z}^{(t+1)}=\boldsymbol{z}^{(t)}- \frac{\mu_{t+1}}{{\boldsymbol{z}^{(t)}}^*\boldsymbol{a}_i}\left(\frac{1}{m}\sum_{i=1}^{m}2\left(\left|\boldsymbol{a}_i^*\boldsymbol{z}^{(t)}\right|_X^2-y_i\right)\boldsymbol{a}_i\boldsymbol{1}_{\mathcal{E}_1^i\cap\mathcal{E}_2^i}\right).
    \end{flalign}
    where
    \begin{equation*}
      \begin{split}
        \mathcal{E}_1^i &\coloneqq \left\{  \alpha_z^{lb}  \leq \frac{\sqrt{n}\left|\boldsymbol{a}_i^*\boldsymbol{z}^{(t)}\right|_X}{\left|\boldsymbol{a}_i\right|_X\left|\boldsymbol{z}^{(t)}\right|_X} \leq  \alpha_z^{ub}   \right\} \\
        \mathcal{E}_2^i &\coloneqq \left\{ \left|y_i-\left|\boldsymbol{a}_i^*\boldsymbol{z}^{(t)}\right|_X^2\right| \leq  \alpha_h{K_t}\frac{\sqrt{n}\left|\boldsymbol{a}_i^*\boldsymbol{z}^{(t)}\right|_X}{\left|\boldsymbol{a}_i\right|_X\left|\boldsymbol{z}^{(t)}\right|_X}   \right\} \\
        K_t             &\coloneqq \frac{1}{m}\sum_{j=1}^{m}\left|y_j-\left|\boldsymbol{a}_i^*\boldsymbol{z}^{(t)}\right|_X^2\right|
      \end{split}
    \end{equation*}
    \textbf{Output}: $\boldsymbol{z}^{(T)}$.
  \end{algorithm}
  A possible \pytorch\cite{Paszke2019} implementation with \ac{CUDA}\cite{Nvidia} acceleration is listed below as:
  % \lstinputlisting[language=Python, firstline=1,lastline=20]{./algorithms/twf.py}
  \lstinputlisting[language=Python]{./algorithms/twf.py}\index{TWF}\label{code:twf}    
\clearpage % End the page
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Reshaped Wirtinger Flow %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
\clearpage % Start a new page
\begin{algorithm}
  \caption{\ac{RWF}\index{RWF} suggested by \cite{Zhang2016}}\label{pseudocode:rwf}
    \textbf{Input}: Let $\boldsymbol{y}=\{y_i\}_{i=1}^m$ and $\{\boldsymbol{a}_i\}_{i=1}^m$ be our measurements and sampling vectors; \\
    \textbf{Parameters:}  step size $\mu$, and thresholds $\alpha_{l},\alpha_{u}$;\\
    \textbf{Initialization}: Let $\hat{\boldsymbol{z}}$, be the eigenvector corresponding to the largest eigenvalue of:
    \begin{equation}
      \boldsymbol{Y} \coloneqq \frac{1}{m}\sum_{i=1}^m y_i\boldsymbol{a}_i \boldsymbol{a}_i^*\boldsymbol{1}_{\left\{ \alpha_{l}n\sum_{i=1}^m \frac{y_i}{\|\boldsymbol{a}_i\|_1} \leq \left|y_i\right|_X \alpha_{u}n\sum_{i=1}^m \frac{y_i}{\|\boldsymbol{a}_i\|_1}\right\}}
    \end{equation}
    and find it using power iteration. Let also $\Lambda=n\sum_{i=1}^m \frac{y_i}{\|\boldsymbol{a}_i\|_1}$ to be the norm estimation of the signal of interest that we are trying to recover. 
    Set $\boldsymbol{z}^{(0)}=\Lambda\hat{\boldsymbol{z}}$ for the initialization.\\
    \textbf{Update loop}: for $t=0, \ldots ,t=T-1$ do the update as:
    \begin{flalign}
      \boldsymbol{z}^{(t+1)}=\boldsymbol{z}^{(t)}- \mu_{t+1}\left(\frac{1}{m}\sum_{i=1}^{m}\left(\boldsymbol{a}_i^*\boldsymbol{z}^{(t)}-y_i\frac{\boldsymbol{a}_i^*\boldsymbol{z}^{(t)}}{\left|\boldsymbol{a}_i^*\boldsymbol{z}^{(t)}\right|_X}\right)\boldsymbol{a}_i\right).
    \end{flalign}
    \textbf{Output}: $\boldsymbol{z}^{(T)}$.
  \end{algorithm}
  A possible \pytorch\cite{Paszke2019} implementation with \ac{CUDA}\cite{Nvidia} acceleration is listed below as:
  % \lstinputlisting[language=Python, firstline=1,lastline=20]{./algorithms/rwf.py}
  \lstinputlisting[language=Python]{./algorithms/rwf.py}\index{RWF}\label{code:rwf}     
\clearpage % End the page
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% Incrementally Reshaped Wirtinger Flow %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
\clearpage % Start a new page
\begin{algorithm}
  \caption{\ac{IRWF}\index{IRWF} suggested by \cite{Zhang2016}}\label{pseudocode:irwf}
    \textbf{Input}: Let $\boldsymbol{y}=\{y_i\}_{i=1}^m$ and $\{\boldsymbol{a}_i\}_{i=1}^m$ be our measurements and sampling vectors; \\
    \textbf{Parameters:}  step size $\mu$, and thresholds $\alpha_{l},\alpha_{u}$;\\
    \textbf{Initialization}: Let $\hat{\boldsymbol{z}}$, be the eigenvector corresponding to the largest eigenvalue of:
    \begin{equation}
      \boldsymbol{Y} \coloneqq \frac{1}{m}\sum_{i=1}^m y_i\boldsymbol{a}_i \boldsymbol{a}_i^*\boldsymbol{1}_{\left\{ \alpha_{l}n\sum_{i=1}^m \frac{y_i}{\|\boldsymbol{a}_i\|_1} \leq \left|y_i\right|_X \alpha_{u}n\sum_{i=1}^m \frac{y_i}{\|\boldsymbol{a}_i\|_1}\right\}}
    \end{equation}
    and find it using power iteration. Let also $\Lambda=n\sum_{i=1}^m \frac{y_i}{\|\boldsymbol{a}_i\|_1}$ to be the norm estimation of the signal of interest that we are trying to recover. 
    Set $\boldsymbol{z}^{(0)}=\Lambda\hat{\boldsymbol{z}}$ for the initialization.\\
    \textbf{Update loop}: for $t=0, \ldots ,t=T-1$ do the update as:
    \begin{flalign}
      \boldsymbol{z}^{(t+1)}=\boldsymbol{z}^{(t)}- \mu_{t+1}\left(\frac{1}{m}\sum_{i=1}^{m}\left(\boldsymbol{a}_{i_t}^*\boldsymbol{z}^{(t)}-y_{i_t}\frac{\boldsymbol{a}_{i_t}^*\boldsymbol{z}^{(t)}}{\left|\boldsymbol{a}_{i_t}^*\boldsymbol{z}^{(t)}\right|_X}\right)\boldsymbol{a}_i\right).
    \end{flalign}
    where $i_t$ is randomly selected from the set $\left\{1,\cdots,m\right\}$.\\
    \textbf{Output}: $\boldsymbol{z}^{(T)}$.
  \end{algorithm}
  A possible PyTorch\cite{Paszke2019} implementation with \ac{CUDA}\cite{Nvidia} acceleration is listed below as:
  % \lstinputlisting[language=Python, firstline=1,lastline=20]{./algorithms/wf.py}
  \lstinputlisting[language=Python]{./algorithms/irwf.py}\index{IRWF}\label{code:irwf}    
\clearpage % End the page
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Incrementally Mini-Batch Reshaped Wirtinger Flow %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
\clearpage % Start a new page
\begin{algorithm}
  \caption{\ac{IMRWF}\index{IMRWF} suggested by \cite{Zhang2016}}\label{pseudocode:imrwf}
    \textbf{Input}: Let $\boldsymbol{y}=\{y_i\}_{i=1}^m$ and $\{\boldsymbol{a}_i\}_{i=1}^m$ be our measurements and sampling vectors; \\
    \textbf{Parameters:}  step size $\mu$, and thresholds $\alpha_{l},\alpha_{u}$, batch size $k$;\\
    \textbf{Initialization}: Let $\hat{\boldsymbol{z}}$, be the eigenvector corresponding to the largest eigenvalue of:
    \begin{equation}
      \boldsymbol{Y} \coloneqq \frac{1}{m}\sum_{i=1}^m y_i\boldsymbol{a}_i \boldsymbol{a}_i^*\boldsymbol{1}_{\left\{ \alpha_{l}n\sum_{i=1}^m \frac{y_i}{\|\boldsymbol{a}_i\|_1} \leq \left|y_i\right|_X \alpha_{u}n\sum_{i=1}^m \frac{y_i}{\|\boldsymbol{a}_i\|_1}\right\}}
    \end{equation}
    and find it using power iteration. Let also $\Lambda=n\sum_{i=1}^m \frac{y_i}{\|\boldsymbol{a}_i\|_1}$ to be the norm estimation of the signal of interest that we are trying to recover. 
    Set $\boldsymbol{z}^{(0)}=\Lambda\hat{\boldsymbol{z}}$ for the initialization.\\
    \textbf{Update loop}: for $t=0, \ldots ,t=T-1$ do the update as:
    \begin{flalign}
      \boldsymbol{z}^{(t+1)}=\boldsymbol{z}^{(t)}- \mu_{t+1}\boldsymbol{A}_{\Gamma_t}\left(\boldsymbol{A}_{\Gamma_t}\boldsymbol{z}^{(t)}-\boldsymbol{y}_{\Gamma_t} \odot \frac{\boldsymbol{A}_{\Gamma_t}\boldsymbol{z}^{(t)}}{\left|\boldsymbol{A}_{\Gamma_t}\boldsymbol{z}^{(t)}\right|_{e_X}}\right).
    \end{flalign}
    where $\Gamma_t$, the index vector, is randomly selected from the subsets of $\left\{1,\cdots,m\right\}$ that has $k$ members. $\boldsymbol{A}_{\Gamma_t}, \boldsymbol{y}_{\Gamma_t}$ 
    correspond to the index vector taken from the original $\boldsymbol{A}$ and $\boldsymbol{y}$ \\
    \textbf{Output}: $\boldsymbol{z}^{(T)}$.
  \end{algorithm}
  A possible \pytorch\cite{Paszke2019} implementation with \ac{CUDA}\cite{Nvidia} acceleration is listed below as:
  % \lstinputlisting[language=Python, firstline=1,lastline=20]{./algorithms/wf.py}
  \lstinputlisting[language=Python]{./algorithms/imrwf.py}\index{IMRWF}\label{code:imrwf}    
\clearpage % End the page
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%% msip-Dell Specs %%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
  % \clearpage % Start a new page
  \begin{table}[!htbp]
    \centering
    \begin{tabular}{||l l||} 
     \hline
     General 		                	&  						                                                             \\ [0.5ex] 
     \hline\hline
     Processor 	         		 			& Intel\textregistered \space Xeon\textregistered         	                 \\
     Accelerator 			 	       		& NVIDIA\textregistered  	                                                 \\ 
     Operating System   			    & GNU/Linux(Ubuntu\textregistered) 	                                    	 \\
     Memory 	               			& 32617768 kB                                                              \\ [1ex] 
     \hline
     \hline
     Processor Details 	      		&  						                                                             \\ [0.5ex] 
     \hline\hline
     Architecture     			 			& X86\_64                                               	                 \\ 
     CPU op-mode(s)         			& 32-bit, 64-bit 	                                                         \\
     Address sizes                & 46 bits physical, 48 bits virtual  		                                   \\
     Byte Order                   & Little Endian  	                                                         \\ 
     CPU(s):                      & 8 	 	                                                                   \\
     On-line CPU(s) list:         & 0-7  	                                              	                   \\
     Vendor ID:                   & GenuineIntel\textregistered 	                         	                 \\
     Model name:                  & Intel\textregistered \space Xeon\textregistered CPU E5-1630 v3 at 3.70GHz \\
     Thread(s) per core:          & 2                                                                        \\
     Core(s) per socket:          & 4                                                   		                 \\
     Socket(s):                   & 1 	                                                	                   \\
     CPU max MHz:                 & 3800.0000 	                                         	                   \\
     CPU min MHz:                 & 1200.0000                                         	  	                 \\
     L1d Cache:                   & 128 KiB (4 instances)                           	    	                 \\
     L1i Cache:                   & 128 KiB (4 instances) 	 	                                               \\
     L2 Cache:                    & 1 MiB (4 instances) 	                                                   \\
     L3 Cache:                    & 10 MiB (1 instance) 	 	                                                 \\
     NUMA node(s):                & 1                                               	 	                     \\
     Optimization Flags           & Please Refer to the Intel\textregistered \space Brochure for the Details  		   \\[1ex] 
     \hline
     \hline
     Accelerator Details 			    &                                                                          \\[0.5ex] 
     \hline\hline
     Full Designation 	    			& NVIDIA\textregistered \space GeForce\textregistered \space RTX 2080 Ti 	               \\ 
     Memory   	              		& 11264 MiB 	                                           	                 \\
     CUDA Version:                & 12.2  	                                               	                 \\
     width:                       & 64 bits                                                                  \\
     clock:                       & 33 MHz 	                                                                 \\[1ex] 
     \hline
     \hline
     Numerical Framework Details	&  				                                            		                 \\[0.5ex] 
     \hline\hline
     python                       & 3.10.12                                                                  \\
     numpy                        & 1.25.2  		                                                             \\
     scipy                        & 1.11.1  		                                                             \\
     matplotlib                   & 3.7.2   		                                                             \\
     scikit-learn                 & 1.3.0  		                                                               \\
     scikit-image                 & 0.21.0  		                                                             \\
     pytorch                      & 2.0.0			 			 	                                                       \\ 
     pytorch-cuda                 & 11.7 	 	                                                                 \\
     \LaTeX \space Distribution   & \TeX-Live			 			 	                                                     \\
     \LaTeX \space Engine/Recipe  & Auto Recipe in VS-Code \LaTeX \space Extension		                       \\[1ex] 
     \hline
    \end{tabular}
    \caption{Software and Hardware that were used on the second system}
    \label{tab:system_specs}
    \end{table}
  % \clearpage % End the page
}



