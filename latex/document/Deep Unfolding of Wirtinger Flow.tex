\chapter{Deep Unfolding of Wirtinger Flow}\label{ch:deep_unfolding_of_wirtinger_flows}



\section{Deep Unfolding}

We give a story like introduction to \dl\cite{Goodfellow2016} and by pointing out some \dl limitations motivate the use of \du/\au\cite{Monga2019} 
which is \dl like, but with context inspired architecture.

\subsection{Multi-Layer Perceptrons}
Let $\boldsymbol{x} \in \mathbb{R}^n,\boldsymbol{y} \in \mathbb{R}^m, \boldsymbol{h}^l \in \mathbb{R}^{k_l}, \boldsymbol{W}^1 \in \mathbb{R}^{k_1 \times n}
, \boldsymbol{b}^1 \in \mathbb{R}^{k_1}, \boldsymbol{W}^{N+1} \in \mathbb{R}^{m \times k_N}, \mathbb{b}^m,\boldsymbol{W}^{l+1} \in \mathbb{R}^{k_l \times k_{l+1}}$ 
where $m,n,l,k_{l\in \left\{0,\ldots,N-1\right\}} \in \mathbb{N}$ and $\varphi \colon \mathbb{R} \to \mathbb{R}$ is a 
nonlinear function with certain properties. 
Consecutive mappings:
\begin{equation}
  \begin{split}
    h_i^{1}   &= \sigma \left( \sum_{j}^{} W_{ij}^{1}x_j + b_i^{1} \right)\\ 
              & \vdots\\
    h_i^{l+1} &= \sigma \left( \sum_{j}^{} W_{ij}^{l+1}h_j^l + b_i^{l+1} \right)\\
              & \vdots\\
    y_i^{}    &= \sigma \left( \sum_{j}^{} W_{ij}^{N+1}h_j^N + b_i^{N+1} \right)
  \end{split}
  \end{equation}
that take $\boldsymbol{x}$ to $\boldsymbol{y}$ is called a \ac{MLP}\cite{Bishop2006} architecture.
In the \ml/\dl jargon \cite{Goodfellow2016}\cite{ShalevShwartz2014} $\boldsymbol{x},\boldsymbol{y},\boldsymbol{h}^j,\boldsymbol{W}^j,\boldsymbol{b}^j,\varphi$ are called \emph{input}, \emph{output}, \emph{hidden variables/layers},
\emph{weight matrices}, \emph{biases}, and \emph{activation function}. \ac{MLP} is one of the variation of a \nn and the \nn name itself is coming from a 
time where scientists were trying sell their architecture as if they were working like the human brain and unfortunately the name stuck\footnote{
  In older non-math books you can find phrases like ``neurons firing up and this is modeled by our activation function'' 
  instead of the non-linearity argument we gave above. Just to be clear \nns do not work like the human brain.
}.
\ac{MLP}s were designed to approximate outputs from inputs without knowing the the actual mapping 
and with just having $N$ pairs of $\left(\boldsymbol{x}^*,\boldsymbol{y}^*\right)$ (having $N$ sample points) through adjusting the \emph{weight matrices} and the \emph{biases} which is know as the \emph{training process}. 
The main steps in the training process are:
\begin{enumerate}
  \item Initialize the \emph{weight matrices} and the \emph{biases} (mostly done randomly).
  \item Predict $\boldsymbol{y}^*$s by giving your model $\boldsymbol{x}^*$s and call it $\boldsymbol{y}_e$.
  \item Select a metric\cite{Alt2016} and and calculate the distance between $\boldsymbol{y}^*$ and $\boldsymbol{y}_e$. Easiest metric that 
  you can come up with if you take the norm (like \cref{def:scalar_product_induced_norm_complex_matrices}, \cref{def:scalar_product_induced_norm_complex_matrices}, or \cref{def:p-norm}) 
  of the difference, $\boldsymbol{y}^* - \boldsymbol{y}_e$, and call it a day.
  \item Try to minimize the distance between your estimate $\boldsymbol{y}_e$ and your ground truth $\boldsymbol{y}^*$ using an optimizer like \adam\cite{Kingma2014}.
\end{enumerate}  
It is worth noting that the existence of $\varphi$ is necessary otherwise the pairs $\boldsymbol{W}^j,\boldsymbol{b}^j$ would 
just be an affine transformation and stacking multiple affine transformation would still be an affine transformation and stacking 
such layers would be redundant. Not having an activation function also means the whole mapping would perform poorly when approximating 
complex mappings that do not resemble affine transformation. The schematic involving only the input, output, and hidden variables can be seen 
in \cref{fig:multi_layer_perceptron}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% Multi-Layer Perceptron %%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
  \centering
  % \captionsetup{justification=centering}
  \resizebox{0.7\textwidth}{!}{\input{./tikz/neural_networks/sample.tex}}
  \caption{$\boldsymbol{x} =\text{input},\boldsymbol{h}^j=\text{hidden variables},\boldsymbol{y}=\text{output}$}
  \label{fig:multi_layer_perceptron}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Deep Learning}
The advancements in \ac{HPC}\cite{Meuer} in terms of both software\cite{Dagum1998}\cite{1993}\cite{Nvidia} and 
hardware\cite{Patterson2014}\cite{Hennessy2019}\cite{Nvidia}, Mathematical Optimization\cite{Boyd2004}\cite{Nocedal2006}\cite{Sun2019}, 
\ad\cite{Naumann2011}\cite{Griewank2008} combined with the abundance of data gathered during the Web $2.0$ era, 
and finally complex approximation architectures gave birth to what is currently known as 
\dl\cite{LeCun2015}\cite{Higham2018}\cite{Berner2021}. In the \ml/\dl/\ai folklore there were these two dark 
periods known as the \emph{\ai winters} that meant the substantial reduction in \ml/\dl/\ai funding and 
interest. In $2012$ when Alex Krizhevsky's and his supervisors' \dl architecture \cite{Krizhevsky2017} decimated all other 
competitors in the ImageNet \cite{Deng2009} image classification challenge\cite{SVLL} he ended the second harsh winter 
that the \ml/\ai scientists were experiencing. It was no fluke and at the time of writing the challenge is 
always won by \dl architectures \cite{Szegedy2014}\cite{He2015}\cite{Simonyan2014} and alike and not carefully 
handcrafted feature extraction modules. All and all \dl architectures had great success in a broad flavors of 
problems ranging from single object image classification \cite{Rawat2017} to real-time multi-object classification and 
tracking\cite{Luo2021}. \DL approach has the following pros:

\begin{itemize}
  \item can extract extremely complicated mappings or extremely subtle features (depending on the desired wording),
  \item requires little to no knowledge about the exact internals of the problem (no need for handcrafted feature engineering),
  \item currently can beat human level performance in lots of areas; object detection (single and multi), 
  object tracking(single and multi), anomaly detection, fraud detection, and strategical games (\cite{Silver2016} being the most astounding one) 
  just to name a few.
\end{itemize}

and the following cons:

\begin{itemize}
  \item requires large and high quality datasets like the ImageNet\cite{Deng2009}, in the case of image classification, which are expensive to acquire and store,
  \item requires tremendous raw computational power at the level of top 500 supercomputers listed in \cite{Meuer} and 
  storage which in turn would result in large electricity bills
  \footnote{In the $2022$ iteration of the \emph{Introduction to High-Performance Computing} course; Matthias M\"{u}ller, the head of 
  the \ac{HPC} center at RWTH Aachen University, explained his rule of thumb as one Megawatt costing them one million 
  euros per year.} and expensive maintenance costs,
  \item since mostly there is no interpretability associated with models, there will be no reasoning when a model gives objectively wrong answers. 
\end{itemize}

Due to the pros and cons tied to \dl what is getting a bit of traction at least in \dsip and \cv is \du or \au. Using whether to use \du 
or \au depends on the preference of the practitioner and most importantly their educational backgrounds. We go with \du as it is befitting ours from now on.
\du is the process of \emph{unfolding} an iterative algorithm finite times, interpreting it as a \nn and 
adjusting certain parameters in the hopes of improving some metric during the training process. \DU architectures are 
different from other \nns in the following aspects:
\begin{itemize}
  \item In the context of the iterative algorithms we say the internal structure of the $k+1$-th iteration acted on the input values, $\boldsymbol{z}^k \in \mathbb{C}^N$, and gave $\boldsymbol{z}^{k+1} \in \mathbb{C}^N$ 
  as the output values. In the \ml/\dl context we say values at layer $k$, $\boldsymbol{z}^k \in \mathbb{C}^N$, are mapped to values at layer $k+1$, $\boldsymbol{z}^{k+1} \in \mathbb{C}^N$.
  \item The previous point is not just a reinterpretation as the internal structure of the iteration dictates the activation functions and the dependency graph. 
  Most of the times the said activation functions and the dependency graphs are so sophisticated that it is next to impossible to guess them unlike the general \nns 
  with their corresponding activation functions and mappings.
  \item In general \nns we add more and more parameters in the hope of retrieving the complex mappings while in \dl we already know the mapping 
  and even if we decide to make the \nn more complex or add more parameters we do it based on analytical arguments. This is in fact one of the most crucial points of 
  \dl compared to general \nns as number of parameters have direct and substantial effects over datasets'
   sizes, training runtimes, required computational powers and many more. 
  \item Initialization of the weight matrices and biases are mostly done randomly for the case of general \nns while in the \du initialization is coming for the 
  iterative algorithm which is mostly well thought and closely inspected.   
\end{itemize}
In short \du tries to bring the best of the both worlds(analytical approach and data driven approach) in one and reconcile the two. 
While nothing beats an actual numerical experiment based on \du like what we did in \cref{ch:results} for now it is best to look at 
the process in \cref{fig:deep_unfolding_unrolling} to get a better feeling of what is being done. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% Deep Unfolding(Schematic Diagram) %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
  \centering
	\captionsetup{justification=centering}
  \resizebox{35em}{10em}{\input{./tikz/diagrams/unfolding.tex}}
  \caption{The Schematic Unfolding/Unrolling of an Iterative Algorithm}
  \label{fig:deep_unfolding_unrolling}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A word of caution to the people who are more familiar with \ml/\dl is that \du/\au is not a \rnn\cite{Gregor2010}. \DU/\AU looks like a \rnn\cite{Gregor2010} but 
a closer look will result in a couple of subtleties between the two

\todo{I don't see how these things don't make it a RNN. Even if you reduce the number of parameters 
or have easy initialization, it is still an RNN, albeit a quite specific one. But is out structure actually 
recurrent? The structure you show in the unfolding figure looks uni-directional, i.e. 
like a feed forward network.}. 


Since in the \du/\au you significantly reduce the number of 
parameters by using a context inspired model and not a full blown general \ml/\dl architecture, the need for large 
datasets and computational power ceases to exist\footnote{It is worthwhile to check the system we used for our 
study in \cref{tab:system_specs} to the general systems\cite{Meuer} that are used in training large \ml/\dl models.}. Initialization 
of the weights, which for some time were the center of attention in \ml/\dl community \cite{Glorot2010}\cite{He2015a}, 
and interpretation of the models become way easier. For initialization in the \du interpretation you basically set them according 
to the original model that is being \emph{unfolded}/\emph{unrolled}. We will explain this in more details when we are describing the 
training process of our \ml/\dl model in our study. A nice review on the recent application of \du in \dsip is done by \cite{Monga2019}.











\section{Importance of the Phase}\label{sec:phase_importance}
\todo{It is unclear why you talk about the phase here. Sounds like a random new topic, we just heard about unfolding. Also, make sure that the term phase is defined. So far, this term is not mentioned in Cahpter 2 or in the part of Chapter 3 above.}
In this section by signal we mean speech wave \todo{I think this is the only time the word speach (and perhaps also wave) appears. Is that relevant for your point?} and by image we mean 2D rectangular natural image. The \ac{DFT} (both 1D and 2D) is bijective\cite{Damelin2011} (also hinted \todo{Why ``also hinted''? This statements essentially proof the invertability of the 1D and 2D transform respectively.} at by \cref{prop:1ddft_vectors_orthononality} and 
\cref{prop:2ddft_matrices_orthogonality}) therefore \todo{This is a consequence of the propositions, not of the fact that the DFT is invertible. In other words, the fact that the DFT is invertible, does not say that the IDFT, as you defined it, is the inverse.} taking the \ac{DFT} and the \ac{IDFT} in succession on a/an signal/image will have no effect on the signal/image.
 As a synthetic example to show the importance of phase; take the \ac{DFT} of two different images but before taking the \ac{IDFT} of the images; 
 swap the phase and then perform the \ac{IDFT} to arrive at the two reconstructed images. This was done by \cite{Oppenheim1981} to show the importance of 
 phase in signals and images. As it can be seen in \cref{image:phase_swap} \todo{You need to more clearly describe what is shown in the figure. I assume first and third row are the input images, and second and fourth are images with phase swap. But is the second row the phase of the first with the amplitude of the third? Same question for the fourth row.} the images \emph{look like} the image with the corresponding phase. As signals must 
 be heard but images can be shown we chose to do the synthetic example on images rather signals, but if had 
 done that the results would have been the same. Different manipulations on the amplitude and the phase of the \ac{FT} during the reconstructed process 
 were also investigated by \cite{Oppenheim1979}\cite{Oppenheim1981} which in short are:\todo{Since you are mentioning all of these, the reader would also wonder how these things look... Your one experiment only shows part of the picture.}
 \begin{itemize}
	\item taking the \ac{DFT}, setting the phase to zero and then reconstructing the image\cite{Oppenheim1981}\todo{You have the reference above, there is no need to restarte the reference in every line about that reference.}.
	\item taking the \ac{DFT}, setting the amplitude to one and then reconstructing the image\cite{Oppenheim1981}
	\item taking the \ac{DFT}, setting a random amplitude and then reconstructing the image\cite{Oppenheim1981}
	\item taking the \ac{DFT}, setting the amplitude to the average of amplitudes of \ac{DFT}s of images of the same type and then reconstructing the image\cite{Oppenheim1981}.
 \end{itemize} 
 All of them suggested that the phase of the \ac{DFT} of an image 
 is more important than the amplitude of the \ac{DFT} of an image\cite{Oppenheim1979}\cite{Oppenheim1981}.  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% Phase in Fourier Reconstruction %%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
  % \clearpage % Start a new page
  % \thispagestyle{empty} % No header/footer on this page
  \begin{figure}
    \centering
	\captionsetup{justification=centering}
    \includegraphics[width=1.0\textwidth,height=65em]{./images/phase_importance/phase_importance_comparison.png}
    \caption{The Importance of Phase in Fourier Reconstruction \cite{Oppenheim1979}\cite{Hayes1980}\cite{Oppenheim1981}\cite{Shechtman2015}}
    \todo[inline]{Why are there four references for a single, simple figure where you took/computed all images yourself?}
    \label{image:phase_swap}
  \end{figure}
  % \clearpage % End the page
}   

\section{Phase Problem}
Sometimes the precious and coveted phase we talked about in \cref{sec:phase_importance} or other forms of it gets lost during a process. The reader must recognize 
that by phase we do not always mean the phase of the \ac{FT} and the name \emph{phase} has a very broad meaning\todo{I would say we always mean exactly the phase of complex numbers. Just where these complex numbers come from differs.}. The physics behind the phase loss is beyond the 
scope of the current work, but in \cref{sec:motivating_application} we give a mathematical formula \todo{\cref{pro:phase_problem} is already a very specific mathematical formulation} that explicitly shows the destruction of the phase 
during a process for a specific real world setting. For the case where material limitations and quantum mechanical effects are responsible for the phase loss we invite 
the intrigued party to take a look at \cite{Shechtman2015} (material limitations) and \cite{DGDS2018}\cite{FranzSchwabl2007} (quantum mechanical effects). 
Mathematically the phase problem can be stated as:
\begin{Pro}[Phase Problem]\label{pro:phase_problem}
\todo{Be more clear: You are given the $G_j$ and then the task is to find a $\psi$ that fulfills $G_j = \varphi(\boldsymbol{A}_j\psi)$ or $G_j \approx \varphi(\boldsymbol{A}_j\psi)$.}
Find $\psi \in \mathbb{C}^N$ such that $G_j = \varphi(\boldsymbol{A}_j\psi)$ or $G_j \approx \varphi(\boldsymbol{A}_j\psi)$ where 
$\boldsymbol{A}_j$s for $j \in \left\{1,\ldots,K\right\}$ are linear operators \todo{This is finite dimensional, so this are just matrices. Also specify their sizes and the sizes of the $G_j$} and $\varphi$ is a function with 
phase corrupting capability (something that is not bijective) \todo{I would just consider $\varphi \colon \mathbb{C} \to \mathbb{R}$, allowing the two choices of $\varphi$ you mention in the footnote and then call this phase retrieval.} $\varphi \colon \mathbb{C} \to \mathbb{C}$ or phase destruction capability 
$\varphi \colon \mathbb{C} \to \mathbb{R}$\footnote{the case of $z \rightarrow \left|z\right| \lor {\left|z\right|^2}$ 
are of great interest}.
\end{Pro} 






\section{Phase Retrieval}

\todo{Don't start a new section. Below you formulate one way to (approximately) solve the problem you just set up. You also need all elements you formulated for the problem, to formulate the soltion.}Let $\Omega \in \mathbb{R}^n$ and $\psi$ \todo{This is not a mapping, but a vector (The $N$ below comes from the discretization and should be the length of $G_j$). You don't have any $\Omega$ here or ever used an $\Omega$ before in this thesis.} a mapping that takes $\Omega$ to $\mathbb{C}$. Mostly $\Omega$ is assumed to be a 
$d$-dimensional cube which makes it easier to work with. The process of extracting $\psi$(up to a global phase) from:
\begin{equation}\label{eq:pr_problem}
	E(\psi) = \underbrace{\frac{1}{2KN} \sum_{j=1}^{K} {\left|\phi(\boldsymbol{A}_j\psi)-G_j\right|_X}^2}_{\coloneqq D(\psi)}+ R(\psi)
  \end{equation}
  \todo{$\left|\cdot\right|_X$ above needs to be replaced with the Euclidean norm in $\mathbb{R}^N$, assuming $N$ denotes the number of elements of $G_j$.}
  is called \acl*{PR}(just a subclass of \acl*{PR}) where:
  \begin{itemize}
  \item $\psi$ is the continuous variable that after the discretization is going to be $\in \mathbb{C}^N$
	\item $E(\psi)$ the functional we are trying to minimize.
	\item $D(\psi)$ the data term.
	\item $R(\psi)$ the regularization term.
	\item $\boldsymbol{A}_j$ the $j$-th linear sampler operator.
	\item $G_j$ the $j$-th measurement.
	\item $\varphi \colon \mathbb{C} \rightarrow \mathbb{R}$, 
	is the function responsible for the phase loss(phase destruction) in the measurement process\footnote{the case of $z \rightarrow \left|z\right| \lor {\left|z\right|^2}$ are of great interest}
  \end{itemize}

Solving the \acl*{PR} is not quite easy due to the non-convex, non-holomorphic and infinite-dimensional characteristics of the \acl*{PR}. 
In short the \acl*{PR} can be solved by the iterative algorithm:
  \begin{equation}\label{eq:pr_solution}
	\psi^{k+1} = \text{prox}_{\tau_{k}R}(\psi^k-\tau_k\nabla{D(\psi^k)})
  \end{equation}

  where the gradient-like structure looks like: 
  \begin{equation}\label{eq:gradient_pr_solution}
	\nabla{D(\psi^k)} = \frac{1}{KN} \sum_{j=1}^{K} \boldsymbol{A}_j^*\left(\varphi\left(\boldsymbol{A}_j\psi\right)-G_j\right)\odot \varphi'(\boldsymbol{A}_j\psi).
  \end{equation}

  where:
  \begin{itemize}
	\item $\mathrm{prox}$ is the Proximal operator mostly used in non-differentiable but still convex setting \cite{Bredies2018}.
	\item $\odot$ is the Hadamard product\cite{Hackbusch2019} which is simply the element-wise product between two multidimensional arrays and occurs naturally when $\psi$ and $\boldsymbol{A}_j$ in \cref{eq:pr_problem} are discretized.
	\item $\boldsymbol{A}^*_j$ is the $j$-th conjugate operator of the $j$-th linear sampler operator $\boldsymbol{A}_j$.
	\item $\varphi' \colon \mathbb{C} \to \mathbb{C}$ the first derivative of the $\varphi \colon \mathbb{C} \to \mathbb{R}$ function.
  \end{itemize}

  A simpler variation of the above formulation were studied extensively and is at the core of the \ac{WF} variants. 
  From now on we consider the following as the \acl*{PR} problem:

  \begin{equation}\label{eq:main_function}
	\boldsymbol{y} = \varphi(\boldsymbol{A}\boldsymbol{x})
  \end{equation}
  where $\boldsymbol{x} \in \mathbb{C}^{n \times 1}$ is the vector of interest, $\boldsymbol{A} \in \mathbb{C}^{m \times n}$ the sampler matrix,  
  $\boldsymbol{y} \in \mathbb{R}^{m \times 1}$ the measurements vector, and $\varphi$ the usual element-wise absolute value(or the squared absolute value) from 
  $\mathbb{C}^{m \times 1}$ to $\mathbb{R}^{m \times 1}$ that destroys the information encoded in the phase. Since 
  $\left|\boldsymbol{A}\boldsymbol{x}\mathrm{e}^{\mathrm{i}\theta}\right|_X = \left|\boldsymbol{A}\boldsymbol{x}\right|_X$, 
  $\min \left|\boldsymbol{x}-\boldsymbol{z}\mathrm{e}^{-\mathrm{i}\theta}\right|_X$ was considered as the loss function throughout the \ac{WF} variants papers \cite{Candes2014}\cite{Chen2015}\cite{Kolte2016}\cite{Zhang2016} where $\boldsymbol{z}$ is the estimation of
  $\boldsymbol{x}$ up to a global phase.






	\begin{Prop}\label{theorem:min distance}
		Let $\varphi \in \mathbb{R}$, $x,z \in \mathbb{C}^n$, where $(z,x)_X$ \todo{As discussed, use \cref{def:scalar_product_complex_vectors} as scalar product and \cref{def:scalar_product_induced_norm_complex_vectors} as norm. I will check the proof assuming that it is using this scalar product.} is 
		the usual scalar product associated with such vector spaces, and $\left|\cdot\right|_X$ the induced norm using the scalar product on the vector space $X$, then the expression
		\begin{equation}\label{eq:loss_function}
			f(\varphi) \coloneqq \left|x-\mathrm{e}^{-\mathrm{i}\varphi}z\right|^2_X
		\end{equation}
		has a minimum \todo{A minimum for a function is always associated to a domain / an admissible set. I would say you consider the function $f:\mathbb{R}\to\mathbb{R}, \varphi\mapsto \left|x-\mathrm{e}^{-\mathrm{i}\varphi}z\right|^2_X$ and that this function has a minimum.} and the minimum can be calculated in a closed form manner.\todo{The closed form solution should be part of the statement.}
		\end{Prop}
		\begin{Proof}
			$f(\varphi)$ \todo{$f$ is continuous, $f(\varphi)$ is just a real number.} is continuous and periodic \todo{``with period $2\pi$''. Without the period, you can't know to which interval you can restrict the search for a minimum.} therefore it will attain its minimum and maximum \todo{This is correct and uses the fact that a continuous function on a compact interval attains minimum and maximum.} for $\varphi^\ast \in [0,2\pi)$\cite{Rudin1976}. 
			Let $w \coloneqq (z,x)_X = \operatorname{Re}(w)+\mathrm{i}\operatorname{Im}(w)$\todo{These is technically correct, but does not follow the train of thought, i.e., you have $w \coloneqq (z,x)_X $ and obviously $w=\operatorname{Re}(w)+\mathrm{i}\operatorname{Im}(w)$. The equation $\operatorname{Re}(w)+\mathrm{i}\operatorname{Im}(w)$ follows from the two before.}
      \todo{Don't write $f(\varphi) \coloneqq $ below ($f$ is already defined, you don't redefine it.). Instead start with $f(\varphi) = $}
			\begin{equation}
				\begin{split}
				f(\varphi) &\coloneqq \left|x-\mathrm{e}^{-\mathrm{i}\varphi}z\right|^2_X = 
				\left(x-\mathrm{e}^{-\mathrm{i}\varphi}z\right)_X\left(x-\mathrm{e}^{-\mathrm{i}\varphi}z\right)_X\\
        &\textcolor{red}{\text{Above should be: }\left(x-\mathrm{e}^{-\mathrm{i}\varphi}z,x-\mathrm{e}^{-\mathrm{i}\varphi}z\right)_X\text{ and the correct scalar product of course}}\\
						   &= \left(x,x\right)_X - \mathrm{e}^{\mathrm{i}\varphi}\left(x,z\right)_X-\mathrm{e}^{-\mathrm{i}\varphi}\left(z,x\right)_X+\left(z,z\right)_X \\
                		   &= \left(x,x\right)_X - (\cos(\varphi)+\mathrm{i}\sin(\varphi))\left(\operatorname{Re}(w) -\mathrm{i}\operatorname{Im}(w)\right)\\
        &\textcolor{red}{\text{I would align the $+$ right of the $=$}}\\
						   &+ \left(z,z\right)_X - (\cos(\varphi)\textcolor{red}{\text{This should be $-$}}+\mathrm{i}\sin(\varphi))\left(\operatorname{Re}(w) +\mathrm{i}\operatorname{Im}(w)\right)\\
               &\textcolor{red}{\text{Either write $\cos\varphi$ or $\cos(\varphi)$ (same for $\sin$). Don't mix both notations in one proof.}}\\
						   &= \left(x,x\right)_X - 2\left(\cos\varphi\operatorname{Re}(w)+\sin\varphi\operatorname{Im}(w)\right)+ \left(z,z\right)_X
				\end{split}
			  \end{equation}
			  Utilizing optimization methods for continuous and differentiable $\mathbb{R} \rightarrow \mathbb{R}$ functions \cite{Boyd2004}\cite{Nocedal2006} and also \cref{lemma:inverse_a_sin_b_cos}:\todo{The following is correct, but the formulation of \cref{lemma:inverse_a_sin_b_cos} makes this tedious to check. One has to use $\theta=-\varphi$, $\varphi=\theta$ and $\sin(-\varphi)=-\sin(\varphi)$. I would definitely change the notation of the Lemma, by switching the symbold $\theta$ and $\varphi$.}
			  \begin{equation}
				\begin{split}
				\frac{\mathrm{d}f(\varphi)}{\mathrm{d}\varphi} &= - 2\left(-\sin\varphi\operatorname{Re}(w)+\cos\varphi\operatorname{Im}(w)\right) = 2\sqrt{\operatorname{Re}(w)^2+\operatorname{Im}(w)^2}\sin(\varphi-\theta)\\ 
				\end{split}
			  \end{equation}
			  where $\cos\theta = \frac{\operatorname{Re}(w)}{\sqrt{\operatorname{Re}(w)^2+\operatorname{Im}(w)}}$ and $\sin\theta = \frac{\operatorname{Im}(w)}{\sqrt{\operatorname{Re}(w)^2+\operatorname{Im}(w)^2}}$.
        \todo{Why do you need two formulations of the second dervative? Isn't the formulation on the very right what you need?}
			  \begin{equation}
				\begin{split}
				\frac{\mathrm{d}^2f(\varphi)}{\mathrm{d}\varphi^2} &= - 2\left(-\cos\varphi\operatorname{Re}(w)-\sin\varphi\operatorname{Im}(w)\right) = -2\sqrt{\operatorname{Re}(w)^2+\operatorname{Im}(w)^2}\cos(\varphi-\theta)\\ 
				\end{split}
			  \end{equation}
        For $\varphi^\ast-\varphi = (2k+1)\pi, k \in \mathbb{Z}$ \todo{What does this mean? There is no $\varphi$ from which you can construct the $\varphi^*$. You need to specify $\varphi^*$ where the minimum is attained. I guess you want to do that by specifiying all zeros of the first derivative on $[0,2\pi)$, then checking the sign of the second derivative, finding out that only one of the zeros of the first deriviate has positive sign, concluding that this is a local minium. Thus, there is only one local minimum in $[0,2\pi)$ and since you know that the minimum is attained from the beginning of the proof, this local minimum must be a global minimum, which is what you wanted. If you fill in the gaps, you also get the explicit formula for the minimum.} and restricting ourselves to $\varphi \in [0,2\pi)$ we would arrive at the unique $\arg\min$ $\varphi^*$ corresponding to the $\min$.
		\end{Proof}


  
\section{Difficulties}

The loss function in \cref{eq:loss_function} is non-convex \cite{Candes2014}. Set $n=1$, $m=2$, $\boldsymbol{x}_1 = \begin{pmatrix}1+i\end{pmatrix}^{1 \times 1}$, 
$\boldsymbol{x}_2 = \begin{pmatrix}-1-i\end{pmatrix}^{1 \times 1}$, $\boldsymbol{A}=\begin{pmatrix}1\\i \end{pmatrix}^{2 \times 1}$, 
$\boldsymbol{y}=\begin{pmatrix}1\\2 \end{pmatrix}^{2 \times 1}$, and $\lambda=1/2$ to build a counterexample. Non-convexity is bad news for 
optimization as most of the optimization methods are build on top of convexity \cite{Boyd2004}\cite{Nocedal2006}. To make the matter worse the loss function is not 
holomorphic\footnote{Cauchy-Riemann equations do not hold. You can take a look at the eleventh chapter of \cite{Rudin1987} for further details.} 
which in turn makes arriving at the gradient descent like structure in \cref{eq:gradient_pr_solution} more involved.

\section{Motivating Application}\label{sec:motivating_application}

Since introducing complex numbers introduced a couple of quite difficult obstacles in the math part and will introduce others in the implementation part 
we think we owe it to the reader to motivate the assumption by giving one application which is imaging using \ac{CDP} coupled with \ac{WF} variants.

\section{Diffracted Imaging}\label{sec:diffracted_imaging}

\todo{You at least need to describe our forward model, i.e., explain how we assume data to be obtained from the image
 we want to know. Then it's at least mathematically obvious why the phase is lost.} 


Without going too much into the Physics of it Consider a ray that is emitted onto an object of interest and the diffracted rays are collected and 
measured at some distance from the object. The goal is to reconstruct the image using the said measurements. If the distance between 
the sample and the detector is far enough, characterized by the Fraunhofer condition\cite{Lipson1995}, then the solution of the diffraction 
problem is well approximated by the \ac{FT} of the emitted ray. Due to quantum mechanical effects\cite{DGDS2018}\cite{FranzSchwabl2007} and material limitations\cite{Shechtman2015} 
the phase of the \ac{FT} can not be measured (we can only measure the amplitude of the \ac{FT}) therefore we encounter the \pp problem. In summary in the discrete 1D case we have:
\begin{equation}
	y_k = \left| \sum_{t=0}^{n-1} x[t] e^{-i2\pi\omega_kt} \right|^2 , \qquad \omega_k \in \Omega, \qquad \boldsymbol{x} \in \mathbb{C}^n , \qquad \boldsymbol{y} \in \mathbb{R}^m, \qquad m > n 
\end{equation}
where $\boldsymbol{x}$ is the emitted ray and $\boldsymbol{y}$ is the measurement. By some physics technics\cite{Loewen2018}\cite{Candes2011} you can modulate 
the ray before it diffracts which in 1D discrete setting becomes:
\begin{equation}\label{eq:modulation_effect}
	y_k = \left| \sum_{t=0}^{n-1} x[t]\overline{d[t]} e^{-i2\pi\omega_kt} \right|^2 , \qquad \omega_k \in \Omega
\end{equation}

If we do the process using $L$ different modulations/masks we would be having:
\begin{equation}\label{eq:l_modulation_effect}
	y_{l,k} = \left| \sum_{t=0}^{n-1} x[t]\overline{d[t]} e^{-i2\pi\omega_kt} \right|^2 \quad \text{for} \quad \begin{split}
	0 &\leq k \leq n-1\\
	1 &\leq l \leq L
	\end{split}
  .
  \end{equation}
\cite{ECXLMS2013} proved that if the number of modulations/masks $L$ is high enough, then the recovery of the ray is possible using \acl*{PR} methods. 
\cite{Gross2017} furthered improved the bounds in which the recovery is possible. The synthetic example would be to assume the ray to be a natural image and 
apply \ac{WF} variants to get a better and natural feeling of \acl*{PR} process. The \ac{WF} vs the \ac{TWF} can be seen in \cref{image:wf_vs_twf}, \ac{WF} vs \ac{RWF} in \cref{image:wf_vs_rwf}, and finally 
\ac{TWF} vs \ac{RWF} in \cref{image:twf_vs_rwf}.

There are however some restrictions regarding the $d[t]$ in \cref{eq:modulation_effect} and \cref{eq:l_modulation_effect}. \cite{ECXLMS2013} proposed:
\begin{equation}
	\mathbb{E}\left[d\right] = 0, \qquad \mathbb{E}\left[d^2\right] = 0, \qquad\mathbb{E}\left[\left|d\right|^4\right] = 2\mathbb{E}\left[\left|d\right|^2\right]^2
\end{equation}
where $\mathbb{E}$ is the expectation value operator and $d$ the random variable, as the admissibility condition and proposed two settings where the conditions are met:
\begin{itemize}
	\item first setting nicknamed \emph{Ternary Modulation}\footnote{as there are 3 different values(outcomes) for the random variable $d$}
	\begin{equation*}
	  d =
		  \begin{cases}
			  +1 & \text{with prob.  $1/4$}\\
			  0 & \text{with prob.  $1/2$}\\
			  -i & \text{with prob.  $1/4$}
		  \end{cases}  
	\end{equation*}
	\item second setting nicknamed \emph{Octanary Modulation}\footnote{as there are 8 different values(outcomes) for the random variable $d$}
	\begin{equation*}
		d = b_1b_2 \qquad \text{such that} \qquad
	  b_1 =
		  \begin{cases}
			  +1 & \text{with prob.  $1/4$}\\
			  -1 & \text{with prob.  $1/4$}\\
			  -i & \text{with prob.  $1/4$}\\
			  +i & \text{with prob.  $1/4$}\\
	
		  \end{cases}  
		  \qquad \text{and} \qquad 
	  b_2 
		  \begin{cases}  
			+\sqrt{2}/2 & \text{with prob.  $4/5$}\\
			+\sqrt{3} & \text{with prob.  $1/5$}\\
		\end{cases}   
	\end{equation*}
\end{itemize}

As it is stated by \cite{ECXLMS2013} different \acl*{CDP}s might be better suited for different applications so naturally 
the possibility exists that the practitioners in their respective field want to experiment with different $d$s. Therefore 
we provide the double checking of the admissibility conditions for the \emph{Octanary Modulation} proposed by \cite{Candes2014} 
in case the reader is in fact one of those practitioners.

\begin{Thm}\label{theorem:expectation_general}
  Let $X$ and $Y$ be independent discrete random variables. For expectation value of functions of our random variables we would be having:
\begin{equation}
  \mathbb{E}(f(X,Y)) = \sum_{X=x}^{}\sum_{Y=y}^{}f(x,y)p(x)p(y)
\end{equation}
  where $p(x)$ and $p(y)$ are the probability of $x$ and $y$ being realized from the random variables $X$ and $Y$.
\end{Thm}
% \begin{Proof}
  % The general case where $X$ and $Y$ are not independent is usually discussed. Then by assuming an independent setting 
  % you will arrive at the conclusion \cite{DasGupta2010}\cite{DasGupta2011}.
% \end{Proof}
\begin{Lem}\label{lemma:expectation_multiplication}
Let $X$ and $Y$ be independent discrete random variables. We would be having:
\begin{equation}
  \mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)
\end{equation}
\end{Lem}

\begin{Prop}
	The Octanary Modulation setting is admissible by the criteria proposed by\cite{Candes2014}. 
\end{Prop}

\begin{Proof}
	By using \cref{lemma:expectation_multiplication} and \cref{theorem:expectation_general} we have:
	\begin{equation*}
		\begin{split}
		E(d) &= \left(+1 \times \frac{\sqrt{2}}{2}\right) \times \frac{4}{20} + \left(+1 \times \sqrt{3}\right) \times \frac{1}{20}+\left(-1 \times \frac{\sqrt{2}}{2}\right) \times \frac{4}{20}+\left(-1 \times \sqrt{3}\right) \times \frac{1}{20}\\
		     &+ \left(+i \times \frac{\sqrt{2}}{2}\right) \times \frac{4}{20} + \left(+i \times \sqrt{3}\right) \times \frac{1}{20}+\left(-i \times \frac{\sqrt{2}}{2}\right) \times \frac{4}{20}+\left(-i \times \sqrt{3}\right) \times \frac{1}{20}\\ 
			 &= 0
		\end{split}
	  \end{equation*}
	  \begin{equation*}
		\begin{split}
		E(d^2) &= \left(+1 \times \frac{1}{2}\right) \times \frac{4}{20} + \left(+1 \times 3\right) \times \frac{1}{20}+\left(+1 \times \frac{1}{2}\right) \times \frac{4}{20}+\left(+1 \times 3\right) \times \frac{1}{20}\\
		     &+ \left(-1 \times \frac{1}{2}\right) \times \frac{4}{20} + \left(-1 \times 3\right) \times \frac{1}{20}+\left(-1 \times \frac{1}{2}\right) \times \frac{4}{20}+\left(-1 \times 3\right) \times \frac{1}{20}\\ 
			 &= 0
		\end{split}
	 \end{equation*}
	 \begin{equation*}
		\begin{split}
		E(\left|d\right|^2) &= \left(\left|+1 \times \frac{\sqrt{2}}{2} \right|\right)^2\times \frac{4}{20} + \left(\left|+1 \times \sqrt{3}\right|\right)^2\times \frac{1}{20}\\
		                    &+\left(\left|-1 \times \frac{\sqrt{2}}{2}\right|\right)^2\times \frac{4}{20}+\left(\left|-1 \times \sqrt{3}\right|\right)^2\times \frac{1}{20}\\
		                    &+ \left(\left|+i \times \frac{\sqrt{2}}{2}\right|\right)^2\times \frac{4}{20} + \left(\left|+i \times \sqrt{3}\right|\right)^2\times \frac{1}{20}\\
							&+\left(\left|-i \times \frac{\sqrt{2}}{2}\right|\right)^2\times \frac{4}{20}+\left(\left|-i \times \sqrt{3}\right|\right)^2\times \frac{1}{20}\\ 
			 &= 1
		\end{split}
	  \end{equation*}
	  \begin{equation*}
		\begin{split}
		E(\left|d\right|^4) &= \left(\left|+1 \times \frac{\sqrt{2}}{2}\right|\right)^4 \times \frac{4}{20} + \left(\left|+1 \times \sqrt{3}\right|\right)^4 \times \frac{1}{20}\\
		                    &+\left(\left|-1 \times \frac{\sqrt{2}}{2}\right|\right)^4 \times \frac{4}{20}+\left(\left|-1 \times \sqrt{3}\right|\right)^4 \times \frac{1}{20}\\
		                    &+ \left(\left|+i \times \frac{\sqrt{2}}{2}\right|\right)^4 \times \frac{4}{20} + \left(\left|+i \times \sqrt{3}\right|\right)^4 \times \frac{1}{20}\\
							&+\left(\left|-i \times \frac{\sqrt{2}}{2}\right|\right)^4 \times \frac{4}{20}+\left(\left|-i \times \sqrt{3}\right|\right)^4 \times \frac{1}{20}\\ 
			 &= 2
		\end{split}
	 \end{equation*}
	 which closes the proof.
\end{Proof}





\section{Wirtinger FLow Variants}
\ac{WF} variants emerged as a response to solve certain settings in \ac{PR} problems. As the same suggests \ac{PR} is the process of 
determining the phase(up to a global phase) of an image simply because it contains information which is of interest depending on the context

 

We first discuss why phase is important by giving a synthetic example. Then we proceed to formulate mathematical formulation and in quick succession 
the variants where the \ac{WF} variants are based on. We give one application of the \ac{WF} variants which is used in imaging 
to both motivate the reasoning behind considering complex numbers in our formulation and motivate the reader by giving a synthetic example based on 
natural images. 


 




\cite{Candes2014} were the first to coin the term \ac{WF} paying homage to \emph{Wirtinger Derivative}. 

















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% WF Variants CDP Reconstruction Comparison %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
  % \clearpage % Start a new page
  \thispagestyle{empty} % No header/footer on this page
  \begin{figure}[!htbp]
    \centering
    % \captionsetup{justification=centering}
    \includegraphics[width=1.0\textwidth]{./images/cdp/out_wf_twf.png}
  \caption{\ac{WF}(left) vs \ac{TWF}(right) Using Coded Diffraction Patterns for Retrieval of the Sat Phone Image from Top to Buttom: After Initialization, 
	at Iteration $=120$, at Iteration $=350$ for \ac{WF} and at Iteration $=200$ for \ac{TWF}, and the Original Image}
  \label{image:wf_vs_twf}
  \end{figure}
  % \clearpage % End the page
}
\afterpage{%
  % \clearpage % Start a new page
  \thispagestyle{empty} % No header/footer on this page
  \begin{figure}[!htbp]
    \centering
    % \captionsetup{justification=centering}
    \includegraphics[width=1.0\textwidth]{./images/cdp/out_wf_rwf.png}
  \caption{\ac{WF}(left) vs \ac{RWF}(right) Using Coded Diffraction Patterns for Retrieval of the Sat Phone Image from Top to Buttom: After Initialization, 
	at Iteration $=120$, at Iteration $=350$ for \ac{WF} and at Iteration $=150$ for \ac{RWF}, and the Original Image}
  \label{image:wf_vs_rwf}
  \end{figure}
  % \clearpage % End the page
}
\afterpage{%
  % \clearpage % Start a new page
  \thispagestyle{empty} % No header/footer on this page
  \begin{figure}[!htbp]
    \centering
    % \captionsetup{justification=centering}
    \includegraphics[width=1.0\textwidth]{./images/cdp/out_twf_rwf.png}
  \caption{\ac{TWF}(left) vs \ac{RWF}(right) Using Coded Diffraction Patterns for Retrieval of the Sat Phone Image from Top to Buttom: After Initialization, 
	at Iteration $=25$, at Iteration $=200$ for \ac{TWF} and at Iteration $=150$ for \ac{RWF}, and the Original Image}
  \label{image:twf_vs_rwf}
  \end{figure}
  % \clearpage % End the page
}










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%% Summary of WF* variants %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}
	\centering
	\begin{tabular}{||c l c||} 
	 \hline
	 \ac{WF} Variant & $\varphi$ 						& loss functions\\ [0.5ex] 
	 \hline\hline
	 \ac{WF}\index{WF}                & $\left|\boldsymbol{z}\right|^2$ 	& quadratic 	\\
	 \ac{TWF}\index{TWF}   & $\left|\boldsymbol{z}\right|^2$ 	& quadratic 	\\
	 \ac{ITWF}\index{ITWF}  & $\left|\boldsymbol{z}\right|^2$   & quadratic 	\\
	 \ac{RWF}\index{RWF}  & $\left|\boldsymbol{z}\right|$ 	& quadratic 	\\
	 \ac{IRWF}\index{IRWF}   & $\left|\boldsymbol{z}\right|$ 	& quadratic 	\\
	 \ac{IMRWF}\index{IMRWF}   & $\left|\boldsymbol{z}\right|$ 	& quadratic 	\\ [1ex]
	 \hline
	\end{tabular}
	\caption{$\varphi$ and the loss function used in \cite{Candes2014}, \cite{Chen2015}, \cite{Kolte2016}, and \cite{Zhang2016}}
	\label{tab:formulation}
	\end{table}




	The whole thing about \ac{WF} variants started with the seminal work of \cite{Candes2014}.
	The most important improvements chronologically were done by \cite{Chen2015}, \cite{Kolte2016}, and\cite{Zhang2016}
	with the nicknames of \ac{TWF}, \ac{ITWF}, \ac{RWF}, \ac{IRWF}, and \ac{IMRWF}.
	For a quite extensive survey on \ac{WF} variants please refer to Liu et al.\cite{Liu2019}. Chandra et al.\cite{Chandra2017} 
	gathered quite number of \emph{Phase Retrieval} methods including a couple of \emph{\ac{WF}} variants in the MATLAB\textregistered\space 
	problem solving environment in a uniform manner.\\
	We quickly go over the problem formulation, difficulties, algorithms, and at the of the chapter we give some numerical experiments we are going
	to refer to in the subsequent chapters. Assume $\varphi$ to be squared element-wise absolute value and the loss function to be quadratic. 
	The summary for all the variants in terms of formulation is in table\ref{tab:formulation}  
	
	
	
	
	
	
	
	\index{Scalar Product}
	
	




\section{Implementation}

As mentioned previously \du/\au looks like the \rnns but it is not therefore it is not possible to use the higher level functions of the usual 
\ml/\dl frameworks\footnote{At the time of writing \tensorflow\cite{Abadi2016}, \keras\cite{Chollet2023}, and \pytorch\cite{Paszke2019} are the 
main players in the \ml/\dl world.}. To put it explicitly there is no:
\begin{itemize}
	\item real-time visualization of the training process vs epochs\footnote{Which is very nice feature to have and makes it easy to stop the training process 
  and save the 
  model the moment you feel that the loss function on the train and the test data are deviating from each other}.
	\item graph based visualization\footnote{\tensorflow accomplishes this for the already defined layers using the \graphviz package.} of the dependency of the parameters in the layers.
	\item high level splitting and sampling mechanisms
	\item automatic \ac{GPU} acceleration. 
\end{itemize}

There are also other difficulties coming from the numerics alone:
\begin{itemize}
	\item suggested learning rates are mostly safe to use with known architectures
	\item when input and outputs are not normalized according to the common practices, training becomes more difficult(not possible to normalize them since they are coming from context inspired algorithms)
	\item the introduction of \ac{CVNN}\cite{CTOBYZDSSSJFSSMNRYBCP2017}\cite{Bassey2021}\cite{Barrachina2023} hinders what is under the hood in the \ml/\dl frameworks.
\end{itemize}

While writing the helping tools and even the \ac{GPU} acceleration is not that difficult and can be done single-handedly
\footnote{Fran\c{c}ois Chollet wrote \keras\cite{Chollet2023} on top of \tensorflow\cite{Abadi2016} all be himself which later was acquired by \google.}, the \ac{CVNN} is another beast. 
One of the beating hearts of \ml/\dl frameworks is \ad/\cd. It is worth emphasizing that \ac{AD}/\ac{CD} is not symbolic differentiation and there are fundamental differences between the two \cite{Naumann2011}\cite{Griewank2008}. 
The difficulty of \ac{CVNN}\cite{CTOBYZDSSSJFSSMNRYBCP2017}\cite{Bassey2021}\cite{Barrachina2023} is directly coming from the \ac{AD}/\ac{CD} part 
that is used during backpropagation. The funny thing is that the general implementation of \ac{CVNN} requires the understanding of the 
\emph{Wirtinger Derivatives}\cite{Wirtinger1927} which \cite{Candes2014} payed homage to by including the \ac{WF} name in the title of their paper\cite{Candes2014}. 
For theory and implementation of \ac{CVNN} please refer to \cite{Wirtinger1927}\cite{KreutzDelgado2009}\cite{Bassey2021}\cite{Barrachina2023}\cite{CTOBYZDSSSJFSSMNRYBCP2017}. For this sole reason 
we used \pytorch\cite{Paszke2019} as our frameworks as it seamlessly supports \ac{CVNN} using \emph{Wirtinger Calculus}\cite{Fischer2002} and in turn 
\emph{Wirtinger Derivatives}\cite{Wirtinger1927}.


\todo{This is apparently just a fragment that it supposed to be extended. Note that we don't know yet 
(unless the introduction says this) that we need complex values in the network.}


complex valued neural networks have other applications too \cite{Bassey2021} \cite{Barrachina2023} \cite{CTOBYZDSSSJFSSMNRYBCP2017}


\cite{Wirtinger1927} \cite{KreutzDelgado2009} \cite{Fischer2002}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% WF Variants Algorithms %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% Wirtinger Flow %%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
\clearpage % Start a new page
\begin{algorithm}
  \caption{\ac{WF}\index{WF} suggested by \cite{Candes2014}}\label{pseudocode:wf}
    \textbf{Input}: Let $\boldsymbol{y}=\{y_i\}_{i=1}^m$ and $\{\boldsymbol{a}_i\}_{i=1}^m$ be our measurements and sampling vectors; \\
    \textbf{Parameters}:  step size $\mu$;\\
    \textbf{Initialization}: Let $\hat{\boldsymbol{z}}$, be the eigenvector corresponding to the largest eigenvalue of:
    \begin{equation}
      \boldsymbol{Y} \coloneqq \frac{1}{m}\sum_{i=1}^m y_i\boldsymbol{a}_i \boldsymbol{a}_i^*\boldsymbol{1}
    \end{equation}
    and find it using power iteration. Let also $\Lambda=\sqrt{n\sum_{i=1}^m \frac{y_i}{\|\boldsymbol{a}_i\|_X^2}}$ to be the norm estimation of the signal of interest that we are trying to recover. 
    Set $\boldsymbol{z}^{(0)}=\Lambda\hat{\boldsymbol{z}}$ for the initialization.\\
    \textbf{Update loop}: for $t=0, \ldots ,t=T-1$ do the update as:
    \begin{flalign}
      \boldsymbol{z}^{(t+1)}=\boldsymbol{z}^{(t)}- \frac{\mu_{t+1}}{\left|z_0\right|_X^2}\left(\frac{1}{m}\sum_{i=1}^{m}\left(\left|\boldsymbol{a}_i^*\boldsymbol{z}^{(t)}\right|_X-y_i\right)\left(\boldsymbol{a}_i\boldsymbol{a}_i^*\right)\boldsymbol{z}^{(t)}\right).
    \end{flalign}
    \textbf{Output}: $\boldsymbol{z}^{(T)}$.
  \end{algorithm}
  \begin{itemize}
    \item Power iteration is described in almost any standard numerical linear algebra book like \cite{Trefethen2022}\cite{Demmel1997}\cite{Golub2013}. 
    \item If vectorized operations are still a thing in the future, try to formulate the algorithm in the most vectorized form possible in whatever 
    numerical framework you are using. For a first exposure to vectorized operations you can have a look at \cite{Hager2010}. For in depth understanding of 
    computer architecture that leads to the vectorized operations please refer to either \cite{Patterson2014} or \cite{Hennessy2019}.
    \item Try to port as much as computation possible from \ac{CPU} to accelerators like \ac{GPU}s and \ac{TPU}s if 
    your analysis on the porting confirms it to be worthwhile.
  \end{itemize}
  A possible \pytorch\cite{Paszke2019} implementation with \ac{CUDA}\cite{Nvidia} acceleration is listed below as:
  % \lstinputlisting[language=Python, firstline=1,lastline=20]{./algorithms/wf.py}
  \lstinputlisting[language=Python]{./algorithms/wf.py}\index{WF}\label{code:wf}    
\clearpage % End the page
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% Truncated Wirtinger Flow %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
\clearpage % Start a new page
\begin{algorithm}
  \caption{\ac{TWF}\index{TWF} suggested by \cite{Chen2015}}\label{pseudocode:twf}
    \textbf{Input}: Let $\boldsymbol{y}=\{y_i\}_{i=1}^m$ and $\{\boldsymbol{a}_i\}_{i=1}^m$ be our measurements and sampling vectors; \\
    \textbf{Parameters}: step size $\mu$, and thresholds $\alpha_{z}^{lb},\alpha_{z}^{ub},\alpha_{h},\alpha_{y}$;\\
    \textbf{Initialization}: Let $\hat{\boldsymbol{z}}$, be the eigenvector corresponding to the largest eigenvalue of:
    \begin{equation}
      \boldsymbol{Y} \coloneqq \frac{1}{m}\sum_{i=1}^m y_i\boldsymbol{a}_i \boldsymbol{a}_i^*\boldsymbol{1}_{\left\{\left|y_i\right|_X\leq \frac{\alpha_{y}^2\sum_{i=1}^{m}y_i}{m}\right\}}
    \end{equation}
    and find it using power iteration. Let also $\Lambda=\sqrt{n\sum_{i=1}^m \frac{y_i}{\|\boldsymbol{a}_i\|_X^2}}$ to be the norm estimation of the signal of interest that we are trying to recover. 
    Set $\boldsymbol{z}^{(0)}=\Lambda\hat{\boldsymbol{z}}$ for the initialization.\\
    \textbf{Update loop}: for $t=0, \ldots ,t=T-1$ do the update as:
    \begin{flalign}
      \boldsymbol{z}^{(t+1)}=\boldsymbol{z}^{(t)}- \frac{\mu_{t+1}}{{\boldsymbol{z}^{(t)}}^*\boldsymbol{a}_i}\left(\frac{1}{m}\sum_{i=1}^{m}2\left(\left|\boldsymbol{a}_i^*\boldsymbol{z}^{(t)}\right|_X^2-y_i\right)\boldsymbol{a}_i\boldsymbol{1}_{\mathcal{E}_1^i\cap\mathcal{E}_2^i}\right).
    \end{flalign}
    where
    \begin{equation*}
      \begin{split}
        \mathcal{E}_1^i &\coloneqq \left\{  \alpha_z^{lb}  \leq \frac{\sqrt{n}\left|\boldsymbol{a}_i^*\boldsymbol{z}^{(t)}\right|_X}{\left|\boldsymbol{a}_i\right|_X\left|\boldsymbol{z}^{(t)}\right|_X} \leq  \alpha_z^{ub}   \right\} \\
        \mathcal{E}_2^i &\coloneqq \left\{ \left|y_i-\left|\boldsymbol{a}_i^*\boldsymbol{z}^{(t)}\right|_X^2\right| \leq  \alpha_h{K_t}\frac{\sqrt{n}\left|\boldsymbol{a}_i^*\boldsymbol{z}^{(t)}\right|_X}{\left|\boldsymbol{a}_i\right|_X\left|\boldsymbol{z}^{(t)}\right|_X}   \right\} \\
        K_t             &\coloneqq \frac{1}{m}\sum_{j=1}^{m}\left|y_j-\left|\boldsymbol{a}_i^*\boldsymbol{z}^{(t)}\right|_X^2\right|
      \end{split}
    \end{equation*}
    \textbf{Output}: $\boldsymbol{z}^{(T)}$.
  \end{algorithm}
  \begin{itemize}
    \item Power iteration is described in almost any standard numerical linear algebra book like \cite{Trefethen2022}\cite{Demmel1997}\cite{Golub2013}. 
    \item If vectorized operations are still a thing in the future, try to formulate the algorithm in the most vectorized form possible in whatever 
    numerical framework you are using. For a first exposure to vectorized operations you can have a look at \cite{Hager2010}. For in depth understanding of 
    computer architecture that leads to the vectorized operations please refer to either \cite{Patterson2014} or \cite{Hennessy2019}.
    \item Try to port as much as computation possible from \ac{CPU} to accelerators like \ac{GPU}s and \ac{TPU}s if 
    your analysis on the porting confirms it to be worthwhile.
  \end{itemize}
  A possible \pytorch\cite{Paszke2019} implementation with \ac{CUDA}\cite{Nvidia} acceleration is listed below as:
  % \lstinputlisting[language=Python, firstline=1,lastline=20]{./algorithms/twf.py}
  \lstinputlisting[language=Python]{./algorithms/twf.py}\index{TWF}\label{code:twf}    
\clearpage % End the page
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Reshaped Wirtinger Flow %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
\clearpage % Start a new page
\begin{algorithm}
  \caption{\ac{RWF}\index{RWF} suggested by \cite{Zhang2016}}\label{pseudocode:rwf}
    \textbf{Input}: Let $\boldsymbol{y}=\{y_i\}_{i=1}^m$ and $\{\boldsymbol{a}_i\}_{i=1}^m$ be our measurements and sampling vectors; \\
    \textbf{Parameters:}  step size $\mu$, and thresholds $\alpha_{l},\alpha_{u}$;\\
    \textbf{Initialization}: Let $\hat{\boldsymbol{z}}$, be the eigenvector corresponding to the largest eigenvalue of:
    \begin{equation}
      \boldsymbol{Y} \coloneqq \frac{1}{m}\sum_{i=1}^m y_i\boldsymbol{a}_i \boldsymbol{a}_i^*\boldsymbol{1}_{\left\{ \alpha_{l}n\sum_{i=1}^m \frac{y_i}{\|\boldsymbol{a}_i\|_1} \leq \left|y_i\right|_X \alpha_{u}n\sum_{i=1}^m \frac{y_i}{\|\boldsymbol{a}_i\|_1}\right\}}
    \end{equation}
    and find it using power iteration. Let also $\Lambda=n\sum_{i=1}^m \frac{y_i}{\|\boldsymbol{a}_i\|_1}$ to be the norm estimation of the signal of interest that we are trying to recover. 
    Set $\boldsymbol{z}^{(0)}=\Lambda\hat{\boldsymbol{z}}$ for the initialization.\\
    \textbf{Update loop}: for $t=0, \ldots ,t=T-1$ do the update as:
    \begin{flalign}
      \boldsymbol{z}^{(t+1)}=\boldsymbol{z}^{(t)}- \mu_{t+1}\left(\frac{1}{m}\sum_{i=1}^{m}\left(\boldsymbol{a}_i^*\boldsymbol{z}^{(t)}-y_i\frac{\boldsymbol{a}_i^*\boldsymbol{z}^{(t)}}{\left|\boldsymbol{a}_i^*\boldsymbol{z}^{(t)}\right|_X}\right)\boldsymbol{a}_i\right).
    \end{flalign}
    \textbf{Output}: $\boldsymbol{z}^{(T)}$.
  \end{algorithm}
  \begin{itemize}
    \item Power iteration is described in almost any standard numerical linear algebra book like \cite{Trefethen2022}\cite{Demmel1997}\cite{Golub2013}. 
    \item If vectorized operations are still a thing in the future, try to formulate the algorithm in the most vectorized form possible in whatever 
    numerical framework you are using. For a first exposure to vectorized operations you can have a look at \cite{Hager2010}. For in depth understanding of 
    computer architecture that leads to the vectorized operations please refer to either \cite{Patterson2014} or \cite{Hennessy2019}.
    \item Try to port as much as computation possible from \ac{CPU} to accelerators like \ac{GPU}s and \ac{TPU}s if 
    your analysis on the porting confirms it to be worthwhile.
  \end{itemize}
  A possible \pytorch\cite{Paszke2019} implementation with \ac{CUDA}\cite{Nvidia} acceleration is listed below as:
  % \lstinputlisting[language=Python, firstline=1,lastline=20]{./algorithms/rwf.py}
  \lstinputlisting[language=Python]{./algorithms/rwf.py}\index{RWF}\label{code:rwf}     
\clearpage % End the page
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% Incrementally Reshaped Wirtinger Flow %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
\clearpage % Start a new page
\begin{algorithm}
  \caption{\ac{IRWF}\index{IRWF} suggested by \cite{Zhang2016}}\label{pseudocode:irwf}
    \textbf{Input}: Let $\boldsymbol{y}=\{y_i\}_{i=1}^m$ and $\{\boldsymbol{a}_i\}_{i=1}^m$ be our measurements and sampling vectors; \\
    \textbf{Parameters:}  step size $\mu$, and thresholds $\alpha_{l},\alpha_{u}$;\\
    \textbf{Initialization}: Let $\hat{\boldsymbol{z}}$, be the eigenvector corresponding to the largest eigenvalue of:
    \begin{equation}
      \boldsymbol{Y} \coloneqq \frac{1}{m}\sum_{i=1}^m y_i\boldsymbol{a}_i \boldsymbol{a}_i^*\boldsymbol{1}_{\left\{ \alpha_{l}n\sum_{i=1}^m \frac{y_i}{\|\boldsymbol{a}_i\|_1} \leq \left|y_i\right|_X \alpha_{u}n\sum_{i=1}^m \frac{y_i}{\|\boldsymbol{a}_i\|_1}\right\}}
    \end{equation}
    and find it using power iteration. Let also $\Lambda=n\sum_{i=1}^m \frac{y_i}{\|\boldsymbol{a}_i\|_1}$ to be the norm estimation of the signal of interest that we are trying to recover. 
    Set $\boldsymbol{z}^{(0)}=\Lambda\hat{\boldsymbol{z}}$ for the initialization.\\
    \textbf{Update loop}: for $t=0, \ldots ,t=T-1$ do the update as:
    \begin{flalign}
      \boldsymbol{z}^{(t+1)}=\boldsymbol{z}^{(t)}- \mu_{t+1}\left(\frac{1}{m}\sum_{i=1}^{m}\left(\boldsymbol{a}_{i_t}^*\boldsymbol{z}^{(t)}-y_{i_t}\frac{\boldsymbol{a}_{i_t}^*\boldsymbol{z}^{(t)}}{\left|\boldsymbol{a}_{i_t}^*\boldsymbol{z}^{(t)}\right|_X}\right)\boldsymbol{a}_i\right).
    \end{flalign}
    where $i_t$ is randomly selected from the set $\left\{1,\cdots,m\right\}$.\\
    \textbf{Output}: $\boldsymbol{z}^{(T)}$.
  \end{algorithm}
  \begin{itemize}
    \item Power iteration is described in almost any standard numerical linear algebra book like \cite{Trefethen2022}\cite{Demmel1997}\cite{Golub2013}. 
    \item If vectorized operations are still a thing in the future, try to formulate the algorithm in the most vectorized form possible in whatever 
    numerical framework you are using. For a first exposure to vectorized operations you can have a look at \cite{Hager2010}. For in depth understanding of 
    computer architecture that leads to the vectorized operations please refer to either \cite{Patterson2014} or \cite{Hennessy2019}.
    \item Try to port as much as computation possible from \ac{CPU} to accelerators like \ac{GPU}s and \ac{TPU}s if 
    your analysis on the porting confirms it to be worthwhile.
  \end{itemize}
  A possible PyTorch\cite{Paszke2019} implementation with \ac{CUDA}\cite{Nvidia} acceleration is listed below as:
  % \lstinputlisting[language=Python, firstline=1,lastline=20]{./algorithms/wf.py}
  \lstinputlisting[language=Python]{./algorithms/irwf.py}\index{IRWF}\label{code:irwf}    
\clearpage % End the page
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Incrementally Mini-Batch Reshaped Wirtinger Flow %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
\clearpage % Start a new page
\begin{algorithm}
  \caption{\ac{IMRWF}\index{IMRWF} suggested by \cite{Zhang2016}}\label{pseudocode:imrwf}
    \textbf{Input}: Let $\boldsymbol{y}=\{y_i\}_{i=1}^m$ and $\{\boldsymbol{a}_i\}_{i=1}^m$ be our measurements and sampling vectors; \\
    \textbf{Parameters:}  step size $\mu$, and thresholds $\alpha_{l},\alpha_{u}$, batch size $k$;\\
    \textbf{Initialization}: Let $\hat{\boldsymbol{z}}$, be the eigenvector corresponding to the largest eigenvalue of:
    \begin{equation}
      \boldsymbol{Y} \coloneqq \frac{1}{m}\sum_{i=1}^m y_i\boldsymbol{a}_i \boldsymbol{a}_i^*\boldsymbol{1}_{\left\{ \alpha_{l}n\sum_{i=1}^m \frac{y_i}{\|\boldsymbol{a}_i\|_1} \leq \left|y_i\right|_X \alpha_{u}n\sum_{i=1}^m \frac{y_i}{\|\boldsymbol{a}_i\|_1}\right\}}
    \end{equation}
    and find it using power iteration. Let also $\Lambda=n\sum_{i=1}^m \frac{y_i}{\|\boldsymbol{a}_i\|_1}$ to be the norm estimation of the signal of interest that we are trying to recover. 
    Set $\boldsymbol{z}^{(0)}=\Lambda\hat{\boldsymbol{z}}$ for the initialization.\\
    \textbf{Update loop}: for $t=0, \ldots ,t=T-1$ do the update as:
    \begin{flalign}
      \boldsymbol{z}^{(t+1)}=\boldsymbol{z}^{(t)}- \mu_{t+1}\boldsymbol{A}_{\Gamma_t}\left(\boldsymbol{A}_{\Gamma_t}\boldsymbol{z}^{(t)}-\boldsymbol{y}_{\Gamma_t} \odot \frac{\boldsymbol{A}_{\Gamma_t}\boldsymbol{z}^{(t)}}{\left|\boldsymbol{A}_{\Gamma_t}\boldsymbol{z}^{(t)}\right|_{e_X}}\right).
    \end{flalign}
    where $\Gamma_t$, the index vector, is randomly selected from the subsets of $\left\{1,\cdots,m\right\}$ that has $k$ members. $\boldsymbol{A}_{\Gamma_t}, \boldsymbol{y}_{\Gamma_t}$ 
    correspond to the index vector taken from the original $\boldsymbol{A}$ and $\boldsymbol{y}$ \\
    \textbf{Output}: $\boldsymbol{z}^{(T)}$.
  \end{algorithm}
  \begin{itemize}
    \item Power iteration is described in almost any standard numerical linear algebra book like \cite{Trefethen2022}\cite{Demmel1997}\cite{Golub2013}. 
    \item If vectorized operations are still a thing in the future, try to formulate the algorithm in the most vectorized form possible in whatever 
    numerical framework you are using. For a first exposure to vectorized operations you can have a look at \cite{Hager2010}. For in depth understanding of 
    computer architecture that leads to the vectorized operations please refer to either \cite{Patterson2014} or \cite{Hennessy2019}.
    \item Try to port as much as computation possible from \ac{CPU} to accelerators like \ac{GPU}s and \ac{TPU}s if 
    your analysis on the porting confirms it to be worthwhile.
  \end{itemize}
  A possible \pytorch\cite{Paszke2019} implementation with \ac{CUDA}\cite{Nvidia} acceleration is listed below as:
  % \lstinputlisting[language=Python, firstline=1,lastline=20]{./algorithms/wf.py}
  \lstinputlisting[language=Python]{./algorithms/imrwf.py}\index{IMRWF}\label{code:imrwf}    
\clearpage % End the page
}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% WF vs TWF vs RWF vs IRWF vs IMRWF  %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
%   \clearpage % Start a new page
  \begin{figure}[!htbp]
    \centering
	\captionsetup{justification=centering}
  % \begin{turn}{-90}
    \input{tikz/wf_variants.tex}
  % \end{turn}
  \caption{\ac{WF} vs \ac{TWF} vs \ac{RWF} vs \ac{IRWF} vs \ac{IMRWF}}
    \label{fig:wf_variants}
  \end{figure}
%   \clearpage % End the page
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%% CDPs on Fourier visualization %%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
  % \clearpage % Start a new page
  \thispagestyle{empty} % No header/footer on this page
  \begin{figure}[!htbp]\label{image:cdp_effect_fourier_visual}
    \centering
	\captionsetup{justification=centering}
    \includegraphics[width=1.0\textwidth,height=65em]{./images/mask_effect/regular_fourier_vs_modulated.png}
    \caption{effect of modulation on fourier visualization}
  \end{figure}
  % \clearpage % End the page
}
\afterpage{%
  % \clearpage % Start a new page
  \thispagestyle{empty} % No header/footer on this page
  \begin{figure}[!htbp]\label{image:cdp_effect_measurements_visual}
    \centering
	\captionsetup{justification=centering}
    \includegraphics[width=1.0\textwidth,height=65em]{./images/coded_diffractions_measurements_sat_phone/measurements.png}
    \caption{Measurements on DC\textregistered\space Universe Characters Due to a Random Modulation Plate from Top to Buttom: 
    Red Channel, Green Channel, Blue Channel, and Full RGB}
  \end{figure}
  % \clearpage % End the page
}
\afterpage{%
  % \clearpage % Start a new page
  \thispagestyle{empty} % No header/footer on this pages
  \begin{figure}[!htbp]\label{image:cdp_effect_measurements_zoomed_visual}
    \centering
	\captionsetup{justification=centering}
    \includegraphics[width=1.0\textwidth,height=65em]{./images/coded_diffractions_measurements_zoomed_sat_phone/measurements.png}
    \caption{Measurements on DC\textregistered\space Universe Characters Due to a Random Modulation Plate from Top to Buttom: 
    Red Channel, Green Channel, Blue Channel, and Full RGB Zoomed Version}
  \end{figure}
  % \clearpage % End the page
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% WF, TWF, RWF for CDPs Retrieval of Natural Images %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
%   \clearpage % Start a new page
  \begin{figure}[!htbp]\label{fig:cdp_wf_twf_rwf}
    \subfloat[Natural Image Retrieval Using  \ac{CDP}s Combined with \ac{WF}]{\input{./tikz/wf/wf_error_sat_phone_500_sat_phone.tex}}\\  
    \subfloat[Natural Image Retrieval Using  \ac{CDP}s Combined with \ac{TWF}]{\input{./tikz/twf/twf_error_sat_phone_500_sat_phone.tex}}\\  
    \subfloat[Natural Image Retrieval Using  \ac{CDP}s Combined with \ac{RWF}]{\input{./tikz/rwf/rwf_error_sat_phone_500_sat_phone.tex}}\\  
  \caption{Natural Image Retrieval Using  \ac{CDP}s Combined with \ac{WF}, \ac{TWF}, and \ac{RWF}}
  \end{figure}
%   \clearpage % End the page
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%% msip-Dell Specs %%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
  % \clearpage % Start a new page
  \begin{table}[!htbp]
    \centering
    \begin{tabular}{||l l||} 
     \hline
     General 		                	&  						                                                             \\ [0.5ex] 
     \hline\hline
     Processor 	         		 			& Intel\textregistered \space Xeon\textregistered         	                 \\
     Accelerator 			 	       		& NVIDIA\textregistered  	                                                 \\ 
     Operating System   			    & GNU/Linux(Ubuntu\textregistered) 	                                    	 \\
     Memory 	               			& 32617768 kB                                                              \\ [1ex] 
     \hline
     \hline
     Processor Details 	      		&  						                                                             \\ [0.5ex] 
     \hline\hline
     Architecture     			 			& X86\_64                                               	                 \\ 
     CPU op-mode(s)         			& 32-bit, 64-bit 	                                                         \\
     Address sizes                & 46 bits physical, 48 bits virtual  		                                   \\
     Byte Order                   & Little Endian  	                                                         \\ 
     CPU(s):                      & 8 	 	                                                                   \\
     On-line CPU(s) list:         & 0-7  	                                              	                   \\
     Vendor ID:                   & GenuineIntel\textregistered 	                         	                 \\
     Model name:                  & Intel\textregistered \space Xeon\textregistered CPU E5-1630 v3 at 3.70GHz \\
     Thread(s) per core:          & 2                                                                        \\
     Core(s) per socket:          & 4                                                   		                 \\
     Socket(s):                   & 1 	                                                	                   \\
     CPU max MHz:                 & 3800.0000 	                                         	                   \\
     CPU min MHz:                 & 1200.0000                                         	  	                 \\
     L1d Cache:                   & 128 KiB (4 instances)                           	    	                 \\
     L1i Cache:                   & 128 KiB (4 instances) 	 	                                               \\
     L2 Cache:                    & 1 MiB (4 instances) 	                                                   \\
     L3 Cache:                    & 10 MiB (1 instance) 	 	                                                 \\
     NUMA node(s):                & 1                                               	 	                     \\
     Optimization Flags           & Please Refer to the Intel\textregistered \space Brochure for the Details  		   \\[1ex] 
     \hline
     \hline
     Accelerator Details 			    &                                                                          \\[0.5ex] 
     \hline\hline
     Full Designation 	    			& NVIDIA\textregistered \space GeForce\textregistered \space RTX 2080 Ti 	               \\ 
     Memory   	              		& 11264 MiB 	                                           	                 \\
     CUDA Version:                & 12.2  	                                               	                 \\
     width:                       & 64 bits                                                                  \\
     clock:                       & 33 MHz 	                                                                 \\[1ex] 
     \hline
     \hline
     Numerical Framework Details	&  				                                            		                 \\[0.5ex] 
     \hline\hline
     python                       & 3.10.12                                                                  \\
     numpy                        & 1.25.2  		                                                             \\
     scipy                        & 1.11.1  		                                                             \\
     matplotlib                   & 3.7.2   		                                                             \\
     scikit-learn                 & 1.3.0  		                                                               \\
     scikit-image                 & 0.21.0  		                                                             \\
     pytorch                      & 2.0.0			 			 	                                                       \\ 
     pytorch-cuda                 & 11.7 	 	                                                                 \\
     \LaTeX \space Distribution   & \TeX-Live			 			 	                                                     \\
     \LaTeX \space Engine/Recipe  & Auto Recipe in VS-Code \LaTeX \space Extension		                       \\[1ex] 
     \hline
    \end{tabular}
    \caption{Software and Hardware that were used on the second system}
    \label{tab:system_specs}
    \end{table}
  % \clearpage % End the page
}



