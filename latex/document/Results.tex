\chapter{Results}

First we explain the inspiration that put us on the path we took briefly and then explain the scenarios were considered. 
Due to the time limit most of what we had in mind could not be explored so we succinctly touch upon them in the hope of another 
brave soul picking up the torch and seeing them through.  

\section{Inspiration}

The approach we took was inspired by \cite{Gregor2010} as it is possibly the earliest successful attempt at unfolding/unrolling\index{\ac{DU}}\index{\ac{AU}} 
an iterative algorithm \cite{Monga2019}. The algorithm it unrolled is named \ac{ISTA}\cite{Daubechies2003}\index{\ac{ISTA}} and was devised to solve \ac{SDL}\index{\ac{SDL}} 
problem which much like the \ac{PR}\cite{Shechtman2015}\cite{Jaganathan2015}\index{\ac{PR}} is an \emph{inverse problem}\cite{Kirsch2021}\index{inverse problem} in \emph{Computational Imaging}\index{computational imaging}.

\subsection{Sparse Dictionary Learning}

Let $\boldsymbol{y} \in \mathbb{R}^m$ and $\boldsymbol{W} \in \mathbb{R}^{m \times n}$($n > m$, overdetermined/overcompleted system/dictionary)
the \ac{SDL}\index{\ac{SDL}} problem is to find a sparse $\boldsymbol{x} \in \mathbb{R}^n$ in a way that it satisfies either $\boldsymbol{y} = \boldsymbol{W}\boldsymbol{x}$ or $\boldsymbol{y} \approx \boldsymbol{W}\boldsymbol{x}$.
The \ac{LASSO}\cite{Hastie2009}\index{\ac{LASSO}} formulation of the problem will be:
\begin{equation*}
  \min_{\boldsymbol{x}} \frac{1}{2} \left|\left|\boldsymbol{y}-\boldsymbol{W}\boldsymbol{x}\right|\right|_2^2 + \lambda \left|\left|\boldsymbol{x}\right|\right|_1
\end{equation*}
where the $\left|\left|\boldsymbol{.}\right|\right|_1$ and $\left|\left|\boldsymbol{.}\right|\right|_2$ are the usual one norm\cite{Alt2016} and two norm\cite{Alt2016} in the Euclidean space, and $\lambda$ is the amount of regularization\cite{Hastie2009}. 
The \ac{ISTA}\cite{Daubechies2003}\index{ISTA} solves the \ac{LASSO}\cite{Hastie2009}\index{\ac{LASSO}} formulation by an iterative algorithm of the form:
\begin{equation*}
  \boldsymbol{x}_{k+1} = \mathcal{S}_\lambda\left(\left(\mathcal{I}-\frac{1}{\mu}\boldsymbol{W}^T\boldsymbol{W}\right)\boldsymbol{x}_k+\frac{1}{\mu}\boldsymbol{W}^T\left(\boldsymbol{y}\right)\right)
\end{equation*}
where $\mathcal{S}_\lambda$ is the elementwise soft thresholding operator given by:
\begin{equation*}
  \mathcal{S}_\lambda = \mathrm{sign}(\boldsymbol{z}) \boldsymbol{.} \max \left\{\left|\boldsymbol{z}\right|-\lambda,\boldsymbol{0}\right\}
\end{equation*}
where $\mathcal{I}$ is the second-order identity tensor\cite{Hackbusch2019}, $\left|\boldsymbol{.}\right|$ the elementwise usual absolute value, 
$\mathrm{sign}(\boldsymbol{.})$ the elementwise usual $\mathrm{sign}$ function, and $\mu$ the largest eigenvalue \cite{Hackbusch2019} 
 of $\boldsymbol{W}^T\boldsymbol{W}$ respectively.
Let $\boldsymbol{W}_t \coloneqq \mathcal{I}-\frac{1}{\mu}\boldsymbol{W}^T\boldsymbol{W}$ and 
$\boldsymbol{W}_e \coloneqq \frac{1}{\mu}\boldsymbol{W}_e\boldsymbol{y}$ to abstract away the details and have the iterative algorithm 
as:
\begin{equation*}
  \boldsymbol{x}_{k+1} = \mathcal{S}_\lambda\left(\boldsymbol{W}_t\boldsymbol{x}_{k}+\boldsymbol{W}_e\boldsymbol{y}\right)
\end{equation*}
which can be considered as a layer of a network. Stacking the layers $\mathrm{L}$ times would give the unfolded/unrolled version.

\subsection{LISTA and Ada-LISTA}

Quite naturally \cite{Gregor2010} went for training the $\boldsymbol{W}_t$ and the $\boldsymbol{W}_e$ to get some improvements 
from the original \ac{ISTA}\cite{Daubechies2003}\index{\ac{ISTA}} algorithm. What maybe was not so natural was that they replaced single $\lambda$ with a vector of $\lambda$s. 
They reported having comparable errors to that of $20\mathrm{L}$ in the accelerated version of \ac{ISTA}\cite{Daubechies2003}\index{\ac{ISTA}}(\ac{FISTA}\cite{Beck2009}\index{\ac{FISTA}})
when $\mathrm{L}$ is not too large. They got some other benefits in the rate of convergence too \cite{Daubechies2003}\cite{Beck2009}\cite{Gregor2010}. 


\section{Scenarios}

We took the \ac{WF}\cite{Candes2014}\index{\ac{WF}} \cref{pseudocode:wf} and the \ac{RWF}\cite{Zhang2016}\index{\ac{RWF}} \cref{pseudocode:rwf} and unfolded/unrolled them $L$ times($160$ times for the \ac{WF} 
and $30$ times for the \ac{RWF}) to arrive at the \ac{UWF} and the \ac{URWF} which are basically just some \ac{NN}\cite{Goodfellow2016}\cite{Bishop2006}\index{\ac{NN}} 
with some special architecture. Data are synthetic and the assumptions to generate them are:
\begin{itemize}
  \item $\boldsymbol{x} \in \boldsymbol{C}^{64}$ and both the real and the complex components are drawn from the normal distribution 
  centered at zero with the standard deviation of one \cite{Candes2014}\cite{Zhang2016}.
  \item $\boldsymbol{A} \in \boldsymbol{C}^{640 \times 64}$ and both the real and the complex components are drawn from the normal distribution 
  centered at zero and with the standard deviation of one \cite{Candes2014}\cite{Zhang2016}.
  \item $\boldsymbol{y}= \left|\boldsymbol{A}\boldsymbol{x}\right|_{X}$ where $\left|\boldsymbol{.}\right|_X$ is the usual elementwise absolute value on the complex field.
  \item $N=100$ for number of sample points. 
\end{itemize}
On how the \ac{WF}\cite{Candes2014}\index{\ac{WF}} and the \ac{RWF}\cite{Zhang2016}\index{\ac{RWF}} are set to retrieve $\boldsymbol{x}$ up to a global phase please refer to \cref{pseudocode:wf}\cite{Candes2014} and \cref{pseudocode:rwf}\cite{Zhang2016}. 
As it is evident from earlier examples \cref{image:wf_vs_rwf}, \ref{image:wf_vs_twf} in \ac{PR}\index{\ac{PR}} using \ac{CDP}\index{\ac{CDP}}, 
and \ref{image:twf_vs_rwf} and their corresponding plots in \cref{fig:cdp_wf_twf_rwf} the relative error of $10^{-4}$ 
makes at least natural images quite indistinguishable when compared to the original image to the naked eye. The 
$\mathrm{L}=160$ and $\mathrm{L}=30$ are chosen with that incentive in mind. If by unfolding/unrolling we can make 
the relative error some orders of magnitude smaller then the reconstructed images will become totally 
indistinguishable from the original image. Relative error is not the only factor in image quality as it is clear 
from \cref{image:twf_vs_rwf}. In \cref{image:twf_vs_rwf} The reconstructed image using \ac{RWF}\cite{Zhang2016}\index{\ac{RWF}} has lower relative error than the 
reconstructed image using the \ac{TWF}\cite{Chen2015}\index{\ac{TWF}} after initialization, but clearly the \ac{TWF}\cite{Chen2015}\index{\ac{TWF}} did a better job than the 
\ac{RWF}\cite{Zhang2016}\index{\ac{RWF}} at construction which further corroborates the phase importance \cite{Oppenheim1979}\cite{Oppenheim1981}\cite{Shechtman2015}. 
Standing on the shoulders of giants \cite{Gregor2010} we try to get the said couple of orders of magnitude reduction in the relative error by 
tinkering with then step size \cite{Gregor2010}. Let the update rule in any 
\ac{WF}\cite{Liu2019}\cite{Jaganathan2015} variant \cref{pseudocode:wf}, \ref{pseudocode:twf}, \ref{pseudocode:rwf}, \ref{pseudocode:irwf}, and \ref{pseudocode:imrwf} be of the form:
\begin{equation*}
  \boldsymbol{z}_{k+1} = \boldsymbol{z}_k - \tau\boldsymbol{\theta}[\boldsymbol{z}_k,\boldsymbol{A},\boldsymbol{A^*},\varphi]
\end{equation*}

\noindent where $\boldsymbol{\theta}$ is of the same size and structure as $\boldsymbol{z}$ and $\tau$ is the step size that was 
proposed by the respective algorithm. Assume that the algorithms are unfolded/unrolled $\mathrm{L}$ times, we considered the following 
scenarios by substituting $\tau$ with:

\begin{itemize}
  \item Single Scalar $\tau \in \mathbb{R}$(no change).
  \item Different Scalars $\tau_k\in\mathbb{R}$.
  \item Single Matrix $\boldsymbol{M}\in \mathbb{R}^{n\times n}$.
  \item Single Semi-Positive Definite Matrix $\boldsymbol{S}\in \mathbb{R}^{n\times n}$.
  \item Different Scalars $\tau_k \in \mathbb{R}$ multiply by a Single Matrix $\boldsymbol{M} \in \mathbb{R}^{n\times n}$.
  \item Different Scalars $\tau_k \in \mathbb{R}$ multiply by a Single Semi-Positive Definite Matrix $\boldsymbol{S} \in \mathbb{R}^{n\times n}$.
  \item Different Matrices $\boldsymbol{M}_k\in \mathbb{R}^{n\times n}$.
  \item Different Semi-Positive Definite Matrices $\boldsymbol{S}_k\in \mathbb{R}^{n\times n}$.
  % \item Adjoint operator $\boldsymbol{A}_k^*$.  
\end{itemize}

\noindent It is worth emphasizing that while from the parameter space point of view alone some scenarios are contained in others we explore them separately. 
The reasons include:
\begin{itemize}
  \item not to burn too many \ac{FLOPS}\cite{Hager2010}\cite{Hennessy2019}\index{\ac{FLOPS}} needlessly.
  \item not to confuse the optimizers unintentionally by introducing too many parameters \cite{Sun2019}.
  \item not to overparameterize and in turn introducing overfitting\cite{Bishop2006}\cite{Goodfellow2016}\cite{ShalevShwartz2014}.
  \item to come up with a \emph{micro model}, which is one of the central ideas behind \ac{DU}/\ac{AU}\cite{Shechtman2015}\index{\ac{DU}}\index{\ac{AU}}, and not a full blown general \ac{ML}/\ac{DL} model.
\end{itemize}

During the training we go with the following setting:
\begin{itemize}
  \item \adam\cite{Kingma2014}\index{\adam} as the optimizer with the starting pseudo learning rate of $\mathrm{lr}=1.000\times10^{-3}$ which is recommended\cite{Kingma2014}\cite{Sun2019}.
  \item taking $2$ samples for the mini-batch stochastic gradient descent that is wrapped inside \adam\cite{Kingma2014}\index{\adam}.
  \item splitting the data into the train data and the test data with the ratio of $9$ to $1$ and having another set of 
  validation data in each epoch, not to overfit \cite{Chollet2023}.
  \item tracking the untrained and the being trained network on train, test, and validation data as to decide on the generalization ability of the model \cite{Chollet2023}.
\end{itemize}
Results can be seen in \cref{fig:uwf_training_01_02_03}, \ref{fig:uwf_training_04_05_06}, and \ref{fig:uwf_training_07_08_optuna} for the 
\ac{UWF}\index{\ac{UWF}} and in \cref{fig:urwf_training_01_02_03}, \ref{fig:urwf_training_04_05_06}, and \ref{fig:urwf_training_07_08_optuna} 
for the \ac{URWF}\index{\ac{URWF}}. While the results look good, there is still room for improvement using \ac{HP}\cite{Hutter2019}\cite{Akiba2019}\index{\ac{HP}}. 
While a crude grid search in our case using the combination of \bash\cite{Ramey2022}\index{\bash} and \awk\cite{Robbins2023}\index{\awk} could be used due to our small parameter space, we settled on 
using a domain specific package for the \ac{HP}\cite{Hutter2019}\cite{Akiba2019} part. There are quite a number of packages that can be used for \ac{HP}\cite{Hutter2019}\cite{Akiba2019} and we took the decision to go with 
\optuna\cite{Akiba2019}\index{\optuna}. Our reasons for using the \optuna\cite{Akiba2019}\index{\optuna} include but not limited to:
\begin{itemize}
  \item Use of the latest technics in \ac{HP}\cite{Hutter2019}\cite{Akiba2019}\index{\ac{HP}}.
  \item It is quite lightweight.
  \item Describing the parameter space is both easy and flexible \cite{Akiba2019}.
  \item Pruning capabilities for not-so-optimistic scenarios by implanting probes \cite{Akiba2019}.
  \item Distributed computing can be done using up to 6 computational nodes.
  \item Usage of \ac{RDBMS} for bookkeeping, safekeeping(in case of crash or just rebooting) and handling of dead-locks associated with the distributed computing.
  \item nice dashboard for better visualization and interpretation of the results.  
\end{itemize}
After \ac{HP}\cite{Hutter2019}\cite{Akiba2019}\index{\ac{HP}} while focusing on scenarios and $\mathrm{lr}$(the pseudo learning rate in \adam) we arrive at the final proposed best scenario for the \ac{UWF} in 
\cref{fig:uwf_training_07_08_optuna} and for the \ac{URWF} in \cref{fig:urwf_training_07_08_optuna}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Training UWF Without Hyperparameter Optimization %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
%   \clearpage % Start a new page
\begin{figure}[!htbp]
  \subfloat[Single Scalar$(\tau)$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=160$]{\input{./tikz/wf/wf_s_00_l_160_e_50_lr_0.001.tex}}\\
  \subfloat[Different Scalars$(\tau_k)$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=160$]{\input{./tikz/wf/wf_s_01_l_160_e_50_lr_0.001.tex}}\\
  \subfloat[Single Matrix$(\boldsymbol{M})$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=160$]{\input{./tikz/wf/wf_s_02_l_160_e_50_lr_0.001.tex}}\\  
  \caption{\ac{UWF}\index{UWF} Training in Different Scenarios Without \optuna\cite{Akiba2019}}
  \label{fig:uwf_training_01_02_03}
  \end{figure}
%   \clearpage % End the page
}
\afterpage{%
%   \clearpage % Start a new page
\begin{figure}[!htbp]
  \subfloat[Single Semi-Positive Definite Matrix$(\boldsymbol{S})$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=160$]{\input{./tikz/wf/wf_s_03_l_160_e_50_lr_0.001.tex}}\\
  \subfloat[Different Scalars Multiplied by a Single Matrix$(\tau_k\boldsymbol{M})$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=160$]{\input{./tikz/wf/wf_s_04_l_160_e_50_lr_0.001.tex}}\\
  \subfloat[Different Scalars Multiplied by a Single Semi-Positive Definite Matrix$(\tau_k\boldsymbol{S})$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=160$]{\input{./tikz/wf/wf_s_05_l_160_e_50_lr_0.001.tex}}\\
  \caption{\ac{UWF}\index{UWF} Training in Different Scenarios Without \optuna\cite{Akiba2019}}
  \label{fig:uwf_training_04_05_06}
  \end{figure}
%   \clearpage % End the page
}

\afterpage{%
%   \clearpage % Start a new page
\begin{figure}[!htbp]
  \subfloat[Different Matrices$(\boldsymbol{M}_k)$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=160$]{\input{./tikz/wf/wf_s_06_l_160_e_50_lr_0.001.tex}}\\  
  \subfloat[Different Semi-Positive Definite Matrices$(\boldsymbol{S}_k)$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=160$]{\input{./tikz/wf/wf_s_07_l_160_e_50_lr_0.001.tex}}\\  
  \subfloat[Proposed Scenario Using \optuna\cite{Akiba2019}\index{\optuna}: Different Scalars Multiplied by a Single Matrix$(\tau_k\boldsymbol{M})$, $\mathrm{lr}=8.798\times10^{-3}, \,\mathrm{L}=160$]{\input{./tikz/wf/optuna.tex}}\\  
  \caption{\ac{UWF}\index{UWF} Training in Different Scenarios With and Without \optuna\cite{Akiba2019}}
  \label{fig:uwf_training_07_08_optuna}
  \end{figure}
%   \clearpage % End the page
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Training URWF Without Hyperparameter Optimization %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
%   \clearpage % Start a new page
\begin{figure}[!htbp]
  \subfloat[Single Scalar$(\tau)$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=30$]{\input{./tikz/rwf/rwf_s_00_l_30_e_50_lr_0.001.tex}}\\
  \subfloat[Different Scalars$(\tau_k)$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=30$]{\input{./tikz/rwf/rwf_s_01_l_30_e_50_lr_0.001.tex}}\\
  \subfloat[Single Matrix$(\boldsymbol{M})$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=30$]{\input{./tikz/rwf/rwf_s_02_l_30_e_50_lr_0.001.tex}}\\  
  \caption{\ac{URWF}\index{URWF} Training in Different Scenarios Without \optuna\cite{Akiba2019}}
  \label{fig:urwf_training_01_02_03}
  \end{figure}
%   \clearpage % End the page
}
\afterpage{%
%   \clearpage % Start a new page
\begin{figure}[!htbp]
  \subfloat[Single Semi-Positive Definite Matrix$(\boldsymbol{S})$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=30$]{\input{./tikz/rwf/rwf_s_03_l_30_e_50_lr_0.001.tex}}\\
  \subfloat[Different Scalars Multiplied by a Single Matrix$(\tau_k\boldsymbol{M})$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=30$]{\input{./tikz/rwf/rwf_s_04_l_30_e_50_lr_0.001.tex}}\\
  \subfloat[Different Scalars Multiplied by a Single Semi-Positive Definite Matrix$(\tau_k\boldsymbol{S})$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=30$]{\input{./tikz/rwf/rwf_s_05_l_30_e_50_lr_0.001.tex}}\\
  \caption{\ac{URWF}\index{URWF} Training in Different Scenarios Without \optuna\cite{Akiba2019}}
  \label{fig:urwf_training_04_05_06}
  \end{figure}
%   \clearpage % End the page
}
\afterpage{%
%   \clearpage % Start a new page
\begin{figure}[!htbp]
  \subfloat[Different Matrices$(\boldsymbol{M}_k)$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=30$]{\input{./tikz/rwf/rwf_s_06_l_30_e_50_lr_0.001.tex}}\\  
  \subfloat[Different Semi-Positive Definite Matrices$(\boldsymbol{S}_k)$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=30$]{\input{./tikz/rwf/rwf_s_07_l_30_e_50_lr_0.001.tex}}\\  
  \subfloat[Proposed Scenario Using \optuna\cite{Akiba2019}\index{\optuna}: Different Scalars Multiplied by a Single Matrix$(\tau_k\boldsymbol{M})$, $\mathrm{lr}=7.622\times10^{-3}, \,\mathrm{L}=30$]{\input{./tikz/rwf/optuna.tex}}\\  
  \caption{\ac{URWF}\index{URWF} Training in Different Scenarios With and Without \optuna\cite{Akiba2019}\index{\optuna}}
  \label{fig:urwf_training_07_08_optuna}
  \end{figure}
%   \clearpage % End the page
}




\section*{Ideas for Future Work}

We can think of a couple of directions to go on from here and we would like to propose them for future works that can be done within the scope of 
the current work.

\subsection*{Different Variants/Different Applications}

There are many \ac{WF}\cite{Jaganathan2015}\cite{Liu2019} variants out there and we can expect more to appear in the future. 
Currently \cite{Jaganathan2015}\cite{Liu2019}\cite{Chandra2017} give an overview of \ac{WF} variants and you might want to start from there for 
\ac{DU}/\ac{AU}\cite{Monga2019} on those variants. I for one would love to see the result of fine tuned 
\ac{DU}/\ac{AU}\cite{Monga2019} version of a \ac{WF} variant for a specific real world problem like \cite{Fogel2013}. 

\subsection*{Data}

\cite{Daubechies2003} solved the \ac{SDL}\index{\ac{SDL}}, in which we have parsimonious \cite{Foucart2013}\index{parsimonious} data representation, 
problem using \ac{ISTA}\index{\ac{ISTA}} and \cite{Gregor2010} unfolded/unrolled it and made \ac{LISTA}\index{\ac{LISTA}}. 
\cite{Aberdam2020} even considered the the presence of noise in the process and improved the work of \cite{Gregor2010}. 
Entire studies can be done on sparsity\index{sparsity} and the presence of noise.

\subsection*{Different Scenarios}

Depending on the function we are trying to minimize and the iterative \ac{WF}\cite{Jaganathan2015}\cite{Liu2019} variant algorithm we are unfolding/unrolling 
it is possible to investigate other scenarios for parameter learning too. Possible candidates are but not limited to:

\begin{itemize}
  \item Adjoint operator $\boldsymbol{A}^*$,
  % \item weights of the measurements $c_j$,
  \item Giving weights to the sampling operation by $\left|\phi(\boldsymbol{A}_j\psi)-G_j\right|_X^2 \rightarrow \left|c_j \odot \left(\phi(\boldsymbol{A}_j\psi)-G_j\right)\right|_X^2$ and optimizing the $c_j$s.
  \item Regularizer's weight $\lambda$.
\end{itemize}
















