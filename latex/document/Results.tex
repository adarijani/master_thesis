\chapter{Results}

First we explain the inspiration that put us on the path we took briefly and then explain the scenarios were considered. 
Due to the time limit most of what we had in mind could not be explored so we succinctly touch upon in \cref{sec:ideas_for_future_work} them in the hope of another 
brave soul picking up the torch and seeing them through.  

\section{Inspiration}

The approach we took was inspired by \cite{Gregor2010} as it is possibly the earliest successful attempt at \emph{unfolding}/\emph{unrolling}\index{\emph{unfolding}}\index{\emph{unrolling}} 
an iterative algorithm \cite{Monga2019}. The algorithm it \emph{unrolled} is named \ac{ISTA}\cite{Daubechies2003}\index{\ac{ISTA}} and was devised to solve \sdl\index{\sdl} \todo{ISTA is solving sparse recovey, not sparse dictionary learning, see also below} 
problem which much like the \pr\cite{Shechtman2015}\cite{Jaganathan2015}\index{\pr} is an inverse problem\cite{Kirsch2021}\index{inverse problem} in computational imaging\index{computational imaging}\cite{Khare2023}.

\subsection{Sparse Dictionary Learning}

Let $\boldsymbol{y} \in \mathbb{R}^m$ and $\boldsymbol{W} \in \mathbb{R}^{m \times n}$($n > m$, overdetermined/overcompleted system/dictionary\todo{An explanation of one sentence of what this property means would be helpful})
the \sdl\index{\sdl} problem is to find a sparse $\boldsymbol{x} \in \mathbb{R}^n$ \todo{Finding a sparse $\boldsymbol{x}$ for a fixed dictionary $\boldsymbol{W}$ is just called sparse recovery. Finding the dictionary $\boldsymbol{W}$ that allows for sparse $x$ if sparse dictionary learning.} in a way that it satisfies either $\boldsymbol{y} = \boldsymbol{W}\boldsymbol{x}$ or $\boldsymbol{y} \approx \boldsymbol{W}\boldsymbol{x}$\todo{Why two cases? Usually, this differnce is noise-free or noisy data $\boldsymbol{y}$.}.
The \ac{LASSO}\cite{Hastie2009}\index{\ac{LASSO}} formulation of the problem will be:
\begin{equation*}
  \min_{\boldsymbol{x}} \frac{1}{2} \left|\left|\boldsymbol{y}-\boldsymbol{W}\boldsymbol{x}\right|\right|_2^2 + \lambda \left|\left|\boldsymbol{x}\right|\right|_1
\end{equation*}
where the $\left|\left|\boldsymbol{\cdot}\right|\right|_1$ and $\left|\left|\boldsymbol{\cdot}\right|\right|_2$ are the usual $1$-norm and the $2$-norm that were defined in \cref{def:p-norm}\todo{I'm not sure if this is a broken reference. I assume these are defined in a Definition, not in a Proposition. In principle, \texttt{cref} should handle that automatically.}, and $\lambda$ is the amount of regularization\cite{Hastie2009}. 
The \ac{ISTA}\cite{Daubechies2003}\index{ISTA} solves the \ac{LASSO}\cite{Hastie2009}\index{\ac{LASSO}} formulation by an iterative algorithm of the form:
\begin{equation*}
  \boldsymbol{x}_{k+1} = \mathcal{S}_\lambda\left(\left(\mathcal{I}-\frac{1}{\mu}\boldsymbol{W}^T\boldsymbol{W}\right)\boldsymbol{x}_k+\frac{1}{\mu}\boldsymbol{W}^T\left(\boldsymbol{y}\right)\right)
\end{equation*}
where $\mathcal{S}_\lambda$ is the elementwise soft thresholding operator(a manifestation of the $\mathrm{prox}$ operator \todo{Why is there a prox manifestation? The proximal gradient algorithm is applied to the LASSO formulation above, where the data term is handled with the explicit gradient step part of the algorithm while the regularizer is handled with the prox part of the algorithm. And the prox of the 1-norm is soft thresholding, the lambda comes from the factor in front of the 1-norm.} in the presence of regularization in \cref{eq:pr_solution} given by:
\begin{equation*}
  \mathcal{S}_\lambda = \mathrm{sign}(\boldsymbol{z}) \boldsymbol{\cdot} \max \left\{\left|\boldsymbol{z}\right|-\lambda,\boldsymbol{0}\right\}
\end{equation*}
where $\mathcal{I}$ is the second-order identity tensor\todo{simlpy: The identity matrix. If you want to add details, you can say that it's of size $n\times n$.}\cite{Hackbusch2019}, $\left|\boldsymbol{\cdot}\right|$ the elementwise \todo{stressing that this is elementwise is extremely important, it's not visible from the notation} usual absolute value, 
$\mathrm{sign}(\boldsymbol{\cdot})$ the elementwise usual $\mathrm{sign}$ function, and $\mu$ the largest eigenvalue \cite{Hackbusch2019} \todo{Is this supposed to be a refernce to the definition of an eigenvalue? If you think that the reader doesn't know what an eigenvalue is, the reference won't really help and you should define it instead. If you think the reader knows, there is no need for a reference.} 
 of $\boldsymbol{W}^T\boldsymbol{W}$ respectively\todo{I think the ``respectively'' doesn't belong here.}.
Let $\boldsymbol{W}_t \coloneqq \mathcal{I}-\frac{1}{\mu}\boldsymbol{W}^T\boldsymbol{W}$ and 
$\boldsymbol{W}_e \coloneqq \frac{1}{\mu}\boldsymbol{W}\boldsymbol{y}$ to abstract away the details \todo{The main reason for this abstraction is that we see now clearly what the linear part (including the bias) of the network and the nonlinear part is.} and have the iterative algorithm 
as:
\begin{equation*}
  \boldsymbol{x}_{k+1} = \mathcal{S}_\lambda\left(\boldsymbol{W}_t\boldsymbol{x}_{k}+\boldsymbol{W}_e\boldsymbol{y}\right)
\end{equation*}
which can be considered as a layer of a \nn\todo{It's important to make this interpretation explicit: $\boldsymbol{W}_t$ is the matrix of the linear part of a fully connected layer, $\boldsymbol{W}_e\boldsymbol{y}$ is the bias of that linear part and $\mathcal{S}_\lambda$ is the activation function. This fits exactly to a standard fully connected ANN.}. It is worth emphasizing that 
while we are using \nns we did not use any of the usual activation functions associated with \ml/\dl.
In the usual \nns, activation functions are there to give flexibility to the affine transformation 
and give them the ability to approximate more and more complex mappings, but in the \du/\au setting we have naturally occurring 
nonlinearities(coming from the domain knowledge of the problem) that will serve as the activation 
functions. Stacking the layers $L$ times would give the \emph{unfolded}/\emph{unrolled} version.

\subsection{LISTA and Ada-LISTA}

Quite naturally \cite{Gregor2010} went for training the $\boldsymbol{W}_t$ and the $\boldsymbol{W}_e$ to get some improvements 
from the original \ac{ISTA}\cite{Daubechies2003}\index{\ac{ISTA}} algorithm. It is natural as they kept the intrinsic 
nonlinearities coming from the iterative solution(to benefit from the domain knowledge of the problem). 
What maybe was not so natural was that they replaced single $\lambda$ with a vector of $\lambda$s. They reported having 
comparable errors to that of $20L$ in the accelerated version of 
\ac{ISTA}\cite{Daubechies2003}\index{\ac{ISTA}}(\ac{FISTA}\cite{Beck2009}\index{\ac{FISTA}})when $L$ is not too 
large. Putting some level of restriction on the $L$ is necessary since large $L$ means you are already near the minimum 
and there is not much to gain from \emph{unfolding}/\emph{unrolling}. Another reason for using quite small $L$ is to benefit 
from not having too many iterations overall in the resulting model after the \emph{unfolded}/\emph{unrolled} model is 
trained(smaller $L$ corresponds to small required \ac{FLOPS}\cite{Hager2010}\cite{Hennessy2019}\index{\ac{FLOPS}} to reach the solution). 
\cite{Gregor2010} got some other benefits in the rate of convergence too 
\cite{Daubechies2003}\cite{Beck2009}\cite{Gregor2010}. \cite{Aberdam2020} further improved the solution(could overcome 
the presence of noise) to the \ac{LASSO}\cite{Hastie2009}\index{\ac{LASSO}} problem using their 
\emph{unfolding}/\emph{unrolling} method called \ac{Ada-LISTA}\index{\ac{Ada-LISTA}}.  


\section{Scenarios}

We took the \ac{WF}\cite{Candes2014}\index{\ac{WF}} \cref{pseudocode:wf} and the \ac{RWF}\cite{Zhang2016}\index{\ac{RWF}} \cref{pseudocode:rwf} and unfolded/unrolled them $L$ times($160$ times for the \ac{WF} 
and $30$ times for the \ac{RWF}) to arrive at the \ac{UWF} and the \ac{URWF} which are basically just some \nn\cite{Goodfellow2016}\cite{Bishop2006}\index{\nn} 
with some special architecture(innate nonlinearities replacing the usual activation functions). Data are synthetic and the assumptions to generate them are:
\begin{itemize}
  \item $\boldsymbol{x} \in \mathbb{C}^{n}, n=64$ and both the real and the complex components are drawn from the normal distribution 
  centered at zero with the standard deviation of one \cite{Candes2014}\cite{Zhang2016}.
  \item $\boldsymbol{A} \in \mathbb{C}^{m \times n}, n=64, m=640$ and both the real and the complex components are drawn from the normal distribution 
  centered at zero and with the standard deviation of one \cite{Candes2014}\cite{Zhang2016}.
  \item $\boldsymbol{y}= \left|\boldsymbol{A}\boldsymbol{x}\right|_{X} \in \mathbb{R}^m, m=640$ where $\left|\boldsymbol{.}\right|_X$ is the elementwise usual absolute value on the complex field.
  \item $N=100$ for number of sample points. 
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% WF Variants CDP Reconstruction 10^-4 Relative Error %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
  % \clearpage % Start a new page
  \thispagestyle{empty} % No header/footer on this page
  \begin{figure}[!htbp]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.7\textwidth]{./images/sat_phone_0.0001/wf_175_tef_040_rwf_045_original.png}
  \caption{Reconstruction of the Sat Phone Image Using \ac{CDP}s When the Relative Error is Almost $10^{-4}$ on all Color Channels\\
   From Top to Buttom: \ac{WF} at Iteration $175$, \ac{TWF} at Iteration $40$, \ac{RWF} at Iteration $45$, and the Original Image}
  \label{image:relative_error_0.0001}
  \end{figure}
  % \clearpage % End the page
}



On how the \ac{WF}\cite{Candes2014}\index{\ac{WF}} and the \ac{RWF}\cite{Zhang2016}\index{\ac{RWF}} are set to retrieve $\boldsymbol{x}$ up to a global phase 
please refer to \cref{pseudocode:wf}\cite{Candes2014} and \cref{pseudocode:rwf}\cite{Zhang2016}. As it is evident from reconstruction 
of the sat phone image using \ac{CDP}\index{\ac{CDP}} in \cref{image:relative_error_0.0001}, the relative error of $10^{-4}$ 
makes at least natural images quite indistinguishable when compared to the original image to the naked eye. The 
$\mathrm{L}=160$ and $\mathrm{L}=30$ are chosen with that incentive in mind. If by unfolding/unrolling we can make 
the relative error some orders of magnitude smaller then the reconstructed images will become totally 
indistinguishable from the original image. Relative error is not the only factor in image quality as it is clear 
from \cref{image:twf_vs_rwf}. In \cref{image:twf_vs_rwf} The reconstructed image using \ac{RWF}\cite{Zhang2016}\index{\ac{RWF}} has lower relative error than the 
reconstructed image using the \ac{TWF}\cite{Chen2015}\index{\ac{TWF}} after initialization, but clearly the \ac{TWF}\cite{Chen2015}\index{\ac{TWF}} did a better job than the 
\ac{RWF}\cite{Zhang2016}\index{\ac{RWF}} at construction which further corroborates the phase importance \cite{Oppenheim1979}\cite{Oppenheim1981}\cite{Shechtman2015}\ref{image:phase_swap}. 
Standing on the shoulders of giants \cite{Gregor2010} we try to get the said couple of orders of magnitude reduction in the relative error by 
tinkering with then step size \cite{Gregor2010}. Let the update rule in any 
\ac{WF}\cite{Liu2019}\cite{Jaganathan2015} variant \cref{pseudocode:wf}, \ref{pseudocode:twf}, \ref{pseudocode:rwf}, \ref{pseudocode:irwf}, and \ref{pseudocode:imrwf} be of the form:
\begin{equation*}
  \boldsymbol{z}_{k+1} = \boldsymbol{z}_k - \tau\boldsymbol{\theta}[\boldsymbol{z}_k,\boldsymbol{A},\boldsymbol{A^*},\varphi]
\end{equation*}
where $\boldsymbol{\theta}$ is of the same size and structure as $\boldsymbol{z}$ and $\tau$ is the step size that was 
proposed by the respective algorithm. Assume that the algorithms are unfolded/unrolled $\mathrm{L}$ times, we considered the following 
scenarios by substituting $\tau$ with:

\begin{itemize}
  \item Single Scalar $\tau \in \mathbb{R}$(no change).
  \item Different Scalars $\tau_k\in\mathbb{R}$.
  \item Single Semi-Positive Definite Matrix $\boldsymbol{S}\in \mathbb{R}^{n\times n}$.
  \item Single Matrix $\boldsymbol{M}\in \mathbb{R}^{n\times n}$.
  \item Different Scalars $\tau_k \in \mathbb{R}$ multiplied by a Single Semi-Positive Definite Matrix $\boldsymbol{S} \in \mathbb{R}^{n\times n}$.
  \item Different Scalars $\tau_k \in \mathbb{R}$ multiplied by a Single Matrix $\boldsymbol{M} \in \mathbb{R}^{n\times n}$.
  \item Different Semi-Positive Definite Matrices $\boldsymbol{S}_k\in \mathbb{R}^{n\times n}$.
  \item Different Matrices $\boldsymbol{M}_k\in \mathbb{R}^{n\times n}$.
  % \item Adjoint operator $\boldsymbol{A}_k^*$.  
\end{itemize}

\noindent It is worth emphasizing that while from the parameter space point of view alone some scenarios are contained in others we explore them separately. 
The reasons include:
\begin{itemize}
  \item not to burn too many \ac{FLOPS}\cite{Hager2010}\cite{Hennessy2019}\index{\ac{FLOPS}} needlessly.
  \item not to confuse the optimizers unintentionally by introducing too many parameters \cite{Sun2019}.
  \item not to overparameterize and in turn introducing overfitting\cite{Bishop2006}\cite{Goodfellow2016}\cite{ShalevShwartz2014}.
  \item to come up with a \emph{micro model}, which is one of the central ideas behind \ac{DU}/\ac{AU}\cite{Shechtman2015}\index{\ac{DU}}\index{\ac{AU}}, and not a full blown general \ac{ML}/\ac{DL} model.
\end{itemize}

During the training we go with the following setting:
\begin{itemize}
  \item \adam\cite{Kingma2014}\index{\adam} as the optimizer with the starting pseudo learning rate of $\mathrm{lr}=1.000\times10^{-3}$ which is recommended\cite{Kingma2014}\cite{Sun2019}.
  \item taking $2$ samples for the mini-batch stochastic gradient descent that is wrapped inside \adam\cite{Kingma2014}\index{\adam}.
  \item splitting the data into the train data and the test data with the ratio of $9$ to $1$ and having another set of 
  validation data in each epoch, not to overfit \cite{Chollet2023}.
  \item tracking the untrained and the being trained network on train, test, and validation data as to decide on the generalization ability of the model \cite{Chollet2023}.
\end{itemize}
Results can be seen in \cref{fig:uwf_training_01_02_03}, \ref{fig:uwf_training_04_05_06}, and \ref{fig:uwf_training_07_08_optuna} for the 
\ac{UWF}\index{\ac{UWF}} and in \cref{fig:urwf_training_01_02_03}, \ref{fig:urwf_training_04_05_06}, and \ref{fig:urwf_training_07_08_optuna} 
for the \ac{URWF}\index{\ac{URWF}}. While the results look good, there is still room for improvement using \ho\cite{Hutter2019}\cite{Akiba2019}\index{\ho}. 
While a crude grid search in our case using the combination of \bash\cite{Ramey2022}\index{\bash} and \awk\cite{Robbins2023}\index{\awk} could be used due to our small parameter space, we settled on 
using a domain specific package for the \ho\cite{Hutter2019}\cite{Akiba2019}\index{\ho} part. There are quite a number of packages that can be used for \ho\cite{Hutter2019}\cite{Akiba2019}\index{\ho} and we took the decision to go with 
\optuna\cite{Akiba2019}\index{\optuna}. Our reasons for using the \optuna\cite{Akiba2019}\index{\optuna} include but not limited to:
\begin{itemize}
  \item Use of the latest technics in \ho\cite{Hutter2019}\cite{Akiba2019}\index{\ho}.
  \item It is quite lightweight.
  \item Describing the parameter space is both easy and flexible \cite{Akiba2019}.
  \item Pruning capabilities for not-so-optimistic scenarios by implanting probes \cite{Akiba2019}.
  \item Distributed computing can be done using up to 6 computational nodes.
  \item Usage of \ac{RDBMS} for bookkeeping, safekeeping(in case of crash or just rebooting) and handling of dead-locks associated with the distributed computing.
  \item Nice dashboard for better visualization and interpretation of the results.  
\end{itemize}
After \ho\cite{Hutter2019}\cite{Akiba2019}\index{\} while focusing on scenarios and $\mathrm{lr}$(the pseudo learning rate in \adam) we arrive at the final proposed best scenario for the \ac{UWF} in 
\cref{fig:uwf_training_07_08_optuna} and for the \ac{URWF} in \cref{fig:urwf_training_07_08_optuna}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Training UWF Without Hyperparameter Optimization %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
%   \clearpage % Start a new page
\begin{figure}[!htbp]
  \subfloat[Single Scalar$(\tau)$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=160$]{\input{./tikz/wf/wf_s_00_l_160_e_50_lr_0.001.tex}}\\
  \subfloat[Different Scalars$(\tau_k)$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=160$]{\input{./tikz/wf/wf_s_01_l_160_e_50_lr_0.001.tex}}\\
  \subfloat[Single Matrix$(\boldsymbol{M})$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=160$]{\input{./tikz/wf/wf_s_02_l_160_e_50_lr_0.001.tex}}\\  
  \caption{\ac{UWF}\index{UWF} Training in Different Scenarios Without \optuna\cite{Akiba2019}}
  \label{fig:uwf_training_01_02_03}
  \end{figure}
%   \clearpage % End the page
}
\afterpage{%
%   \clearpage % Start a new page
\begin{figure}[!htbp]
  \subfloat[Single Semi-Positive Definite Matrix$(\boldsymbol{S})$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=160$]{\input{./tikz/wf/wf_s_03_l_160_e_50_lr_0.001.tex}}\\
  \subfloat[Different Scalars Multiplied by a Single Matrix$(\tau_k\boldsymbol{M})$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=160$]{\input{./tikz/wf/wf_s_04_l_160_e_50_lr_0.001.tex}}\\
  \subfloat[Different Scalars Multiplied by a Single Semi-Positive Definite Matrix$(\tau_k\boldsymbol{S})$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=160$]{\input{./tikz/wf/wf_s_05_l_160_e_50_lr_0.001.tex}}\\
  \caption{\ac{UWF}\index{UWF} Training in Different Scenarios Without \optuna\cite{Akiba2019}}
  \label{fig:uwf_training_04_05_06}
  \end{figure}
%   \clearpage % End the page
}

\afterpage{%
%   \clearpage % Start a new page
\begin{figure}[!htbp]
  \subfloat[Different Matrices$(\boldsymbol{M}_k)$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=160$]{\input{./tikz/wf/wf_s_06_l_160_e_50_lr_0.001.tex}}\\  
  \subfloat[Different Semi-Positive Definite Matrices$(\boldsymbol{S}_k)$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=160$]{\input{./tikz/wf/wf_s_07_l_160_e_50_lr_0.001.tex}}\\  
  \subfloat[Proposed Scenario Using \optuna\cite{Akiba2019}\index{\optuna}: Different Scalars Multiplied by a Single Matrix$(\tau_k\boldsymbol{M})$, $\mathrm{lr}=8.798\times10^{-3}, \,\mathrm{L}=160$]{\input{./tikz/wf/optuna.tex}}\\  
  \caption{\ac{UWF}\index{UWF} Training in Different Scenarios With and Without \optuna\cite{Akiba2019}}
  \label{fig:uwf_training_07_08_optuna}
  \end{figure}
%   \clearpage % End the page
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% Training URWF Without Hyperparameter Optimization %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\afterpage{%
%   \clearpage % Start a new page
\begin{figure}[!htbp]
  \subfloat[Single Scalar$(\tau)$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=30$]{\input{./tikz/rwf/rwf_s_00_l_30_e_50_lr_0.001.tex}}\\
  \subfloat[Different Scalars$(\tau_k)$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=30$]{\input{./tikz/rwf/rwf_s_01_l_30_e_50_lr_0.001.tex}}\\
  \subfloat[Single Matrix$(\boldsymbol{M})$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=30$]{\input{./tikz/rwf/rwf_s_02_l_30_e_50_lr_0.001.tex}}\\  
  \caption{\ac{URWF}\index{URWF} Training in Different Scenarios Without \optuna\cite{Akiba2019}}
  \label{fig:urwf_training_01_02_03}
  \end{figure}
%   \clearpage % End the page
}
\afterpage{%
%   \clearpage % Start a new page
\begin{figure}[!htbp]
  \subfloat[Single Semi-Positive Definite Matrix$(\boldsymbol{S})$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=30$]{\input{./tikz/rwf/rwf_s_03_l_30_e_50_lr_0.001.tex}}\\
  \subfloat[Different Scalars Multiplied by a Single Matrix$(\tau_k\boldsymbol{M})$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=30$]{\input{./tikz/rwf/rwf_s_04_l_30_e_50_lr_0.001.tex}}\\
  \subfloat[Different Scalars Multiplied by a Single Semi-Positive Definite Matrix$(\tau_k\boldsymbol{S})$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=30$]{\input{./tikz/rwf/rwf_s_05_l_30_e_50_lr_0.001.tex}}\\
  \caption{\ac{URWF}\index{URWF} Training in Different Scenarios Without \optuna\cite{Akiba2019}}
  \label{fig:urwf_training_04_05_06}
  \end{figure}
%   \clearpage % End the page
}
\afterpage{%
%   \clearpage % Start a new page
\begin{figure}[!htbp]
  \subfloat[Different Matrices$(\boldsymbol{M}_k)$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=30$]{\input{./tikz/rwf/rwf_s_06_l_30_e_50_lr_0.001.tex}}\\  
  \subfloat[Different Semi-Positive Definite Matrices$(\boldsymbol{S}_k)$, $\mathrm{lr}=1.000\times10^{-3}, \,\mathrm{L}=30$]{\input{./tikz/rwf/rwf_s_07_l_30_e_50_lr_0.001.tex}}\\  
  \subfloat[Proposed Scenario Using \optuna\cite{Akiba2019}\index{\optuna}: Different Scalars Multiplied by a Single Matrix$(\tau_k\boldsymbol{M})$, $\mathrm{lr}=7.622\times10^{-3}, \,\mathrm{L}=30$]{\input{./tikz/rwf/optuna.tex}}\\  
  \caption{\ac{URWF}\index{URWF} Training in Different Scenarios With and Without \optuna\cite{Akiba2019}\index{\optuna}}
  \label{fig:urwf_training_07_08_optuna}
  \end{figure}
%   \clearpage % End the page
}




\section*{Ideas for Future Work}\label{sec:ideas_for_future_work}

We can think of a couple of directions to go on from here and we would like to propose them for future works that can be done within the scope of 
the current work or by extending it.

\subsection*{Different Variants/Different Applications}

There are many \ac{WF}\cite{Jaganathan2015}\cite{Liu2019} variants out there and we can expect more to appear in the future. 
Currently \cite{Jaganathan2015}\cite{Liu2019}\cite{Chandra2017} give an overview of \ac{WF} variants and you might want to start from there for 
\du/\au\cite{Monga2019} on those variants. I for one would love to see the result of fine tuned 
\du/\au\cite{Monga2019} version of a \ac{WF} variant for a specific real world problem like \cite{Fogel2013}. 

\subsection*{Data}

\cite{Daubechies2003} solved the \sdl\index{\sdl}, in which we have parsimonious 
\cite{Foucart2013}\index{parsimonious} data representation, problem using \ac{ISTA}\index{\ac{ISTA}} and \cite{Gregor2010} 
\emph{unfolded}/\emph{unrolled} it and made \ac{LISTA}\index{\ac{LISTA}}. 
\cite{Aberdam2020} even considered the the presence of noise in the process and improved the work of \cite{Gregor2010}. 
Entire studies can be done on sparsity\index{sparsity} and the presence of noise.

\subsection*{Different Scenarios}

Depending on the function we are trying to minimize and the iterative \ac{WF}\cite{Jaganathan2015}\cite{Liu2019} variant algorithm we are \emph{unfolding}/\emph{unrolling} 
it is possible to investigate other scenarios for parameter learning too. Possible candidates are but not limited to:
\begin{itemize}
  \item Adjoint operator $\boldsymbol{A}^*$,
  % \item weights of the measurements $c_j$,
  \item Giving weights to the sampling operation by $\left|\phi(\boldsymbol{A}_j\psi)-G_j\right|_X^2 \rightarrow \left|c_j \odot \left(\phi(\boldsymbol{A}_j\psi)-G_j\right)\right|_X^2$ and optimizing the $c_j$s,
  \item Regularizer's weight $\lambda$.
\end{itemize}
















