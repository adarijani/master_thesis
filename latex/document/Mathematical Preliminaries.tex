\chapter{Mathematical Preliminaries}
\todo{A few words on what we are supposed to expect in this chapter would be helpful.}
\section{Basic Analysis}
\begin{Prop}
    For complex valued \todo{I think the term ``complex valued vectors'' is not necessarily wrong, but I think it's more common to just call them ``complex vectors''.} vectors $\boldsymbol{x},\boldsymbol{y} \in \mathbb{C}^n$ the following is a proper \todo{Is there an ``improper'' scalar product?} scalar product\index{scalar product}
    \todo{Make more clear what you ``let'' and what you define. You actually define (or rather should define) a mappint from $\mathbb{C}^n\times \mathbb{C}^n\to\mathbb{C}$ and then say that this is a scalar product. Also, you also sneak in the definition of how you write vectors.}
    \begin{equation*}
        \boldsymbol{x} = \left\{x_i\right\}_{i=1,\ldots,n-1}, \quad \boldsymbol{y} = \left\{y_i\right\}_{i=1,\ldots,n-1}
    \end{equation*}
    \begin{equation*}
        \langle\boldsymbol{x},\boldsymbol{y}\rangle \coloneqq \sum_{i=0}^{n-1} x_i \overline{y_i} 
    \end{equation*}
\end{Prop}
\begin{Proof}
    You can consult \cite{Frazier1999}\cite{Horn2012}\cite{Hackbusch2019}.\todo{I would not use the proof enviroment / put a qed symbol, if you don't give the proof. Just say ``For a proof, we refer to ...'' or something along those lines. Also, you shouldn't put three different references for one simple proof. One reference would be appropriate.}
\end{Proof}
\begin{Prop}
    For complex valued second order tensors \todo{As mentioned before, unless you need the extra generality of tensors, I would just talk about matrices. Otherwise, you'd also have to argue why you call vectors vectors and not first order tensors ;-)} $\boldsymbol{X},\boldsymbol{Y} \in \mathbb{C}^{m \times n}$ the following is a proper scalar product\index{scalar product}\todo{Same comments regarding the formulation as for the proposition above.}
    \begin{equation*}
        \boldsymbol{X} = \left\{X_{i,j}\right\}_{\substack{i=0,\ldots,m-1\\ j=0,\ldots,n-1}}, \quad \boldsymbol{Y} = \left\{Y_{i,j}\right\}_{\substack{i=0,\ldots,m-1\\ j=0,\ldots,n-1}}
    \end{equation*}
    \begin{equation*}
        \langle\boldsymbol{X},\boldsymbol{Y}\rangle \coloneqq \sum_{j=0}^{n-1}\sum_{i=0}^{m-1} X_{i,j} \overline{Y_{i,j}} 
    \end{equation*}
\end{Prop}

Which is also called the Frobenius scalar product\index{scalar product!Frobenius}, the Schur scalar product\index{scalar product!Schur} or 
the Hilbert-Schmidt scalar product\index{scalar product!Hilbert-Schmidt}.

\begin{Proof}
    You can consult \cite{Frazier1999}\cite{Horn2012}\cite{Hackbusch2019}
\end{Proof}
\begin{Def}\label{def:p-norm}
    Let $1 \leq p \leq \infty$, and $\boldsymbol{x} \in \mathbb{K}^n$ be the $n$-tuple vector on $\mathbb{K}$ which is either $\mathbb{R}$ or $\mathbb{C}$ then:
    \begin{equation*}
        \begin{split} 
            \left|\left|\boldsymbol{x}\right|\right|_p \coloneqq    
            \begin{cases}
                \left(\sum_{i=1}^{n}\left|x_i\right|^p\right)^{\frac{1}{p}} & \text{for } 1 \leq p < \infty\\
                \underset{{i=1,\dots,n}}{\max} \left|x_i\right| & \text{for } p = \infty,
            \end{cases}
        \end{split}
    \end{equation*}
    where $\left|\left|\boldsymbol{\cdot}\right|\right|$ \todo{Above the absolute value is $\left|\boldsymbol{\cdot}\right|$ not $\left|\left|\boldsymbol{\cdot}\right|\right|$} is the absolute value on the $\mathbb{K}$ field,
    is called the \textbf{p-norm} of the vector $\boldsymbol{x}$ and as the name implies is a proper norm.
\end{Def}
\begin{Proof}
    Step by step proof can be found in \cite{Alt2016}.\todo{You cannot prove a definition. In addition to the definition of $\left|\left|\cdot\right|\right|_p$, you have a statement/proposition that this object is a norm. That is the thing that is proven.}
\end{Proof}
\begin{Thm}\label{theorem:euler_formula}
    Let $\alpha,\beta,\theta, \in \mathbb{R}$ and $\mathrm{i} \coloneqq (0,1) \in \mathbb{C}$ \todo{This notation is unclear. Your $\mathbb{C}$ contains 2-vectors, not complex numbers? I think this would be rather unusual. If you want to introduce $i$, you could say it is the imaginary unit, i.e., defined by the property $i^2=-1$.} with the usual field operations associated with the 
	complex field. Then:
	\begin{equation*}
		\mathrm{e}^{\mathrm{i}\theta} = \cos \theta +  \mathrm{i}\sin \theta \qquad  \mathrm{e}^{\mathrm{i}(\alpha+\beta)} = \mathrm{e}^{\mathrm{i}\alpha}\mathrm{e}^{\mathrm{i}\beta}\index{Euler formula}\index{Function!$\exp$}\index{Function!$\sin$}\index{Function!$\cos$}
	\end{equation*}
    where $\sin$\index{Function!$\sin$} and $\cos$\index{Function!$\cos$} are the usual trigonometric functions but defined without the usual geometric arguments\todo{What does this mean? $\sin$ and $\cos$ are mappings $\mathbb{R}\to\mathbb{R}$. I don't see what you mean with ``geometric arguments'' in this context.}.
	\end{Thm}
	\begin{Proof}
        \todo{Either give the proof or don't. I don't think that a very vague discription on the proof techniques is helpful.}
		For the first part expanding the $\exp$ function and separating the so-called Real and Imaginary part and convergence arguments 
	on infinite series would confirm the claim\cite{Rudin1976}\cite{Rudin1987}\cite{Stein2005}\cite{Stein2003}. For the second part 
	convergence of multiplication of infinite series is needed and the rest is like the first part \cite{Rudin1976}\cite{Rudin1987}\cite{Stein2005}\cite{Stein2003}\todo{Again, this are extremely well known properties. One reference for the proof is more than enough.}. 
	\end{Proof}
	\begin{Cor}
		Let $a$, $b$, $\theta$, and $\varphi$ $\in \mathbb{R}$ and $\sin$\index{Function!$\sin$}, $\cos$\index{Function!$\cos$} \todo{You should not define $\sin$ and $\cos$ multiple times and in particular not differently in two consecutive statements.} the usual trigonometric functions derived from the $\exp$ function, then:
		\begin{equation*}
			a\sin \theta+b\cos \theta  = \sqrt{a^2+b^2}\sin(\theta + \varphi)
		\end{equation*}
		such that:
		\begin{equation*}
			\sin\varphi=b, \qquad \cos\varphi = a
		\end{equation*}
		\end{Cor}
	\begin{Proof}
		Use \cref{theorem:euler_formula}.\todo{This is not a proof. Also, I don't remember this particular trigonometric identity. If you need it, you should prove it. If you don't need it, I would drop it. Since this proposition doesn't have a label, I guess you don't reference / use it anywhere, but I didn't check.}
	\end{Proof}


\section{Discrete Fourier Transform}
\ac{FT} and its discrete counterparts will play an important role in one of our examples and here we give a fast refresher on the basic properties of \ac{DFT} and \ac{IDFT}.

\begin{Def}\label{def:1ddft}
    \emph{$1$-$d$ \ac{DFT}}\\
    The \emph{$1$-$d$ \ac{DFT}} of the $1$-$d$ array $\boldsymbol{X} \in \mathbb{C}^{N}$ is denoted by 
    $\hat {\boldsymbol{X}} \in \mathbb{C}^{N}$ and is defined by
    \begin{equation}\label{eq:1ddft}
        \{\hat {\boldsymbol{X}}\}_{k} \coloneqq \frac{1}{N}\sum_{n=0}^{N-1} \{{\boldsymbol{X}}\}_{n}\exp\left({\frac{-2\pi ink}{N}}\right)\index{\ac{FT}!$1$-$d$ \ac{DFT}}
    \end{equation}
    and to get back the original array one can use the \ac{IDFT} 
    \begin{equation}\label{eq:1didft}
        \{{\boldsymbol{X}}\}_{n} \coloneqq \sum_{k=0}^{N-1}\{\hat {\boldsymbol{X}}\}_{k}\exp\left({\frac{2\pi ink}{N}}\right)\index{\ac{FT}!$1$-$d$ \ac{IDFT}}
    \end{equation}    
\end{Def}

As it is evident from the formula the $1$-$d$ \ac{DFT} is a linear transformation therefore there a corresponding matrix 
and a set of basis vectors. The matrix is a dense matrix\cite{Frazier1999}\cite{Cormen2022} and due to computational 
efficiency\cite{Frazier1999}\cite{Cormen2022} is almost never computed directly, however taking a closer look at the 
basis vectors would shed some light on the nature of the said transform and is a time well spent.

\begin{Prop}\label{Prop:1ddftbasisvectors}
    The basis vectors
    \begin{equation}\label{eq:1ddftbasisvectors}
        \boldsymbol{g}^n = \left\{\exp\left({\frac{-2\pi ink}{N}}\right)\right\}_{k=0,\ldots,N-1}
    \end{equation}
    are orthogonal to each other with respect to the usual inner product for complex valued vectors 
    with the normalization constant of $N$
    \begin{equation}
        \langle\boldsymbol{g}^n,\boldsymbol{g}^{n'}\rangle= N \delta_{n,n'}
    \end{equation}
\end{Prop}

\begin{Proof}
    \begin{equation*}
        \boldsymbol{g}^n = \left\{\exp\left({\frac{-2\pi ink}{N}}\right)\right\}_{k=0,\ldots,N-1}, \quad \boldsymbol{g}^{n'} = \left\{\exp\left({\frac{-2\pi in'k}{N}}\right)\right\}_{k=0,\ldots,N-1}
    \end{equation*}
    \begin{equation*}
    \begin{split} 
        \langle\boldsymbol{g}^n,\boldsymbol{g}^{n'}\rangle &= \sum_{k=0}^{N-1} \exp\left({\frac{-2\pi ink}{N}}\right)\overline{\exp\left({\frac{-2\pi in'k}{N}}\right)}
        = \sum_{k=0}^{N-1} \exp\left({\frac{-2\pi ink}{N}}\right)\exp\left({\frac{+2\pi in'k}{N}}\right)\\
        &= \sum_{k=0}^{N-1} \exp\left({\frac{-2\pi i(n'-n)k}{N}}\right)=
        \begin{cases}
            N & \text{when $n = n'$}\text{(trivial)},\\
            0 & \text{when $n\neq n'$}\text{(using geometric sum formula)}.
        \end{cases}
    \end{split}
\end{equation*}
    
\end{Proof}


\begin{Rem}
    As stated before it is possible to represent the \ac{DFT} with a matrix\cite{Frazier1999}\cite{Bredies2018}\cite{Damelin2011}. 
    Let $\boldsymbol{W}_N$ be the matrix with $\left\{\boldsymbol{W}_N\right\}_{\substack{mn \\ 0 \leq m,n \leq N-1}}$ 
    \begin{equation*}
        \boldsymbol{W}_N \coloneqq 
        \begin{bmatrix}
            1     & 1                & 1                   & 1                   & \cdots & 1                      \\
            1     & \omega_{N}^{}    & \omega_{N}^{2}      & \omega_{N}^{3}      & \cdots & \omega_{N}^{N-1}       \\
            1     & \omega_{N}^{2}   & \omega_{N}^{4}      & \omega_{N}^{6}      & \cdots & \omega_{N}^{2(N-1)}    \\
            1     & \omega_{N}^{3}   & \omega_{N}^{6}      & \omega_{N}^{9}      & \cdots & \omega_{N}^{3(N-1)}    \\
            \cdot & \cdot            & \cdot               & \cdot               & \cdots & \cdot                  \\ 
            \cdot & \cdot            & \cdot               & \cdot               & \cdots & \cdot                  \\ 
            1     & \omega_{N}^{N-1} & \omega_{N}^{2(N-1)} & \omega_{N}^{3(N-1)} & \cdots & \omega_{N}^{(N-1)(N-1)}
            \end{bmatrix}
    \end{equation*}
    where $\omega_N = e^{-2\pi i/N}$ then $e^{-2\pi imn/N} = \omega_N^{mn}$. Considering $\boldsymbol{X}$ 
    and $\hat {\boldsymbol{X}}$ as column vectors we can rewrite the $1$-$d$ \ac{DFT} as:
    \begin{equation*}
        \hat {\boldsymbol{X}} \coloneqq \frac{1}{N}\boldsymbol{W}_N\boldsymbol{X}
    \end{equation*}
\end{Rem}








\begin{Def}\label{def:2ddft}
    \emph{$2$-$d$ \ac{DFT}}\\
    The \emph{$2$-$d$ \ac{DFT}} of the $2$-$d$ array $\boldsymbol{X} \in \mathbb{C}^{N \times M}$ is denoted by 
    $\hat {\boldsymbol{X}} \in \mathbb{C}^{N \times M}$ and is defined by
    \begin{equation}\label{eq:2ddft}
        \{\hat {\boldsymbol{X}}\}_{k,l} \coloneqq \frac{1}{MN}\sum_{m=0}^{M-1}\sum_{n=0}^{N-1} \{{\boldsymbol{X}}\}_{n,m}\exp\left({\frac{-2\pi ink}{N}}\right)\exp\left({\frac{-2\pi iml}{M}}\right)
    \end{equation}
    and to get back the original array one can use the inversion formula
    \begin{equation}\label{eq:2didft}
        \{{\boldsymbol{X}}\}_{n,m} \coloneqq \sum_{k=0}^{N-1}\sum_{l=0}^{M-1}\{\hat {\boldsymbol{X}}\}_{k,l}\exp\left({\frac{2\pi ink}{N}}\right)\exp\left({\frac{2\pi iml}{M}}\right)
    \end{equation}    
\end{Def}

As it is evident from the formula the $2$D Discrete Fourier Transform is a linear transformation therefor 
there a corresponding matrix and basis vectors. The matrix is dense matrix and due to computational efficiency 
is almost never computed directly, however taking a closer look at the basis vectors would shed some light on 
the nature of the said transform and is a time well spent.





\begin{Prop}\label{Prop:2ddftbasisvectors}
    The basis vectors
    \begin{equation}\label{eq:2ddftbasisvectors}
        \boldsymbol{g}^{n,m} = \left\{\exp\left({\frac{-2\pi ink}{N}}\right)\exp\left({\frac{-2\pi iml}{M}}\right)\right\}_{\substack{k=0,\ldots,N-1\\l=0,\ldots,M-1}}
    \end{equation}
    are orthogonal to each other with respect to the usual inner product for complex valued vectors 
    with the normalization constant of $MN$
    \begin{equation}
        \langle\boldsymbol{g}^{n,m},\boldsymbol{g}^{n',m'}\rangle= MN \delta_{n,n'}\delta_{m,m'}
    \end{equation}
\end{Prop}

\begin{Proof}
    \begin{align*} 
        \boldsymbol{g}^{n,m}    &= \left\{\exp\left({\frac{-2\pi ink}{N}}\right)\exp\left({\frac{-2\pi iml}{M}}\right)\right\}_{\substack{k=0,\ldots,N-1\\l=0,\ldots,M-1}}\\
        \boldsymbol{g}^{n',m'}  &= \left\{\exp\left({\frac{-2\pi in'k}{N}}\right)\exp\left({\frac{-2\pi im'l}{M}}\right)\right\}_{\substack{k=0,\ldots,N-1\\l=0,\ldots,M-1}}
    \end{align*}
    \begin{equation*}
        \begin{split}  
            \langle\boldsymbol{g}^{n,m},\boldsymbol{g}^{n',m'}\rangle &= \sum_{l=0}^{M-1}\sum_{k=0}^{N-1} \exp\left({\frac{-2\pi ink}{N}}\right)\exp\left({\frac{-2\pi iml}{M}}\right)\overline{\exp\left({\frac{-2\pi in'k}{N}}\right)\exp\left({\frac{-2\pi im'l}{M}}\right)}\\
            &= \sum_{l=0}^{M-1}\sum_{k=0}^{N-1} \exp\left({\frac{-2\pi ink}{N}}\right)\exp\left({\frac{-2\pi iml}{M}}\right)\exp\left({\frac{+2\pi in'k}{N}}\right)\exp\left({\frac{+2\pi im'l}{M}}\right)\\
            &= \sum_{l=0}^{M-1}\sum_{k=0}^{N-1} \exp\left({\frac{-2\pi i(n'-n)k}{N}}\right)\exp\left({\frac{-2\pi i(m'-m)l}{M}}\right)\\
            &= \sum_{k=0}^{N-1} \exp\left({\frac{-2\pi i(n'-n)k}{N}}\right)\sum_{l=0}^{M-1} \exp\left({\frac{-2\pi i(m'-m)k}{M}}\right)\\
            &= 
            \begin{cases}
                MN & \text{when $n = n' \wedge m=m'$}\text{(trivial)},\\
                0 & \text{when $\neg(n = n' \wedge m=m')$}\text{(using geometric sum formula)}.
            \end{cases}    
        \end{split}
    \end{equation*}
    

    
\end{Proof}



Fast Fourier Transform \cite{Cooley1965} \cite{Good1960} \cite{Frazier1999} \cite{Cormen2022}




\begin{Thm}\label{theorem:dft is unitary}
    
    Here goes the actual theorem description.
\end{Thm}
\begin{Proof}
    
\end{Proof}
















\section{Probability Theory}

\begin{Thm}\label{theorem:expectation_general}
    Let $X$ and $Y$ be independent discrete random variables. For expectation value of functions of our random variables we would be having:
	\begin{equation*}
		\mathbb{E}(f(X,Y)) = \sum_{X=x}^{}\sum_{Y=y}^{}f(x,y)p(x)p(y)
	\end{equation*}
    where $p(x)$ and $p(y)$ are the probability of $x$ and $y$ being realized from the random variables $X$ and $Y$.
\end{Thm}
\begin{Proof}
    The general case where $X$ and $Y$ are not independent is usually discussed. Then by assuming an independent setting 
    you will arrive at the conclusion \cite{DasGupta2010}\cite{DasGupta2011}.
\end{Proof}
\begin{Cor}\label{theorem:expectation_multiplication}
	Let $X$ and $Y$ be independent discrete random variables. We would be having:
	\begin{equation*}
		\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)
	\end{equation*}
\end{Cor}
\begin{Proof}
Use \cref{theorem:expectation_general}.
\end{Proof}






