\chapter{Mathematical Preliminaries}
\todo{A few words on what we are supposed to expect in this chapter would be helpful.}
\section{Basic Analysis}
\begin{Def}\label{def:scalar_product_complex_vectors}
    For the complex vectors $\boldsymbol{x},\boldsymbol{y} \in \mathbb{C}^N$ the mapping  $\langle\boldsymbol{.},\boldsymbol{.}\rangle \colon \mathbb{C}^N \times \mathbb{C}^N \to \mathbb{C}$ 
    \begin{equation}
        \langle\boldsymbol{x},\boldsymbol{y}\rangle \coloneqq \sum_{i=1}^{N} x_i \overline{y_i} 
    \end{equation}
    is the scalar product where
    \begin{equation}
        \boldsymbol{x} = \left(x_i\right)_{i=1,\ldots,N}, \quad \boldsymbol{y} = \left(y_i\right)_{i=1,\ldots,N}.
    \end{equation}    
\end{Def}
\begin{Def}\label{def:scalar_product_induced_norm_complex_vectors}
    For the complex vector $\boldsymbol{x} \in \mathbb{C}^N$ the mapping  $\left|\left|\boldsymbol{.}\right|\right| \colon \mathbb{C}^N \to [0,+\infty)$ 
    \begin{equation}
        \left|\left|\boldsymbol{x}\right|\right| \coloneqq  \sqrt{\langle\boldsymbol{x},\boldsymbol{x}\rangle}
    \end{equation}
    is the norm induced by the scalar product for complex vectors where
    \begin{equation}
        \boldsymbol{x} = \left(x_i\right)_{i=1,\ldots,N}.
    \end{equation}    
\end{Def}
\begin{Def}
    For the complex matrices $\boldsymbol{X},\boldsymbol{Y} \in \mathbb{C}^{M \times N}$ the mapping $\langle\boldsymbol{.},\boldsymbol{.}\rangle \colon \mathbb{C}^{M \times N} \times \mathbb{C}^{M \times N} \to \mathbb{C}$ 
    \begin{equation}
        \langle\boldsymbol{X},\boldsymbol{Y}\rangle \coloneqq \sum_{j=1}^{N}\sum_{i=1}^{M} X_{i,j} \overline{Y_{i,j}} 
    \end{equation}
    is the scalar product where
    \begin{equation}
        \boldsymbol{X} = \left(X_{i,j}\right)_{\substack{i=1,\ldots,M\\ j=1,\ldots,N}}, \quad \boldsymbol{Y} = \left(Y_{i,j}\right)_{\substack{i=1,\ldots,M\\ j=1,\ldots,N}}.
    \end{equation}    
\end{Def}

\noindent This scalar product is also called the Frobenius scalar product\index{scalar product!Frobenius}, the Schur scalar product\index{scalar product!Schur} or 
the Hilbert-Schmidt scalar product\index{scalar product!Hilbert-Schmidt}.

\begin{Def}\label{def:p-norm}
    Let $1 \leq p \leq \infty$, and $\boldsymbol{x} \in \mathbb{K}^N$ be a vector, while $\mathbb{K}$ can be either $\mathbb{R}$ or $\mathbb{C}$ then the mapping $\left|\left| \boldsymbol{.} \right|\right|_p \colon \mathbb{K}^N \to [0,+\infty)$
    \begin{equation}
        \begin{split} 
            \left|\left|\boldsymbol{x}\right|\right|_p \coloneqq    
            \begin{cases}
                \left(\sum_{i=1}^{N}\left|x_i\right|^p\right)^{\frac{1}{p}} & \text{for } 1 \leq p < \infty\\
                \underset{{i=1,\dots,N}}{\max} \left|x_i\right| & \text{for } p = \infty,
            \end{cases}
        \end{split}
    \end{equation}
    where $\left|\boldsymbol{\cdot}\right|$ is the absolute value on the $\mathbb{K}$ field,
    is called the \textbf{p-norm} of the vector $\boldsymbol{x}$.
\end{Def}
\begin{Thm}\label{theorem:euler_formula}
    Let $\alpha,\beta,\theta, \in \mathbb{R}$ and $\mathrm{i}$ the imaginary unit associated with the complex numbers in $\mathbb{C}$ 
    then we would be having:
	\begin{equation}
		\mathrm{e}^{\mathrm{i}\theta} = \cos \theta +  \mathrm{i}\sin \theta \qquad  \mathrm{e}^{\mathrm{i}(\alpha+\beta)} = \mathrm{e}^{\mathrm{i}\alpha}\mathrm{e}^{\mathrm{i}\beta}\index{Euler formula}\index{Function!$\exp$}\index{Function!$\sin$}\index{Function!$\cos$}
	\end{equation}
    where $\sin$\index{Function!$\sin$} and $\cos$\index{Function!$\cos$} are the usual trigonometric functions.
	\end{Thm}
	\begin{Rem}
	It is not so obvious why multiplication of two infinite series($\mathrm{e}^{\mathrm{i}\alpha}$ and $\mathrm{e}^{\mathrm{i}\beta}$) would converge, let alone to a specific function namely  $\mathrm{e}^{\mathrm{i}(\alpha+\beta)}$.
    It is recommended for the curious reader to check out \emph{The Exponential and Logarithmic Functions} and \emph{The Trigonometric Functions} in Chapter 8 of \cite{Rudin1976}. 
	\end{Rem}
	\begin{Lem}
		Let $a$, $b$, $\theta$, and $\varphi$ $\in \mathbb{R}$ and $\sin$\index{Function!$\sin$}, $\cos$\index{Function!$\cos$} 
        the usual trigonometric functions then:
		\begin{equation}
			a\sin \theta+b\cos \theta  = \sqrt{a^2+b^2}\sin(\theta + \varphi)
		\end{equation}
		such that:
		\begin{equation}
			\sin\varphi=\frac{b}{\sqrt{a^2+b^2}}, \qquad \cos\varphi = \frac{a}{\sqrt{a^2+b^2}}
		\end{equation}
		\end{Lem}


\section{Discrete Fourier Transform}
The \ac{FT} and its discrete counterpart will play an important role in one of our examples and here we give a fast refresher on the basic properties of the \ac{DFT} and the \ac{IDFT}.

\begin{Def}[1D \ac{DFT}]\label{def:1ddft}
    The 1D \ac{DFT}  of the vector $\boldsymbol{x} \in \mathbb{C}^{N}$ is denoted by the vector $\hat {\boldsymbol{x}} \in \mathbb{C}^{N}$ and is defined by
    \begin{equation}\label{eq:1ddft}
        {\hat x}_k \coloneqq \frac{1}{N}\sum_{n=0}^{N-1} x_n\exp\left({\frac{-2\pi ink}{N}}\right)\index{\ac{FT}!$1$-$d$ \ac{DFT}}
    \end{equation}
    and to get back the original vector one can use the \ac{IDFT}\todo{You cannot redfine $x_n$, this is just the $n$-component of the vector $\boldsymbol{x}$ you declared at the beginning of the statement. I guess you want to define the IDFT here and then have the statement (which is not a definition) that $\boldsymbol{x}=IDFT(DFT(\boldsymbol{x}))$ holds.}
    \begin{equation}\label{eq:1didft}
        x_n \coloneqq \sum_{k=0}^{N-1}\hat{x}_{k}\exp\left({\frac{2\pi ink}{N}}\right)\index{\ac{FT}!$1$-$d$ \ac{IDFT}}
    \end{equation}    
\end{Def}

\begin{Rem}
As it is evident from the formula the 1D \ac{DFT} is a linear transformation therefore there is a corresponding matrix 
and a set of basis vectors\todo{Since we are talking about a linear mapping from $\mathbb{C}^{N}$ to $\mathbb{C}^{N}$, it would be natural to consider the corresponding matrix in $\mathbb{C}^{N\times N}$ without mentioning that there is a set of basis vectors and then not specifying them.}. Let $\boldsymbol{W}_N$ be the matrix with $\left\{\boldsymbol{W}_N\right\}_{\substack{mn \\ 0 \leq m,n \leq N-1}}$\todo{Don't use curly braces to specify components. Also, I would just say $\boldsymbol{W}_N\in \mathbb{C}^{N\times N}$. You don't seem to need the notation $\left\{\boldsymbol{W}_N\right\}_{\substack{mn \\ 0 \leq m,n \leq N-1}}$, which extracts all the entries of the matrix and then puts them into a matrix...} 
\begin{equation}
    \boldsymbol{W}_N \coloneqq 
    \begin{bmatrix}
        1     & 1                & 1                   & 1                   & \cdots & 1                      \\
        1     & \omega_{N}^{}    & \omega_{N}^{2}      & \omega_{N}^{3}      & \cdots & \omega_{N}^{N-1}       \\
        1     & \omega_{N}^{2}   & \omega_{N}^{4}      & \omega_{N}^{6}      & \cdots & \omega_{N}^{2(N-1)}    \\
        1     & \omega_{N}^{3}   & \omega_{N}^{6}      & \omega_{N}^{9}      & \cdots & \omega_{N}^{3(N-1)}    \\
        \cdot & \cdot            & \cdot               & \cdot               & \cdots & \cdot                  \\ 
        \cdot & \cdot            & \cdot               & \cdot               & \cdots & \cdot                  \\ 
        1     & \omega_{N}^{N-1} & \omega_{N}^{2(N-1)} & \omega_{N}^{3(N-1)} & \cdots & \omega_{N}^{(N-1)(N-1)}
        \end{bmatrix}
\end{equation}
where $\omega_N = e^{-2\pi i/N}$ \todo{grammar} then $e^{-2\pi imn/N} = \omega_N^{mn}$. Considering $\boldsymbol{X}$ 
and $\hat {\boldsymbol{X}}$ as column vectors we can rewrite the 1D \ac{DFT} as:\todo{There is no need to reinterpret the vectors $\boldsymbol{x}$ and $\boldsymbol{y}$ as column vectors. The product of a matrix and a vector is properly defined (if the sizes match). So, I think what you want to state here is that $\hat {\boldsymbol{x}} =\frac{1}{N}\boldsymbol{W}_N\boldsymbol{x}$. You are not redefining the Fourier transform, you are stating that one gets the transform you defined above with the matrix you just defined.}
\begin{equation}
    \hat {\boldsymbol{X}} \coloneqq \frac{1}{N}\boldsymbol{W}_N\boldsymbol{X}
\end{equation}
 The matrix is a dense matrix\cite{Frazier1999}\cite{Cormen2022} \todo{As usual, only one reference for well known statements.} and due to computational 
efficiency\todo{And computational efficiency here means both number of operations and memory use. The DFT is simpy not computed using this formulation of the sum directly, but using the FFT.}\cite{Frazier1999}\cite{Cormen2022} is almost never computed directly, however taking a closer look at the 
basis vectors would shed some light on the nature of the said transform and is a time well spent.
\end{Rem}

\begin{Prop}\label{Prop:1ddftbasisvectors}
    The basis \todo{We don't know yet that these vectors form a basis. It would be better to just call them ``vectors''. From their orthononality, you can directly conclude that they form a basis.} vectors
    \begin{equation}\label{eq:1ddftbasisvectors}
        \boldsymbol{g}^n = \left(\exp\left({\frac{-2\pi ink}{N}}\right)\right)_{k=0,\ldots,N-1}
    \end{equation}
    are orthogonal to each other with respect to the usual \todo{Don't say ``usual'', refer to your definition: \cref{def:scalar_product_induced_norm_complex_vectors}} inner product for complex valued vectors 
    with the normalization constant \todo{I don't think that the concept of being ``orthogonal with a normalization constant'' is defined. It would be more clear to state that they are orthogonal and each of them has a norm of $\sqrt{N}$, which is what your formula says.} of $N$
    \begin{equation}
        \langle\boldsymbol{g}^n,\boldsymbol{g}^{n'}\rangle= N \delta_{n,n'}
    \end{equation}
\end{Prop}

\begin{Proof}\todo{Why do you put equations for $\boldsymbol{g}^n$ and $\boldsymbol{g}^{n'}$ that are identical to the one you just defined above? I think you just want to say ``Let $n,n'\in\{0,\ldots,N-1\}$''. Then, you can continue with $\langle\boldsymbol{g}^n,\boldsymbol{g}^{n'}\rangle =...$.}
    \begin{equation}
        \boldsymbol{g}^n = \left(\exp\left({\frac{-2\pi ink}{N}}\right)\right)_{k=0,\ldots,N-1}, \quad \boldsymbol{g}^{n'} = \left(\exp\left({\frac{-2\pi in'k}{N}}\right)\right)_{k=0,\ldots,N-1}
    \end{equation}
    \begin{equation}
    \begin{split} 
        \langle\boldsymbol{g}^n,\boldsymbol{g}^{n'}\rangle &= \sum_{k=0}^{N-1} \exp\left({\frac{-2\pi ink}{N}}\right)\overline{\exp\left({\frac{-2\pi in'k}{N}}\right)}
        = \sum_{k=0}^{N-1} \exp\left({\frac{-2\pi ink}{N}}\right)\exp\left({\frac{+2\pi in'k}{N}}\right)\\
        &= \sum_{k=0}^{N-1} \exp\left({\frac{-2\pi i(n'-n)k}{N}}\right)=
        \begin{cases}
            N & \text{when $n = n'$}\text{(trivial)},\\
            0 & \text{when $n\neq n'$}\text{(using geometric sum formula)}.
        \end{cases}
    \end{split}
\end{equation}    
\todo{This is not really a proof. The non-trivial part happens in ``(using geometric sum formula)''. If you want to have the proof, you need to include this part, it's only one line anyway (the geometric sum formula you can use without proof). Otherwise, skip the proof and just reference a book for it. Since you need this again in the 2D case below, I would add this here.}
\end{Proof}
\begin{Def}[2D \ac{DFT}]\label{def:2ddft}
    The 2D \ac{DFT} of the matrix $\boldsymbol{X} \in \mathbb{C}^{N \times M}$ is denoted by 
    $\hat {\boldsymbol{X}} \in \mathbb{C}^{N \times M}$ and is defined by
    \begin{equation}\label{eq:2ddft}
        {\hat X}_{k,l} \coloneqq \frac{1}{MN}\sum_{m=0}^{M-1}\sum_{n=0}^{N-1} X_{n,m}\exp\left({\frac{-2\pi ink}{N}}\right)\exp\left({\frac{-2\pi iml}{M}}\right)
    \end{equation}
    and to get back the original matrix one can use the inversion formula\todo{As in the 1D case, you cannot redfine $X_{n,m}$, this is just the $n,n$-component of the matrix $\boldsymbol{X}$ you declared at the beginning of the statement. I guess you want to define the IDFT here and then have the statement (which is not a definition) that $\boldsymbol{X}=IDFT(DFT(\boldsymbol{X}))$ holds.}
    \begin{equation}\label{eq:2didft}
        X_{n,m} \coloneqq \sum_{k=0}^{N-1}\sum_{l=0}^{M-1}{\hat X}_{k,l}\exp\left({\frac{2\pi ink}{N}}\right)\exp\left({\frac{2\pi iml}{M}}\right)
    \end{equation}    
\end{Def}

\begin{Rem}
Like the 1D \ac{DFT} it can be seen from the formula that the 2D \ac{DFT} is a linear transformation but unlike the 1D \ac{DFT}
unless you reorder the matrix that you are taking the 2D \ac{DFT} of into a vector, The transformation \emph{can not} be described using a matrix. The transformation \emph{can be} described however using a 4th order tensor. 
is denseness and computational efficiency still hold which is why we again take a closer look at the properties of the basis vectors in the hope gaining
\todo{adding some stuff like all the linear mappings of the 2nd order tensors to 2nd tensor orders can be shown using 4th order tensor}
\end{Rem}

\begin{Prop}\label{Prop:2ddftbasisvectors}
    The basis vectors
    \begin{equation}\label{eq:2ddftbasisvectors}
        \boldsymbol{g}^{n,m} = \left\{\exp\left({\frac{-2\pi ink}{N}}\right)\exp\left({\frac{-2\pi iml}{M}}\right)\right\}_{\substack{k=0,\ldots,N-1\\l=0,\ldots,M-1}}
    \end{equation}
    are orthogonal to each other with respect to the usual inner product for complex valued vectors 
    with the normalization constant of $MN$
    \begin{equation}
        \langle\boldsymbol{g}^{n,m},\boldsymbol{g}^{n',m'}\rangle= MN \delta_{n,n'}\delta_{m,m'}
    \end{equation}
\end{Prop}

\begin{Proof}
    \begin{align} 
        \boldsymbol{g}^{n,m}    &= \left\{\exp\left({\frac{-2\pi ink}{N}}\right)\exp\left({\frac{-2\pi iml}{M}}\right)\right\}_{\substack{k=0,\ldots,N-1\\l=0,\ldots,M-1}}\\
        \boldsymbol{g}^{n',m'}  &= \left\{\exp\left({\frac{-2\pi in'k}{N}}\right)\exp\left({\frac{-2\pi im'l}{M}}\right)\right\}_{\substack{k=0,\ldots,N-1\\l=0,\ldots,M-1}}
    \end{align}
    \begin{equation}
        \begin{split}  
            \langle\boldsymbol{g}^{n,m},\boldsymbol{g}^{n',m'}\rangle &= \sum_{l=0}^{M-1}\sum_{k=0}^{N-1} \exp\left({\frac{-2\pi ink}{N}}\right)\exp\left({\frac{-2\pi iml}{M}}\right)\overline{\exp\left({\frac{-2\pi in'k}{N}}\right)\exp\left({\frac{-2\pi im'l}{M}}\right)}\\
            &= \sum_{l=0}^{M-1}\sum_{k=0}^{N-1} \exp\left({\frac{-2\pi ink}{N}}\right)\exp\left({\frac{-2\pi iml}{M}}\right)\exp\left({\frac{+2\pi in'k}{N}}\right)\exp\left({\frac{+2\pi im'l}{M}}\right)\\
            &= \sum_{l=0}^{M-1}\sum_{k=0}^{N-1} \exp\left({\frac{-2\pi i(n'-n)k}{N}}\right)\exp\left({\frac{-2\pi i(m'-m)l}{M}}\right)\\
            &= \sum_{k=0}^{N-1} \exp\left({\frac{-2\pi i(n'-n)k}{N}}\right)\sum_{l=0}^{M-1} \exp\left({\frac{-2\pi i(m'-m)k}{M}}\right)\\
            &= 
            \begin{cases}
                MN & \text{when $n = n' \wedge m=m'$}\text{(trivial)},\\
                0 & \text{when $\neg(n = n' \wedge m=m')$}\text{(using geometric sum formula)}.
            \end{cases}    
        \end{split}
    \end{equation}
\end{Proof}


\begin{Rem}
    \todo{write some stuff about the FFT}
Fast Fourier Transform \cite{Cooley1965} \cite{Good1960} \cite{Frazier1999} \cite{Cormen2022}
\end{Rem}





























