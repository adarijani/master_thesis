\chapter{Mathematical Preliminaries}
\todo{A few words on what we are supposed to expect in this chapter would be helpful.}
\section{Basic Analysis}
\begin{Def}\label{def:scalar_product_complex_vectors}
    For the complex vectors $\boldsymbol{x},\boldsymbol{y} \in \mathbb{C}^N$ the mapping  $\langle\boldsymbol{.},\boldsymbol{.}\rangle \colon \mathbb{C}^N \times \mathbb{C}^N \to \mathbb{C}$ 
    \begin{equation}
        \langle\boldsymbol{x},\boldsymbol{y}\rangle \coloneqq \sum_{i=1}^{N} x_i \overline{y_i} 
    \end{equation}
    is the scalar product where
    \begin{equation}
        \boldsymbol{x} = \left(x_i\right)_{i=1,\ldots,N}, \quad \boldsymbol{y} = \left(y_i\right)_{i=1,\ldots,N}.
    \end{equation}    
\end{Def}
\begin{Def}\label{def:scalar_product_induced_norm_complex_vectors}
    For the complex vector $\boldsymbol{x} \in \mathbb{C}^N$ the mapping  $\left|\left|\boldsymbol{.}\right|\right| \colon \mathbb{C}^N \to [0,+\infty)$ 
    \begin{equation}
        \left|\left|\boldsymbol{x}\right|\right| \coloneqq  \sqrt{\langle\boldsymbol{x},\boldsymbol{x}\rangle}
    \end{equation}
    is the norm induced by the scalar product for complex vectors where
    \begin{equation}
        \boldsymbol{x} = \left(x_i\right)_{i=1,\ldots,N}.
    \end{equation}    
\end{Def}
\begin{Def}
    For the complex matrices $\boldsymbol{X},\boldsymbol{Y} \in \mathbb{C}^{M \times N}$ the mapping $\langle\boldsymbol{.},\boldsymbol{.}\rangle \colon \mathbb{C}^{M \times N} \times \mathbb{C}^{M \times N} \to \mathbb{C}$ 
    \begin{equation}
        \langle\boldsymbol{X},\boldsymbol{Y}\rangle \coloneqq \sum_{j=1}^{N}\sum_{i=1}^{M} X_{i,j} \overline{Y_{i,j}} 
    \end{equation}
    is the scalar product where
    \begin{equation}
        \boldsymbol{X} = \left(X_{i,j}\right)_{\substack{i=1,\ldots,M\\ j=1,\ldots,N}}, \quad \boldsymbol{Y} = \left(Y_{i,j}\right)_{\substack{i=1,\ldots,M\\ j=1,\ldots,N}}.
    \end{equation}    
\end{Def}

\noindent This scalar product is also called the Frobenius scalar product\index{scalar product!Frobenius}, the Schur scalar product\index{scalar product!Schur} or 
the Hilbert-Schmidt scalar product\index{scalar product!Hilbert-Schmidt}.

\begin{Def}\label{def:p-norm}
    Let $1 \leq p \leq \infty$, and $\boldsymbol{x} \in \mathbb{K}^N$ be a vector, while $\mathbb{K}$ can be either $\mathbb{R}$ or $\mathbb{C}$ then the mapping $\left|\left| \boldsymbol{.} \right|\right|_p \colon \mathbb{K}^N \to [0,+\infty)$
    \begin{equation}
        \begin{split} 
            \left|\left|\boldsymbol{x}\right|\right|_p \coloneqq    
            \begin{cases}
                \left(\sum_{i=1}^{N}\left|x_i\right|^p\right)^{\frac{1}{p}} & \text{for } 1 \leq p < \infty\\
                \underset{{i=1,\dots,N}}{\max} \left|x_i\right| & \text{for } p = \infty,
            \end{cases}
        \end{split}
    \end{equation}
    where $\left|\boldsymbol{\cdot}\right|$ is the absolute value on the $\mathbb{K}$ field,
    is called the \textbf{p-norm} of the vector $\boldsymbol{x}$.
\end{Def}
\begin{Thm}\label{theorem:euler_formula}
    Let $\alpha,\beta,\theta, \in \mathbb{R}$ and $\mathrm{i}$ the imaginary unit associated with the complex numbers in $\mathbb{C}$ 
    then we would be having:
	\begin{equation}
		\mathrm{e}^{\mathrm{i}\theta} = \cos \theta +  \mathrm{i}\sin \theta \qquad  \mathrm{e}^{\mathrm{i}(\alpha+\beta)} = \mathrm{e}^{\mathrm{i}\alpha}\mathrm{e}^{\mathrm{i}\beta}\index{Euler formula}\index{Function!$\exp$}\index{Function!$\sin$}\index{Function!$\cos$}
	\end{equation}
    where $\sin$\index{Function!$\sin$} and $\cos$\index{Function!$\cos$} are the usual trigonometric functions.
	\end{Thm}
	\begin{Rem}
	For the first part expanding the $\exp$ function and separating the so-called Real and Imaginary part and convergence arguments 
	on infinite series would confirm the claim\cite{Rudin1976}For the second part 
	convergence of multiplication of infinite series is needed.
	\end{Rem}
	\begin{Lem}
		Let $a$, $b$, $\theta$, and $\varphi$ $\in \mathbb{R}$ and $\sin$\index{Function!$\sin$}, $\cos$\index{Function!$\cos$} 
        the usual trigonometric functions then:
		\begin{equation}
			a\sin \theta+b\cos \theta  = \sqrt{a^2+b^2}\sin(\theta + \varphi)
		\end{equation}
		such that:
		\begin{equation}
			\sin\varphi=\frac{b}{\sqrt{a^2+b^2}}, \qquad \cos\varphi = \frac{a}{\sqrt{a^2+b^2}}
		\end{equation}
		\end{Lem}


\section{Discrete Fourier Transform}
The \ac{FT} and its discrete counterpart will play an important role in one of our examples and here we give a fast refresher on the basic properties of the \ac{DFT} and the \ac{IDFT}.

\begin{Def}[1D \ac{DFT}]\label{def:1ddft}
    The 1D \ac{DFT}  of the 1D array $\boldsymbol{X} \in \mathbb{C}^{N}$ is denoted by 
    $\hat {\boldsymbol{X}} \in \mathbb{C}^{N}$ and is defined by
    \begin{equation}\label{eq:1ddft}
        \{\hat {\boldsymbol{X}}\}_{k} \coloneqq \frac{1}{N}\sum_{n=0}^{N-1} \{{\boldsymbol{X}}\}_{n}\exp\left({\frac{-2\pi ink}{N}}\right)\index{\ac{FT}!$1$-$d$ \ac{DFT}}
    \end{equation}
    and to get back the original array one can use the \ac{IDFT} 
    \begin{equation}\label{eq:1didft}
        \{{\boldsymbol{X}}\}_{n} \coloneqq \sum_{k=0}^{N-1}\{\hat {\boldsymbol{X}}\}_{k}\exp\left({\frac{2\pi ink}{N}}\right)\index{\ac{FT}!$1$-$d$ \ac{IDFT}}
    \end{equation}    
\end{Def}

As it is evident from the formula the $1$-$d$ \ac{DFT} is a linear transformation therefore there a corresponding matrix 
and a set of basis vectors. The matrix is a dense matrix\cite{Frazier1999}\cite{Cormen2022} and due to computational 
efficiency\cite{Frazier1999}\cite{Cormen2022} is almost never computed directly, however taking a closer look at the 
basis vectors would shed some light on the nature of the said transform and is a time well spent.

\begin{Prop}\label{Prop:1ddftbasisvectors}
    The basis vectors
    \begin{equation}\label{eq:1ddftbasisvectors}
        \boldsymbol{g}^n = \left\{\exp\left({\frac{-2\pi ink}{N}}\right)\right\}_{k=0,\ldots,N-1}
    \end{equation}
    are orthogonal to each other with respect to the usual inner product for complex valued vectors 
    with the normalization constant of $N$
    \begin{equation}
        \langle\boldsymbol{g}^n,\boldsymbol{g}^{n'}\rangle= N \delta_{n,n'}
    \end{equation}
\end{Prop}

\begin{Proof}
    \begin{equation*}
        \boldsymbol{g}^n = \left\{\exp\left({\frac{-2\pi ink}{N}}\right)\right\}_{k=0,\ldots,N-1}, \quad \boldsymbol{g}^{n'} = \left\{\exp\left({\frac{-2\pi in'k}{N}}\right)\right\}_{k=0,\ldots,N-1}
    \end{equation*}
    \begin{equation*}
    \begin{split} 
        \langle\boldsymbol{g}^n,\boldsymbol{g}^{n'}\rangle &= \sum_{k=0}^{N-1} \exp\left({\frac{-2\pi ink}{N}}\right)\overline{\exp\left({\frac{-2\pi in'k}{N}}\right)}
        = \sum_{k=0}^{N-1} \exp\left({\frac{-2\pi ink}{N}}\right)\exp\left({\frac{+2\pi in'k}{N}}\right)\\
        &= \sum_{k=0}^{N-1} \exp\left({\frac{-2\pi i(n'-n)k}{N}}\right)=
        \begin{cases}
            N & \text{when $n = n'$}\text{(trivial)},\\
            0 & \text{when $n\neq n'$}\text{(using geometric sum formula)}.
        \end{cases}
    \end{split}
\end{equation*}
    
\end{Proof}


\begin{Rem}
    As stated before it is possible to represent the \ac{DFT} with a matrix\cite{Frazier1999}\cite{Bredies2018}\cite{Damelin2011}. 
    Let $\boldsymbol{W}_N$ be the matrix with $\left\{\boldsymbol{W}_N\right\}_{\substack{mn \\ 0 \leq m,n \leq N-1}}$ 
    \begin{equation*}
        \boldsymbol{W}_N \coloneqq 
        \begin{bmatrix}
            1     & 1                & 1                   & 1                   & \cdots & 1                      \\
            1     & \omega_{N}^{}    & \omega_{N}^{2}      & \omega_{N}^{3}      & \cdots & \omega_{N}^{N-1}       \\
            1     & \omega_{N}^{2}   & \omega_{N}^{4}      & \omega_{N}^{6}      & \cdots & \omega_{N}^{2(N-1)}    \\
            1     & \omega_{N}^{3}   & \omega_{N}^{6}      & \omega_{N}^{9}      & \cdots & \omega_{N}^{3(N-1)}    \\
            \cdot & \cdot            & \cdot               & \cdot               & \cdots & \cdot                  \\ 
            \cdot & \cdot            & \cdot               & \cdot               & \cdots & \cdot                  \\ 
            1     & \omega_{N}^{N-1} & \omega_{N}^{2(N-1)} & \omega_{N}^{3(N-1)} & \cdots & \omega_{N}^{(N-1)(N-1)}
            \end{bmatrix}
    \end{equation*}
    where $\omega_N = e^{-2\pi i/N}$ then $e^{-2\pi imn/N} = \omega_N^{mn}$. Considering $\boldsymbol{X}$ 
    and $\hat {\boldsymbol{X}}$ as column vectors we can rewrite the $1$-$d$ \ac{DFT} as:
    \begin{equation*}
        \hat {\boldsymbol{X}} \coloneqq \frac{1}{N}\boldsymbol{W}_N\boldsymbol{X}
    \end{equation*}
\end{Rem}








\begin{Def}\label{def:2ddft}
    \emph{$2$-$d$ \ac{DFT}}\\
    The \emph{$2$-$d$ \ac{DFT}} of the $2$-$d$ array $\boldsymbol{X} \in \mathbb{C}^{N \times M}$ is denoted by 
    $\hat {\boldsymbol{X}} \in \mathbb{C}^{N \times M}$ and is defined by
    \begin{equation}\label{eq:2ddft}
        \{\hat {\boldsymbol{X}}\}_{k,l} \coloneqq \frac{1}{MN}\sum_{m=0}^{M-1}\sum_{n=0}^{N-1} \{{\boldsymbol{X}}\}_{n,m}\exp\left({\frac{-2\pi ink}{N}}\right)\exp\left({\frac{-2\pi iml}{M}}\right)
    \end{equation}
    and to get back the original array one can use the inversion formula
    \begin{equation}\label{eq:2didft}
        \{{\boldsymbol{X}}\}_{n,m} \coloneqq \sum_{k=0}^{N-1}\sum_{l=0}^{M-1}\{\hat {\boldsymbol{X}}\}_{k,l}\exp\left({\frac{2\pi ink}{N}}\right)\exp\left({\frac{2\pi iml}{M}}\right)
    \end{equation}    
\end{Def}

As it is evident from the formula the $2$D Discrete Fourier Transform is a linear transformation therefor 
there a corresponding matrix and basis vectors. The matrix is dense matrix and due to computational efficiency 
is almost never computed directly, however taking a closer look at the basis vectors would shed some light on 
the nature of the said transform and is a time well spent.





\begin{Prop}\label{Prop:2ddftbasisvectors}
    The basis vectors
    \begin{equation}\label{eq:2ddftbasisvectors}
        \boldsymbol{g}^{n,m} = \left\{\exp\left({\frac{-2\pi ink}{N}}\right)\exp\left({\frac{-2\pi iml}{M}}\right)\right\}_{\substack{k=0,\ldots,N-1\\l=0,\ldots,M-1}}
    \end{equation}
    are orthogonal to each other with respect to the usual inner product for complex valued vectors 
    with the normalization constant of $MN$
    \begin{equation}
        \langle\boldsymbol{g}^{n,m},\boldsymbol{g}^{n',m'}\rangle= MN \delta_{n,n'}\delta_{m,m'}
    \end{equation}
\end{Prop}

\begin{Proof}
    \begin{align*} 
        \boldsymbol{g}^{n,m}    &= \left\{\exp\left({\frac{-2\pi ink}{N}}\right)\exp\left({\frac{-2\pi iml}{M}}\right)\right\}_{\substack{k=0,\ldots,N-1\\l=0,\ldots,M-1}}\\
        \boldsymbol{g}^{n',m'}  &= \left\{\exp\left({\frac{-2\pi in'k}{N}}\right)\exp\left({\frac{-2\pi im'l}{M}}\right)\right\}_{\substack{k=0,\ldots,N-1\\l=0,\ldots,M-1}}
    \end{align*}
    \begin{equation*}
        \begin{split}  
            \langle\boldsymbol{g}^{n,m},\boldsymbol{g}^{n',m'}\rangle &= \sum_{l=0}^{M-1}\sum_{k=0}^{N-1} \exp\left({\frac{-2\pi ink}{N}}\right)\exp\left({\frac{-2\pi iml}{M}}\right)\overline{\exp\left({\frac{-2\pi in'k}{N}}\right)\exp\left({\frac{-2\pi im'l}{M}}\right)}\\
            &= \sum_{l=0}^{M-1}\sum_{k=0}^{N-1} \exp\left({\frac{-2\pi ink}{N}}\right)\exp\left({\frac{-2\pi iml}{M}}\right)\exp\left({\frac{+2\pi in'k}{N}}\right)\exp\left({\frac{+2\pi im'l}{M}}\right)\\
            &= \sum_{l=0}^{M-1}\sum_{k=0}^{N-1} \exp\left({\frac{-2\pi i(n'-n)k}{N}}\right)\exp\left({\frac{-2\pi i(m'-m)l}{M}}\right)\\
            &= \sum_{k=0}^{N-1} \exp\left({\frac{-2\pi i(n'-n)k}{N}}\right)\sum_{l=0}^{M-1} \exp\left({\frac{-2\pi i(m'-m)k}{M}}\right)\\
            &= 
            \begin{cases}
                MN & \text{when $n = n' \wedge m=m'$}\text{(trivial)},\\
                0 & \text{when $\neg(n = n' \wedge m=m')$}\text{(using geometric sum formula)}.
            \end{cases}    
        \end{split}
    \end{equation*}
    

    
\end{Proof}



Fast Fourier Transform \cite{Cooley1965} \cite{Good1960} \cite{Frazier1999} \cite{Cormen2022}




\begin{Thm}\label{theorem:dft is unitary}
    
    Here goes the actual theorem description.
\end{Thm}
\begin{Proof}
    
\end{Proof}
















\section{Probability Theory}

\begin{Thm}\label{theorem:expectation_general}
    Let $X$ and $Y$ be independent discrete random variables. For expectation value of functions of our random variables we would be having:
	\begin{equation*}
		\mathbb{E}(f(X,Y)) = \sum_{X=x}^{}\sum_{Y=y}^{}f(x,y)p(x)p(y)
	\end{equation*}
    where $p(x)$ and $p(y)$ are the probability of $x$ and $y$ being realized from the random variables $X$ and $Y$.
\end{Thm}
\begin{Proof}
    The general case where $X$ and $Y$ are not independent is usually discussed. Then by assuming an independent setting 
    you will arrive at the conclusion \cite{DasGupta2010}\cite{DasGupta2011}.
\end{Proof}
\begin{Cor}\label{theorem:expectation_multiplication}
	Let $X$ and $Y$ be independent discrete random variables. We would be having:
	\begin{equation*}
		\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)
	\end{equation*}
\end{Cor}
\begin{Proof}
Use \cref{theorem:expectation_general}.
\end{Proof}






